\documentclass[11pt]{article}
\pagestyle{plain}
%\documentclass{article}
%\usepackage[utf8]{inputenc}
%\usepackage[english]{babel}

\usepackage{latexsym,amsmath,amssymb}
\usepackage{amsthm}
%\usepackage[notref,notcite]{showkeys}
\usepackage{amsfonts}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{pifont}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{thmtools}
\usepackage{wrapfig}
\usepackage{extarrows}
\usepackage{breqn}
\usepackage{physics}
\usepackage{afterpage}
\usepackage{enumitem}
\usepackage[utf8]{inputenc}
\usepackage{mathrsfs}
\usepackage{scalerel}
\usepackage{stackengine,wasysym}
\usepackage{aligned-overset}
\usepackage{stackengine}
\usepackage{mathtools}
\usepackage{nccmath}
\usepackage{float}
\usepackage{url}
\usepackage{esint}
\graphicspath{ {images/} }

\usepackage{hyperref}
\hypersetup{
    colorlinks = true,
    linkcolor = blue,
    filecolor = magenta,      
    urlcolor = blue,
    citecolor = blue,
}

\urlstyle{same}


\setlength{\oddsidemargin}{1pt}
\setlength{\evensidemargin}{1pt}
\setlength{\marginparwidth}{30pt} % these gain 53pt width
\setlength{\topmargin}{1pt}       % gains 26pt height
\setlength{\headheight}{1pt}      % gains 11pt height
\setlength{\headsep}{1pt}         % gains 24pt height
%\setlength{\footheight}{12 pt} 	  % cannot be changed as number must fit
\setlength{\footskip}{24pt}       % gains 6pt height
\setlength{\textheight}{650pt}    % 528 + 26 + 11 + 24 + 6 + 55 for luck
\setlength{\textwidth}{460pt}     % 360 + 53 + 47 for luck

\title{Sections and Chapters}

\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{exercise}{Exercise}[section]
\newtheorem{remark}{Remark}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\numberwithin{equation}{subsection}

\makeatletter
\newenvironment{intheorem}[1][]
  {%
   \refstepcounter{theorem}%
   {\the\thm@headfont Theorem \thetheorem\@ifempty{#1}{}{ (#1)}.}%
   \the\thm@bodyfont
  }%
  {\par}
\makeatother

\def\dsp{\def\baselinestretch{1.35}\large
\normalsize}
%%%%This makes a double spacing. Use this with 11pt style. If you
%%%%want to use this just insert \dsp after the \begin{document}
%%%%The correct baselinestretch for double spacing is 1.37. However
%%%%you can use different parameter.


\def\U{{\mathcal U}}

\begin{document}

\centerline{\Large \bf Analysis Qualifying Exam}
\centerline{Zhen Yao}

\tableofcontents{}


\bigskip


\newpage


\section{May 2019 Exam}

\begin{exercise}
Let $\sigma > 0$. Let $\left\{f_k\right\}_{k\in \mathbb{N}}$ be a sequence of functions $f_k: \mathbb{R} \to \mathbb{R}$ with $f_k(0) = 0$. Moreover, let $\left\{A_k\right\}_{k\in \mathbb{N}} \subset [0,\infty)$ be a {\it bounded} sequence of real numbers such that 
\begin{align*}
    \left|f_k(x) - f_k(y)\right| \leq A_k |x - y|^\sigma \,\, \text{for all} \,\, x, y\in \mathbb{R}.
\end{align*}
\begin{enumerate}[label=(\alph*)]
    \item Show that there exists $f: \mathbb{R}\to \mathbb{R}$ such that a subsequence $f_{k_i}$ converges uniformly to $f$ in every interval $[-a,a], a > 0$.
    
    \item {\rm *} Show that $f$ satisfies 
    \begin{align*}
        |f(x) - f(y)| \leq A|x - y|^\sigma,
    \end{align*}
    where $\displaystyle A = \liminf_{k\to\infty}A_k$.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item First, since $\left\{A_k\right\}_{k\in \mathbb{N}}$ is bounded, then there exists $M > 0$ such that for all $k > 0, \left|A_k\right| < M$. For all $k > 0$ and all $x\in [-a,a]$, with $f_k(0) = 0$, we have
    \begin{align*}
        \left|f_k(x)\right| = \left|f_k(x) - f_k(0)\right| \leq A_k |x|^\sigma \leq a^\sigma M.
    \end{align*}
    Then, $\left\{f_k\right\}_{k\in \mathbb{N}}$ is bounded.
    
    Second, we want to prove that $\left\{f_k\right\}_{k\in \mathbb{N}}$ is equicontinuous. For $\forall \varepsilon > 0$, there exists $\delta = \left(\varepsilon/M\right)^{1/\sigma}$, then for $\forall f_k, \forall x, y \in [-a,a]$, if $|x - y| \leq \delta$, then 
    \begin{align*}
        \left|f_k(x) - f_k(y)\right| \leq A_k |x - y|^\sigma \leq M \frac{\varepsilon}{M} = \varepsilon.
    \end{align*}
    Then the set of all $f_k$ is compact. By Arzela-Ascoli theorem, there exists subsequence of $f_k$ converges uniformly to $f$ on $[-a,a]$.
    
    \item 
    \begin{enumerate}[label = \arabic*)]
        \item We use diagonal method to prove that there exists a subsequence $\left\{f_{k_i}\right\}^\infty_{i=1}$ of $\left\{f_k\right\}_{k\in \mathbb{N}}$ converging pointwise to $f$ on $\mathbb{R}$. For interval $[-1,1]$, $\left\{f_k\right\}_{k\in \mathbb{N}}$ has a convergent subsequence, denoted by $f_{11}, f_{12}, \cdots$. Now the sequence $\left\{f_{1n}\right\}$ is bounded on the interval $[-2,2]$, thus it has convergent subsequence, denoted by $f_{21}, f_{22}, \cdots$. Continue this process and we have subsequences
        \begin{align*}
            & f_{11}, f_{12}, f_{13}, \cdots \\
            & f_{21}, f_{22}, f_{23}, \cdots \\
            & f_{31}, f_{32}, f_{33}, \cdots \\
            & \cdots 
        \end{align*}
        and sequence in each line is a subsequence of the previous one. Now we select $f_{11}, f_{22}, f_{33}, \cdots$. 
    
        Now we claim $\{f_{nn}\}$ is pointwise convergent at every point in $\mathbb{R}$. First, $\{f_{nn}(x), x \in [-i, i]\}$ is convergent for for every $i = 1,2,\cdots$. Indeed, the sequence 
        $$f_{11}(x), f_{22}(x), f_{33}(x), \cdots$$
        is a subsequence of the convergent sequence $f_{i1}(x), f_{i2}(x), f_{i3}(x), \cdots$. Second, let $\varepsilon > 0$, take $\delta > 0$ as before. Then, for $\forall x \in \mathbb{R}$, and $|x - y| < \delta$, there exists $N_1 > 0$ such that $x, y\in [-N_1,N_2] \subset [-a,a]$. Since the sequence $f_{nn}(y)$ is convergent, then there is $N_2 > 0$ such that for all $n,m \geq N_2$, 
        \begin{align*}
            \left|f_{nn}(y) - f_{mm}(y)\right| \leq \varepsilon.
        \end{align*}
        Now, for $N = \max\{N_1, N_2\}$, and $\forall n, m > N$, if $|x - y| < \delta$, then 
        \begin{align*}
            \left|f_{nn}(x) - f_{mm}(x)\right| & \leq \left|f_{nn}(x) - f_{nn}(y)\right| + \left|f_{nn}(y) - f_{mm}(y)\right| \\
            & + \left|f_{nn}(y) - f_{mm}(x)\right| \\
            & \leq 3 \varepsilon.
        \end{align*}
        Hence, $\{f_{nn}(x)\}$ is convergent as a Cauchy sequence. Set $\displaystyle f(x) = \lim_{m\to\infty}f_{mm}(x)$, then for $x\in \mathbb{R}$, there exists $N > 0$ such that for all $n > N$, 
        \begin{align*}
            \left|f_{nn}(x) - f(x)\right| \leq 3\varepsilon.
        \end{align*}
        Thus, $\{f_{nn}(x)\}$ is pointwise convergent to $f(x)$ on $\mathbb{R}$.
        
        \item Now we have a subsequence $\{f_{k_i}\}$ of $\{f_k\}$ such that for any $x \in \mathbb{R}$, $\displaystyle \lim_{i\to \infty}f_{k_i}(x) = f(x)$. Then, 
        \begin{align*}
            \left|f(x) - f(y)\right| = \lim_{i\to\infty} \left|f_{k_i}(x) - f_{k_i}(y)\right|.
        \end{align*}
        Also, we know a proposition about $\liminf$ \footnote{This proposition states that let $\{x_n\}$ be a bounbed sequence in $\mathbb{R}$ and $a\in \mathbb{R}$, if $a > \liminf x_n$, then there exists $k \in \mathbb{N}$ such that for all $n \geq k$, $x_n > a$.} that if $\displaystyle H > \liminf_{k\to\infty} A_k$, then there exists $N > 0$, for all $k > N$, such that $A_k < H$\cite{1}. Then we take $\displaystyle H = \liminf_{k\to\infty}A_k + \varepsilon$ and let $\varepsilon \to 0$, then we have $\displaystyle \lim_{i\to\infty} \left|f_{k_i}(x) - f_{k_i}(y)\right| \leq \liminf_{k\to\infty}A_k |x - y|^\sigma$.
    \end{enumerate}
\end{enumerate}
\end{proof}

\medskip


\begin{exercise}{\rm *}
Prove that if $X$ is a metric space and $f:X \times [0,1] \to \mathbb{R}$ is a continuous function, then $g:X \to \mathbb{R}$, defined by $\displaystyle g(x) = \sup_{t\in [0,1]}f(x,t)$ is also continuous. 
\end{exercise}
\begin{proof}
Prove by contradiction and suppose $g$ is not continuous. Then there exists $\varepsilon > 0$, for all $\delta > 0$, there exists $x_0 \in X$ such that if $d_X(x,x_0) < \delta$, then $\left|g(x) - g(x_0)\right| > \varepsilon$. 

Fix such $\varepsilon$ and pick $\delta = 1/n$, then there exists $x_n \in X$ such that if $d_X(x_n, x_0) < \delta$, then $|g(x_n, x_0)| > \varepsilon$, i.e., 
\begin{align*}
    \left|\sup_{t}f(x_n,t) - \sup_{t}f(x_0,t)\right| > \varepsilon.
\end{align*}
Also, there exists $t_n, t_0 \in [0,1]$ such that $\displaystyle f(x_n, t_n) = \sup_{t}f(x_n,t)$ and $\displaystyle f(x_n, t_0) = \sup_{t}f(x_0,t)$. Then, we have 
\begin{align*}
    \left|f(x_n,t_n) - f(x_0,t_0)\right| > \varepsilon,
\end{align*}
where $x_n \to x_0$. Since $\{t_n\}$ is bounded in compact set $[0,1]$, then it has a convergent subsequence $\{t_{n_k}\}$ such that $t_{n_k}\to s$ and $f(x_{n_k}, t_{n_k}) \to f(x_n, s)$ since $f$ is continuous. Then, 
\begin{align*}
    f\left(x_{n_k}, t_{n_k}\right) & = \sup_{t} f\left(x_{n_k}, t\right) \geq f\left(x_{n_k}, t_0\right), \\
    f\left(x_n, t_0\right) & = \sup_{t}f(x_n,t) \geq f(x_n, s),
\end{align*}
which yields
\begin{align*}
    f\left(x_n, t_0\right) \gets f\left(x_{n_k}, t_0\right) \leq f\left(x_{n_k}, t_{n_k}\right) \to f\left(x_n, s\right) \leq f\left(x_n, t_0\right).
\end{align*}
Thus, $f\left(x_{n_k}, t_{n_k}\right) \to f\left(x_n, t_0\right)$, which is a contradiction.
\end{proof}

\medskip

\begin{exercise}\label{May_2019_3}
Prove that there is no continuous and one-to-one function $f:\mathbb{R}^2 \to \mathbb{R}$. \textbf{Hint:} \textit{Assume that such a function exists and then restrict the function to the unit circle in $\mathbb{R}^2$}.
\end{exercise}
\begin{proof}
Let $S^1 = \{(x,y)| x^2 + y^2 = 1\}$. Suppose there exists continuous and one-to-one function $f: S^1 \to \mathbb{R}$. Now let $g(x) = f(x) - f(-x)$, then $g$ is also continuous. And,
\begin{align*}
    g(-x) = f(-x) - f(x) = - g(x),
\end{align*}
then $g$ is an odd function. If $g(x) = 0$, then $f(x) = -f(x)$. If not, then $g(x) > 0$ and $g(-x) < 0$ of $g(x) < 0$ and $g(-x) > 0$. In either case, $S^1$ is a connected subspace of $\mathbb{R}^2$, with intermediate value theorem, there exists $c$ between $x$ and $-x$ such that $g(c) = 0$. Thus, $f(c) = f(-c)$. Thus, $f$ cannot be one-to-one, which is a contradiction\cite{2}.
\end{proof}

\medskip

\begin{proof}[Second Proof of Exercise \ref{May_2019_3}]
Since $S^1$ is compact, then $f$ attains maximum $a$ at some point $x_a \in S^1$ and minimum $b < a$ at some point $x_b \in S^1$. These two points separate the circle into two arcs. On each of the two arcs, the function $f$ will attain $\frac{a+b}{2}$ at some points, denoted by $x_1$ and $x_2$, since $f$ is continuous. Then $f(x_1) = f(x_2)$, which is contradicted by the fact that $f$ is one-to-one\cite{3}.
\end{proof}

\medskip

\begin{exercise}\label{May_2019_4}
Suppose $f: \mathbb{R}^2_+ \to \mathbb{R}$ is a continuous function defined on
\begin{align*}
    \mathbb{R}^2_+ = \{(x,y): x \in \mathbb{R}, y > 0\}.
\end{align*}
Assume also that the limits
\begin{align*}
    g(u,v) & = \lim_{t \to 0} \frac{f((u+t)\cos v, (u+t)\sin v) - f(u\cos v, u\sin v)}{t}, \\
    h(u,v) & = \lim_{t \to 0} \frac{f(u\cos (v+t), u\sin (v+t)) - f(u\cos v, u\sin v)}{t},
\end{align*}
exist and define continuous functions $g, h$ on the domain $D = \{(u,v): u > 0, 0 < v < \pi\}$. Prove that the function $f$ is differentiable on $\mathbb{R}^2_+$.
\end{exercise}
\begin{proof}
Define $M(u,v) = f(u\cos v, v\sin v) = f(x,y)$, where $u > 0, 0 < v < \pi$. And by assumption, $\displaystyle \frac{\partial M}{\partial u}$ and $\displaystyle \frac{\partial M}{\partial v}$ exist and are continuous. Then, for any $(u_1, v_1), (u_2, v_2) \in D$, there exist $\xi \in (u_1, u_2)$ and $\zeta \in (v_1,v_2)$ such that
\begin{align*}
    M(u_2,v_2) - M(u_1,v_1) & = M(u_2,v_2) - M(u_1,v_2) + M(u_1,v_2) - M(u_1,v_1) \\
    & = \frac{\partial M}{\partial u}(\xi, v_2)(u_2 - u_1) + \frac{\partial M}{\partial v}(u_1, \zeta)(v_2 - v_1).
\end{align*}
Then,
\begin{align*}
    & \lim_{(u_2,v_2) \to (u_1,v_1)}  \frac{\left|M(u_2,v_2) - M(u_1,v_1) - \frac{\partial M}{\partial u}(u_1, v_1)(u_2 - u_1) - \frac{\partial M}{\partial v}(u_1, v_1)(v_2 - v_1) \right|}{\left\|(u_2,v_2) - (u_1,v_1)\right\|} \\
    \leq & \underbrace{\left(\left|\frac{\partial M}{\partial u}(\xi, v_2) - \frac{\partial M}{\partial u}(u_1, v_1)\right| + \left|\frac{\partial M}{\partial v}(u_1, \zeta) - \frac{\partial M}{\partial u}(u_1, v_1)\right| \right)}_{\to 0} \underbrace{\frac{|u_2 - u_1|}{\left\|(u_2,v_2) - (u_1,v_1)\right\|}}_{\leq 1} \to 0.
\end{align*}
Thus, $M(u,v)$ is differentiable in $D$. So is $f$ on $\mathbb{R}^2_+$.
\end{proof}

\medskip

\begin{proof}[Second Proof of Problem \ref{May_2019_4}]
For any $(x_1, y_1), (x_2, y_2) \in \mathbb{R}^2_+$, there exist $(u_1, v_1), (u_2, v_2) \in D$ such that $(x_1, y_1) = (u_1 \cos v_1, u_1 \sin v_1)$ and $(x_2, y_2) = (u_2 \cos v_2, u_2 \sin v_2)$. Then,
\begin{align*}
    & \lim_{(x_2,y_2) \to (x_1,y_1)} \frac{\left|f(x_2,y_2) - f(x_1,y_1) - g(u_1,v_1)(u_2 - u_1) - h(u_1,v_1)(v_2 - v_1) \right|}{\sqrt{(x_2-x_1)^2 + (y_2-y_1)^2}} \\
    = & \lim_{(x_2,y_2) \to (x_1,y_1)} | f(u_2 \cos v_2, u_2 \sin v_2) - f(u_1 \cos v_2, u_2 \sin v_2) + f(u_1 \cos v_2, u_2 \sin v_2) \\
    & - f(u_1 \cos v_1, u_1 \sin v_1) - g(u_1,v_1)(u_2 - u_1) - h(u_1,v_1)(v_2 - v_1)| / \|x_2-x_1, y_2-y_1\| \\
    = &\,\,  0,
\end{align*}
where in the last step we used the definition of $g(u,v)$ and $h(u,v)$. Thus, $f$ is differentiable in $\mathbb{R}^2_+$.
\end{proof}

\medskip

\begin{proof}[Third Proof of Problem \ref{May_2019_4}]
Let $\Phi: D \to \mathbb{R}^2_+$ defined as $\Phi(u,v) = (u \cos v, u \sin v)$ is diffeomorphism, since $J\Phi(u,v) = \begin{bmatrix} \cos v & -u \sin v \\ \sin v & u \cos v \end{bmatrix} = u > 0$. Also,
\begin{align*}
    g(u,v) = \frac{\partial (f \circ \Phi)}{\partial u}, \quad h(u,v) = \frac{\partial (f \circ \Phi)}{\partial v}
\end{align*}
and partial derivatives of $f \circ \Phi$ are continuous, and then $f \circ \Phi \in C^1$. Thus, $f = (f \circ \Phi) \circ \Phi^{-1} \in C^1$.
\end{proof}

\medskip

\begin{exercise}
Let $f \in C^2(\Omega) \cap C^0(\overline{\Omega})$, where $\Omega \subset \mathbb{R}^n$ is open and bounded.
Let $\Delta f = \sum_{i=1}^n \partial^2f/\partial x_i^2$ be the Laplace operator.
\begin{enumerate}
    \item[(a)] Show that if for some $\varepsilon > 0$ and $x_0 \in \Omega$ we have $\Delta f(x_0) \geq \varepsilon$, then $f$ has no local maximum at $x_0$.
    
    \item[(b)] Conclude that if $\Delta f(x) \geq \varepsilon$ for some $\varepsilon > 0$ and all $x \in \Omega$, then we have $\sup_{\Omega} f = \sup_{\partial \Omega} f$.
    
    \item[(c)] Conclude that if $\Delta f(x) \geq 0$ for all $x \in \Omega$, then we have $\sup_{\Omega} f = \sup_{\partial \Omega} f$.
\end{enumerate}
{\bf Hint for part (c):} Observe that $\Delta |x|^2 = 2n$. Use it to modify a function $f$ in (c) so that you can apply part (b).
\end{exercise}
\begin{proof}
~\begin{enumerate}
    \item[(a)] Local maximum requires that $H_{x_0}(f)$ is positive semidefinitely, which means the trace $\partial^2f/\partial x_i^2, i = 1,2,\cdots,n$ of $H_{x_0}(f)$ are not positive. This is a contradiction with the fact $\Delta f(x_0) \geq \varepsilon$, then $f$ has no local maximum at $x_0$.
    
    \item[(b)] With (a), we can know that $f$ has no local maximum in $\Omega \setminus \partial \Omega$. Thus, $\displaystyle \sup_{\Omega} f = \sup_{\partial \Omega} f$.
    
    \item[(c)] Let $f_\varepsilon(x) = f(x) + \varepsilon |x|^2$, then $\Delta f_\varepsilon(x) = \Delta f(x) + 2 \varepsilon n$. Then, we have
    \begin{align*}
        \sup_{\Omega} f(x) \leq \sup_{\Omega} f_\varepsilon(x) \leq \sup_{\partial\Omega} f_\varepsilon(x) \leq \sup_{\partial \Omega} f(x) + 2 \varepsilon n \xrightarrow{\varepsilon \to 0} \sup_{\partial \Omega} f(x).
    \end{align*}
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}
For $x = (x_1, x_2) \in \mathbb{R}^2$, let $|x| = \sqrt{x_1^2 + x_2^2}$. Let $D = \{x \in \mathbb{R}^2: |x| < 1\}$ and let $f: \overline{D} \to \mathbb{R}$ be continuous on $\overline{D}$. Prove that
\begin{align*}
    \lim_{n\to\infty} \int\int_D (n + 2)|x|^n f(x)\, dA = \int^{2\pi}_0 f(\cos t, \sin t)\, dt.
\end{align*}
\end{exercise}
\begin{proof}
Since $\overline{D}$ is compact, then $f$ attains maximum on $\overline{D}$, then there exists $M$ such that $|f(x)| \leq M, \forall x \in \overline{D}$. With polar coordinates, we want to prove that 
\begin{align*}
    \lim_{n\to\infty} \int^{2\pi}_0 \int^1_0 (n+2) |r|^{n+1} \left[f(r\cos t,r\sin t) - f(\cos t, \sin t) \right] \, dr dt = 0.
\end{align*}
Indeed, for any $\varepsilon > 0$, choose $a < 1$ such that $|f(r\cos t,r\sin t) - f(\cos t, \sin t)| < \varepsilon$ for $a < r < 1$, then 
\begin{align*}
    & \left| \int^{2\pi}_0 \int^1_0 (n+2) |r|^{n+1} \left[f(r\cos t,r\sin t) - f(\cos t, \sin t) \right] \, dr dt \right| \\
    \leq & \left| \int^{2\pi}_0 \int^a_0 (n+2) |r|^{n+1} \left[f(r\cos t,r\sin t) - f(\cos t, \sin t) \right] \, dr dt \right| \\
    & + \left| \int^{2\pi}_0 \int^1_a (n+2) |r|^{n+1} \left[f(r\cos t,r\sin t) - f(\cos t, \sin t) \right] \, dr dt \right| \\
    \leq & \underbrace{\int^{2\pi}_0 2 M a^{n+2} \, dt}_{\to 0\,\,{\rm as}\,\, n \to \infty}  + \underbrace{\int^{2\pi}_0 (n+2) \varepsilon \, dt}_{\to 0\,\,{\rm as}\,\, \varepsilon \to 0} \to 0.
\end{align*}
Thus, we complete the proof\footnote{This method comes from homework for Math1530 Advanced Calculus I: Let $f:[0,1] \to \mathbb{R}$ be continuous function. Prove that $\lim_{n\to\infty} n \int^1_0 x^n f(x)\, dx = f(1)$.}.
\end{proof}

\newpage

\section{August 2018 Exam}


\begin{exercise}
For $n$ a positive integer, put:
\begin{align*}
    t_n = \frac{1}{2n + 1} - \frac{1}{2n + 2} + \frac{1}{2n + 3} - \frac{1}{2n + 4} + \cdots + \frac{1}{4n - 1} - \frac{1}{4n},
\end{align*}
with $2n$ terms in the right hand side. Find, with proof, the following limit $\mathcal{T}$:
\begin{align*}
    \mathcal{T} = \lim_{n\to\infty} nt_n.
\end{align*}
{\bf Hint:} {\em Relate the given limit to suitable Riemann sums for the function $(1 + x)^{-1}$.}
\end{exercise}
\begin{proof}
\begin{align*}
    \mathcal{T} & = n \left(\frac{1}{2n + 1} + \frac{1}{2n + 3} + \cdots + \frac{1}{4n - 1}\right) - n \left(\frac{1}{2n + 2} + \frac{1}{2n + 4} + \cdots +  \frac{1}{4n}\right) \\
    & = n \int^2_0 \frac{1}{2 + x}\, dx - 2n \int^1_0 \frac{1}{1 + x}\, dx \\
    & = n\ln 4 - n\ln 2 - n\ln 2 \\
    & = 0.
\end{align*}
\end{proof}

\medskip

\begin{exercise}{\rm \cite{4}*}
Prove that if $f: [a,b] \to \mathbb{R}$ is continuous, the 
\begin{align*}
    \lim_{n\to\infty} \sqrt[n]{\int^b_a |f(x)|^n\, dx} = \sup_{x\in[a,b]} |f(x)|.
\end{align*}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Since $f$ is continuous on compact space $[a,b]$, then $f$ attains its maximum at some point $c \in [a,b]$ such that $\displaystyle \left|f(c)\right| = \sup_{x\in[a,b]} \left|f(x)\right| = M$. Then\footnote{We cannot use $\lim$ here since we could use it unless the limit exists. However, we do not know the limit exists. Thus, we use $\liminf$ and $\limsup$.},
    \begin{align*}
        \limsup_{n\to\infty} \sqrt[n]{\int^b_a |f(x)|^n\, dx} & \leq \limsup_{n\to\infty} M (b - a)^{\frac{1}{n}} = \sup_{x\in[a,b]} |f(x)|.
    \end{align*}
    
    \item Given $\varepsilon > 0$, it follows from the continuity of $|f|$ that $|f(x)| \geq M - \varepsilon$ for all $x \in [a,b]$ in some interval $I$ that contains $c$. Then,
    \begin{align*}
        \liminf_{n\to\infty} \sqrt[n]{\int^b_a |f(x)|^n\, dx} & \geq  \liminf_{n\to\infty} \sqrt[n]{\int_I |f(x)|^n\, dx} \geq \liminf_{n\to\infty} (M - \varepsilon) |I|^{\frac{1}{n}} = M - \varepsilon.
    \end{align*}
    Since the above inequality holds for any $\varepsilon > 0$, then taking $\varepsilon \to 0$ gives
    \begin{align*}
        M \leq \liminf_{n\to\infty} \sqrt[n]{\int^b_a |f(x)|^n\, dx} \leq \limsup_{n\to\infty} \sqrt[n]{\int^b_a |f(x)|^n\, dx} \leq M.
    \end{align*}
    Thus, the equality holds.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}
Let $\mathcal{M}$ denote the space of all real $2 \times 2$ matrices, equipped with the norm $\|A\| = \sqrt{{\rm tr} \left(A^T A\right)}$, for $A \in \mathcal{M}$. Consider the map $F$ from $\mathbb{R}^2$ to $\mathcal{M}$ given by formula, for any $(s,t) \in \mathbb{R}^2$:
\begin{align*}
    F(s,t) = \frac{1}{2} 
    \begin{vmatrix}
        \cos t + \cos s & \sin t + \sin s \\
        - \sin t + \sin s & \cos t - \cos s
    \end{vmatrix}.
\end{align*}
Denote by $\mathcal{N} \subset \mathcal{M}$ the space of all real $2 \times 2$ matrices of rank one and norm one. Prove that the image of $F$ is the space $\mathcal{N}$ and that the map $F$ is a local homeomorphism to its image. 
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item \begin{enumerate}[label=\arabic*)]
        \item Firstly, consider the elementary row operation of $F(s,t)$, which is
        \begin{align*}
            F(s,t) \to \frac{1}{2} 
            \begin{vmatrix}
                \cos t + \cos s & \sin t + \sin s \\
                0 & (\cos t - \cos s) + \frac{\sin^2 t - \sin^2 s}{\cos t + \cos s}
            \end{vmatrix} \to 
            \frac{1}{2}\begin{vmatrix}
                \cos t + \cos s & \sin t + \sin s \\
                0 & 0
            \end{vmatrix}.
        \end{align*}
        Also, $F(s,t)$ has rank one. Also, we have 
        $$\sqrt{{\rm tr} \left(F^T F\right)} = \sqrt{1/2 (\cos^2 t + \sin^2 t + \cos^2 s + \sin^2 s)} = 1.$$ 
        Thus, $F$ maps $\mathbb{R}^2$ to $\mathcal{N}$.
        
        \item Now we need to prove that $F$ is onto $\mathcal{N}$. For any matrix $A \in \mathcal{N}$, since $\rank A = 1$, we can assume 
        \begin{align*}
            A = \begin{vmatrix}
            a & b \\
            ka & kb
            \end{vmatrix},
        \end{align*}
        and then with norm being one, we have $(k^2 + 1)(a^2 + b^2) = 1$. 
        
        Now take\cite{5}
        \begin{align*}
            a & = \frac{\cos t + \cos s }{2} = \cos\left(\frac{t+s}{2}\right) \cos \left(\frac{t-s}{2}\right), \\
            b & = \frac{\sin t + \sin s }{2} = - \cos \left(\frac{t+s}{2}\right) \sin \left(\frac{t-s}{2}\right),
        \end{align*}
        and plug it back into $(k^2 + 1)(a^2 + b^2) = 1$ gives $k^2 = \sin^2 \left(\frac{t-s}{2}\right)/\cos^2 \left(\frac{t-s}{2}\right)$. Choosing $k = - \sin \left( \frac{t-s}{2} \right)/\cos \left( \frac{t-s}{2} \right)$ implies that $A$ can be represent by $F(s,t)$. Thus, $F$ is onto. Hence, the image of $F$ is the space $\mathcal{N}$.
    \end{enumerate}
    
    \item* The space $\mathcal{N}$ can be characterised as
    \begin{align*}
        \mathcal{N} = \left\{ \begin{pmatrix}
            a & b \\ 
            c & d
        \end{pmatrix}: a^2 + b^2 + c^2 + d^2=1, ad-bc = 0\right\}.
    \end{align*}
    The map $F$ can be viewed from $(s,t)$ to $\begin{pmatrix}
        x(s,t) & y(s,t) \\ 
        z(s,t) & w(s,t)
    \end{pmatrix}$, and the map $\begin{pmatrix}
        x & y\\ 
        z & w
    \end{pmatrix} \to \begin{pmatrix}
        a & b \\ 
        c & d
    \end{pmatrix}$ is a linear transformation and hence a homeomorphism\cite{6}.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}{\rm *}
Let $\mathcal{F} \subset C^\infty[0,1]$ be a uniformly bounded and equicontinuous family of smooth functions on $[0,1]$ such that $f' \in \mathcal{F}$ whenever $f \in \mathcal{F}$. Suppose that 
\begin{align*}
    \sup_{x\in[0,1]} \left|f'(x) - g'(x)\right| \leq \frac{1}{2} \sup_{x\in[0,1]} \left|f(x) - g(x)\right|, \,\, {\rm for all}\,\, f, g \in \mathcal{F}.
\end{align*}
Show that there exists a sequence $f_n$ of functions in $\mathcal{F}$ that converges uniformly to $Ce^x$ for some real value $C$.\\
{\bf Hint:} Use the contraction principle. In order to apply the the contraction principle you can use, without proof, the fact that if $X$ is a complete metric space, $A \subset X$ and $T: A \to X$ is uniformly continuous, then $T$ uniformly converges to a continuous map $\overline{T}: \overline{A} \to X$ defined on the closure $\overline{A}$.
\end{exercise}
\begin{proof}
$\mathcal{F}$ is a subset of the complete metric space $\left(C([0,1]), d_\infty \right)$. Let $\overline{\mathcal{F}}$ be the closure in that space, clearly, $\left(\overline{\mathcal{F}}, d_\infty \right)$ is a complete metric space as a closed subset of a metric space\footnote{If $X$ is a complete metric space, then a subset of $X$ is closed if and only if it is complete.\cite{7}} \footnote{{\bf Theorem:} Let $A \subset X$ be a closed subspace of a complete metric space $(X, d)$, then
$(A, d)$ is a complete metric space as well.\cite{8}}. The mapping $T: \mathcal{F} \to \mathcal{F} \subset \overline{\mathcal{F}}, Tf = f'$ satisfies
\begin{align*}
    d_\infty(Tf, Tg) \leq \frac{1}{2} d_\infty(f, g), \,\, \forall f,g \in \mathcal{F}.
\end{align*}
That means $T: \mathcal{F} \to \overline{\mathcal{F}}$ is Lipschitz continuous and hence uniformly continuous\cite{9}. Extend $T$ uniquely to a continuous mapping $\overline{T}: \overline{\mathcal{F}} \to \overline{\mathcal{F}}$ \footnote{Let $\left(X, d_X\right)$ be a metric space, $A \subset X$ is a dense subset and $\left(Y, d_Y\right)$ a complete metric space. If $f: A \to Y$ is uniformly continuous, then there is a unique continuous function $F: X \to Y$ such that $F(x) = f(x)$ for all $x \in A$. Also, $F$ is uniformly continuous.} such that 
\begin{align*}
    \overline{T}f = Tf = f', \,\, f \in \mathcal{F}.
\end{align*}
Note that 
\begin{align*}
    d_\infty \left(\overline{T}f, \overline{T}g \right) \leq \frac{1}{2} d_\infty (f,g), \,\, f, g \in \overline{\mathcal{F}}.
\end{align*}
Indeed, if $f, g \in \overline{\mathcal{F}}$, then there exists $\mathcal{F} \ni f_n \rightrightarrows f$ and $\mathcal{F} \ni g_n \rightrightarrows g$. Then,
\begin{align*}
    d_\infty \left(\overline{T}f, \overline{T}g \right) = \lim_{n\to\infty} d_\infty \left(\overline{T}f_n, \overline{T}g_n \right) \leq \lim_{n\to\infty} \frac{1}{2} d_\infty \left(f_n, g_n\right) = \frac{1}{2} d_\infty (f,g).
\end{align*}
Thus, $\overline{T}: \overline{\mathcal{F}} \to \overline{\mathcal{F}}$ is a contraction. Since $\left(\overline{F}, d_\infty \right)$ is complete, then $\overline{T}$ has a unique fixed point $\overline{T} f = f$. However, we cannot claim that $\overline{T}f = f'$, since $\overline{T}$ is defined as an extension of $Tg = g'$ to the space $\overline{\mathcal{F}}$ of continuous functions. 

However, we can argue as follows. If $g \in \mathcal{F} \subset \overline{\mathcal{F}}$ is any function, with the contraction principle, the iterations $\mathcal{F} \ni g_n = g^{(n)} = T^n g = \overline{T}^n g \rightrightarrows f$ converge uniformly to the unique fixed point of $\overline{T}$. Note that 
\begin{align*}
    g_n' = g^{(n+1)} = g_{n+1} \rightrightarrows f.
\end{align*}
Since $g_n \rightrightarrows f$, then $g_n' \rightrightarrows f$, and thus $f$ is differentiable and $f = f'$. For the completeness, 
\begin{align*}
    f(x) - f(0) \leftarrow g_n(x) - g_n(0) = \int^x_0 g_n'(t)\, dt \to \int^x_0 f_n'(t)\, dt,
\end{align*}
and then $\displaystyle f(x) - f(0) = \int^x_0 f_n'(t)\, dt$. Thus, $f'(x) = f(x)$ for $x \in [0,1]$, and hence $f(x) = Ce^x$ for some $C \in \mathbb{R}$. Indeed, 
\begin{align*}
    \left(f e^{-x} \right)' = f' e^{-x} - f e^{-x} = 0,
\end{align*}
and then $f e^{-x} = C$ for some $C$. Now we proved that if $g \in \mathcal{F}$, then
\begin{align*}
    \mathcal{F} \ni g_n = g^{n} \rightrightarrows C e^x.
\end{align*}
\end{proof}

\medskip

\begin{exercise}{\rm *}
Let $f: \mathbb{R}^n \to \mathbb{R}^n$ be a mapping of class $C^1$. Prove that there is an open and dense set $\Omega \subset \mathbb{R}^n$ such that the function $R(x) = \rank Df(x)$ is locally constant on $\Omega$, i.e., it is constant in a neighborhood of every point $x \in \Omega$.
\end{exercise}
\begin{proof}
Let $\Omega$ be the set of points where $R(x) = \rank Df(x)$ attains a local maximum, that is
\begin{align*}
    \Omega = \left\{x \,|\, \exists \varepsilon_1 > 0, \forall y \in B^n(x, \varepsilon), \rank Df(y) \leq \rank Df(x) \right\}.
\end{align*}
\begin{enumerate}[label=(\alph*)]
    \item We claim that $\Omega$ is open and $\rank Df$ is locally constant in $\Omega$. Indeed, $\{\rank Df \geq k\}$ is open \footnote{If $\rank Df(x) \geq k$, then the determinant of a certain $k\times k$ minor of $Df(x)$ is nonzero. Then we can choose a point $y \in B(x, \varepsilon)$ such that $\rank Df(y) \geq k$, since $\det$ function is continuous.}, so if $x \in \Omega$ and $\rank Df(x) = k$, then $\rank Df(y) \geq k$ in $B(x, \varepsilon_2)$. But $\rank Df$ attains locally maximum at $x$, then $\rank Df(y) = k$ in $B^n(x, \varepsilon)$, where $\varepsilon = \min \{\varepsilon_1, \varepsilon_2\}$. In particular, $B^n(x, \varepsilon) \subset \Omega$. This proves that $\Omega$ is open in $\Omega$ and $\rank Df$ is locally constant in $\Omega$. 
    
    \item It remains to prove that $\Omega$ is dense, Let $B^n(x, \varepsilon) \subset \mathbb{R}^n$ be any ball. It suffices to show that $\Omega \cap B^n(x, \varepsilon) \neq \varnothing$. Since $\rank Df$ attains only finitely many values, then it attains a global maximum in $B^n(x, \varepsilon)$, which is a local maximum in $\mathbb{R}^n$, so $\Omega \cap B^n(x, \varepsilon) \neq \varnothing$.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}{\rm *}
Let $\Phi: \mathbb{R}^2 \to \Phi\left(\mathbb{R}^2\right) \subset \mathbb{R}^2$ be a diffeomorpism. Prove that
\begin{align*}
    \int_{B^2(0,1)} \left\|D\Phi \right\| = \int_{\Phi(B^2(0,1))} \left\|D\left(\Phi^{-1}\right) \right\|,
\end{align*}
where $\|A\| = \left(\sum^2_{i,j=1} a_{ij}^2\right)^{1/2}$ is the Hilbert-Schmidt norm of the matrix.\\
{\bf Hint:} Compare $\|A\|$ and $\left\|A^{-1}\right\|$ for a $2 \times 2$ matrix.
\end{exercise}
\begin{proof}
For $A = \begin{pmatrix} 
    a & b \\
    c & d
\end{pmatrix}$, we have $\|A\| = \left(a^2 + b^2 + c^2 + d^2\right)^{1/2}$, then $A^{-1} = \frac{1}{ad - bc}\begin{pmatrix} 
    d & -b \\
    -c & a
\end{pmatrix}$ and thus $\left\|A^{-1}\right\| = \frac{\left(a^2 + b^2 + c^2 + d^2\right)^{1/2}}{ad - bc}$. 

Now we consider $\Phi: \mathbb{R}^2 \to \Phi\left(\mathbb{R}^2\right) \subset \mathbb{R}^2$, and for $x \in \mathbb{R}^2$, there exists $y \in \mathbb{R}^2$, $\Phi(x) = y$. Then, with 
\begin{align*}
    \int_{B^2(0,1)} (f \circ \Phi)\left|J_\Phi\right| = \int_{\Phi\left(B^2(0,1)\right)} f,
\end{align*}
taking $f = \left\|D\left(\Phi^{-1}\right) \right\|$ gives
\begin{align*}
    \int_{\Phi\left(B^2(0,1)\right)} \left\|D\left(\Phi^{-1}\right) \right\| = \int_{B^2(0,1)} \left\|D\left(\Phi^{-1}\right) (\Phi(x)) \right\| \cdot \left|J_\Phi\right|.
\end{align*}

Then it suffices to show that
\begin{align*}
    \left\|D\left(\Phi^{-1}\right) (\Phi(x)) \right\| \cdot \left|J_\Phi(x)\right| = \left\|D\Phi(x) \right\|.
\end{align*}
Also, we have $\Phi^{-1} \circ \Phi = I$\footnote{Here $I$ means identity map, like $f(x) = x, x \in \mathbb{R}^n$. Hence, the derivative of $\Phi^{-1} \circ \Phi$ is identity.}, then $D\left(\Phi^{-1}\right)(\Phi(x)) D\Phi(x) = I$. Then,
\begin{align*}
    D\left(\Phi^{-1}\right)(\Phi(x)) = \left(D\Phi(x)\right)^{-1},
\end{align*}
and then multiplying both sides by $\left|J_\Phi\right|$ gives
\begin{align*}
    \left\|D\left(\Phi^{-1}\right) (\Phi(x)) \right\| \cdot \left|J_\Phi(x)\right| = \left\|\left(D\Phi(x)\right)^{-1} \right\| \left|J_\Phi(x)\right|.
\end{align*}

Then it remains to show that 
\begin{align*}
    \left\|\left(D\Phi(x)\right)^{-1} \right\| \left|J_\Phi(x)\right| = \left\|D\Phi(x) \right\|.
\end{align*}
With above results, we have $\left\|A^{-1}\right\| |\det A|= \|A\|$, and the above equality follows.
\end{proof}


\newpage

\section{May 2018 Exam}


\begin{exercise}
Prove that every real-valued function, continuous on the real interval $[0,1]$, is the uniform limit of continuous piecewise linear functions.
\end{exercise}
\begin{proof}
For any continuous function $f:[0,1] \to \mathbb{R}$, we can separate $[0,1]$ into $n$ pieces, with step length $1/n$. Denote by $[x_j, x_{j+1}], 1 \leq j \leq n$ the interval $\left[j-1/n, j/n\right]$. We define linear function $f_j: [x_j, x_{j+1}] \to \mathbb{R}$ by
\begin{align*}
    f_j(x) = f(x_j) + \frac{f(x_{j+1}) - f(x_j)}{1/n} x,
\end{align*}
and then we can define a continuous piecewise linear function
\begin{align*}
    \widetilde{f}(x) = \begin{cases}
        f_1(x), & x \in [x_1, x_2], \\
        \quad \vdots \\
        f_n(x), & x \in [x_n, x_{n+1}].
    \end{cases}
\end{align*}

It remains to show that $f(x)$ converges uniformly to $\widetilde{f}(x)$. For any $\varepsilon > 0$, then for $x_j$, there exists a $\delta_j > 0$, such that if $|x - x_j| < \delta_j$, then $|f(x) - f(x_j)| < \varepsilon$. Now we can pick $N > 0$ such that $1/N < \min_{j}\{\delta_j\}$, i.e., the step length is less than $\min_{j}\{\delta_j\}$. Then, for any $n > N$, and any $x \in [0,1]$, we can find a $j$ such that $x \in [x_j, x_{j+1}]$. Then,
\begin{align*}
    \left|\widetilde{f}(x) - f(x)\right| & = \left|f_j(x) - f(x)\right| \\
    & \leq \left|f_j(x) -  f_j(x_{j+1})\right| + \left|f_j(x_{j+1}) - f(x)\right| \\
    & \leq \left|f_j(x_j) -  f_j(x_{j+1})\right| + \left|f(x_{j+1}) - f(x)\right| \\
    & \leq 2 \varepsilon,
\end{align*}
where in the last step, we used the continuity of linear function $f_j(x)$ in the interval $[x_j, x_{j+1}]$. Thus, $f$ converges uniformly to a continuous piecewise linear function.
\end{proof}

\medskip

\begin{exercise}
~\begin{enumerate}[label=(\alph*)]
    \item State a theorem that says under what considitions we can differentiate a function series term by term, i.e., $f_n:(a,b) \to \mathbb{R}$,
    \begin{align*}
        \left(\sum^\infty_{n=1}f_n(x) \right)' = \sum^\infty_{n=1} f_n'(x).
    \end{align*}
    
    \item Prove that if $a > 1$ and $k \geq 1$, then
    \begin{align*}
        \sum^\infty_{n=2} \frac{(\log n)^k}{n^a} < \infty.
    \end{align*}
    
    \item Prove that the function
    \begin{align*}
        \zeta(x) = \sum^\infty_{n=1} \frac{1}{n^x}, x > 1,
    \end{align*}
    is infinitely differentiable in $(1, \infty)$.
\end{enumerate}
{\bf Hint:} You can use part (b) to prove (c), even if you do not know how to prove (b).
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Suppose that $f_n:(a,b) \to \mathbb{R}$ has continuous derivatives on $(a,b)$ and \footnote{\begin{intheorem}{\rm \cite{10}}
    Suppose that $f_n:[a,b] \to \mathbb{R}$ has continuous derivatives on $[a,b]$ and further that
    \begin{enumerate}[label=(\alph*)]
        \item the series $\displaystyle \sum^\infty_{n=1} f_n(x)$ converges at some point $x_0 \in (a, b)$,
        
        \item the series of derivatives $\displaystyle \sum^\infty_{n=1} f_n'(x)$ converges uniformly on $[a,b]$.
    \end{enumerate}
    Then,
    \begin{enumerate}[label=(\alph*)]
        \item the series $\displaystyle \sum^\infty_{n=1} f_n(x)$ converges at every $x \in [a,b]$ and the sum $\displaystyle \sum^\infty_{n=1} f_n(x)$ is differentiable with $\displaystyle \left( \sum^\infty_{n=1} f_n(x) \right)' = \sum^\infty_{n=1} f_n'(x)$ for each $x \in [a,b]$,
        
        \item the convergence of $\displaystyle \sum^\infty_{n=1} f_n(x)$ is uniform on $[a,b]$.
    \end{enumerate}
    \end{intheorem}}
    \begin{enumerate}[label=\arabic*)]
        \item the series $\displaystyle \sum^\infty_{n=1} f_n(x)$ converges at some point $x_0 \in (a, b)$,
        
        \item the series of derivative $\displaystyle \sum^\infty_{n=1} f_n'(x)$ converges uniformly on $(a,b)$.
    \end{enumerate}
    Then the series $\displaystyle \sum^\infty_{n=1} f_n(x)$ converges at every $x \in (a,b)$ and
    \begin{align*}
        \left(\sum^\infty_{n=1}f_n(x) \right)' = \sum^\infty_{n=1} f_n'(x).
    \end{align*}
    
    \item Since $a > 1$, then let $a = 1 + 2b > 1$, where $b > 0$. Then,
    \begin{align*}
        \lim_{n\to\infty} \frac{\frac{(\log n)^k}{n^{1+2b}}}{\frac{1}{n^{1+b}}} = \lim_{n\to\infty} \frac{(\log n)^k}{n^b} = 0,
    \end{align*}
    and to see the last step, we could assume $f(x) = (\log x)^k / x^b$. Since 
    \begin{align*}
        f'(x) = x^{-b-1} (\log x)^{k-1} (k - b \log x),
    \end{align*}
    then letting $x \to \infty$ gives $f'(x) < 0$, which implies that $f(x)$ is decreasing and thus $\lim_{x} f(x) = 0$, which gives $\lim_{n\to\infty} (\log n)^k / n^b = 0$. Now, since $\displaystyle \sum \frac{1}{n^{1+b}}$ converges, so does $\displaystyle \sum \frac{(\log n)^k}{n^a}$.
    
    \item For $x > 1$, 
    \begin{align*}
        \sum^\infty_{n=1} \frac{1}{n^x} < 1 + \sum^\infty_{n=2} \frac{(\log n)^k}{n^x} < \infty,
    \end{align*}
    then $\displaystyle \sum \frac{1}{n^x}$ converges for $x \in (1, \infty)$. Also, since $\displaystyle \left(\frac{1}{n^x}\right)' = - n^{-x} \log n$, then considering in $k = 1$ and $a = x > 1$ part (b) gives
    \begin{align*}
        \sum^\infty_{n=1} \left|\frac{\log n}{n^x} \right| < \infty.
    \end{align*}
    Therefore, by part (a),
    \begin{align*}
        \left(\sum^\infty_{n=1} \frac{1}{n^x}\right)' = \sum^\infty_{n=1} \left(\frac{1}{n^x}\right)'.
    \end{align*}
    
    Now we can continue this process and have
    \begin{align*}
        \left(\sum^\infty_{n=1} \frac{1}{n^x}\right)^{(n)} = \sum^\infty_{n=1} \left(\frac{1}{n^x}\right)^{(n)},
    \end{align*}
    and since $\displaystyle \frac{1}{n^x}$ is infinitely differentiable, so does $\zeta(x)$.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}
Let $f: \mathbb{Q}^n \to \mathbb{R}$ be a function defined on the set $\mathbb{Q}^n \subset \mathbb{R}^n$ consisting of points whose coordinates are rational numbers. Prove that if $f$ satisfies the inequality $\left|f(x) - f(y)\right|^{2018} \leq 2018 \left|x - y\right|$ for all $x, y \in \mathbb{Q}^n$, then there is a unique continuous function $F: \mathbb{R}^n \to \mathbb{R}$ such that $F(x) = f(x)$ for all $x \in \mathbb{Q}^n$. Provide a direct proof without referring to any deep results \footnote{This problem is similar to the following one: Let $f:A\to X$ be a mapping between a dense subset $A \subset \mathbb{R}^n$ and a complete metric space $(X,d)$. Assume that $d(f(x),f(y))\leq |x-y|$ for all $x,y\in A$. Prove that there is a mapping $F:\mathbb{R}^n\to X$ such that $d(F(x),F(y))\leq |x-y|$ for all $x,y\in \mathbb{R}^n$ and $F(x)=f(x)$ whenever $x\in A$.}.
\end{exercise}
\begin{proof}
Since $\mathbb{Q}^n$ is dense in $\mathbb{R}^n$, then for any point $x$ in $\mathbb{R}^n$, we can find a sequence $\{x_n\}^\infty_{n=1}$ of points in $\mathbb{Q}^n$ such that $\displaystyle \lim_{n\to\infty} x_n = x$. Now we define $F(x)$ as
\begin{align*}
    F(x) = \begin{cases}
        \displaystyle \lim_{n\to\infty} f(x_n), & \displaystyle  x \notin \mathbb{Q}^n,  x_n \in \mathbb{Q}^n, \lim_{n\to\infty} x_n = x, \\
        f(x), & x \in \mathbb{Q}^n.
    \end{cases}
\end{align*}
Then, it is natural that $F(x) = f(x)$ for all $x \in \mathbb{Q}^n$. 

It remains to show that $F(x)$ defined above is unique. Suppose that there exists another continuous function $G(x)$ such that $G(x) = f(x)$ for all $x \in \mathbb{Q}^n$, but $G(x) \neq F(x)$ for all $x \notin \mathbb{Q}^n$. However, for $x \notin \mathbb{Q}^n$,
\begin{align*}
    |F(x) - G(x)| \leq |F(x) - f(x_n)| + |f(x_n) - G(x)| \xrightarrow[]{n \to \infty} 0,
\end{align*}
where we used the fact that $G(x)$ is continuous. This is a contradiction.
\end{proof}

\medskip

\medskip

\begin{exercise}{\rm *}
Let $P = \{(x_1, x_2, x_3, x_4): x_4 = 0\}$ be a hyperplane in $\mathbb{R}^4$. Let $\Phi: \mathbb{R}^4 \to \mathbb{R}^4$ be a $C^1$-diffeomorphism of $\mathbb{R}^4$ onto $\mathbb{R}^4$, and let $S = \Phi(P)$ be the image of the hyperplane $P$ under the diffeomorphism $\Phi$. Prove that for every $x \in S$, there is a neighborhood $B(x, \varepsilon) \subset \mathbb{R}^4$ such that the set $S\cap B(x, \varepsilon)$ is a graph of a $C^1$ function of one of the following forms
\begin{align*}
    x_1 = f(x_2, x_3, x_4), \,\,\, {\rm or} \,\,\, x_2 = f(x_1, x_3, x_4), \,\,\, {\rm or} \,\,\, x_3 = f(x_1, x_2, x_4), \,\,\, {\rm or} \,\,\, x_4 = f(x_1, x_2, x_3).
\end{align*}
In the proof you are allowed to use the inverse function theorem or the implicit function function theorem only.
\end{exercise}
\begin{proof}
Define $\Psi = (\Psi_1, \Psi_2, \Psi_3, \Psi_4): \mathbb{R}^4 \to \mathbb{R}^4$ being inverse diffeomorphism such that $\Psi(S) = P$. Then,
\begin{align*}
    S & = \{(x_1, x_2, x_3, x_4): \Psi(x_1, x_2, x_3, x_4) \in P\} \\
    & = \{(x_1, x_2, x_3, x_4): \Psi_4(x_1, x_2, x_3, x_4) = 0\}. 
\end{align*}
Since $\Psi$ is a diffeomorphism, then $\det D\Psi \neq 0$ everywhere. And since 
\begin{align*}
    D\Psi = \begin{pmatrix} 
    \nabla \Psi_1 \\
    \nabla \Psi_2 \\
    \nabla \Psi_3 \\
    \nabla \Psi_4
    \end{pmatrix},
\end{align*}
then $\nabla \Psi_4 \neq 0$ everywhere and therefore, $\nabla \Psi_4 \neq 0$ on the surface $S$. Then we have
\begin{align*}
    \frac{\partial \Psi_4}{\partial x_i} \neq 0,
\end{align*}
for some $i = 1,2,3,4$. Thus, by implicit function theorem, for every $x \in S$, there is a neighborhood $B(x, \varepsilon)$ such that the set $S \cap B(x, \varepsilon)$ is a graph of a $C^1$ function of the form 
\begin{align*}
    x_i = f(x_1, \cdots, \hat{x}_i, \cdots, x_4),
\end{align*}
for some $i = 1,2,3,4$, where $\hat{x}_i$ indicates that $x_i$ is ignored.
\end{proof}

\medskip

\begin{exercise}{\rm *}
Let $n$ be a positive integer. Denote by $\mathcal{M}_n$ the space of all real $n \times n$ matrices. By $A^T \in \mathcal{M}_n$ denote the transpose of a matrix $A \in \mathcal{M}_n$ and by $tA, t \in \mathbb{R}$, the matrix where all components of $A$ are multiplied by $t$. Prove that if $A \in \mathcal{M}_n$, then there is $B \in \mathcal{M}_n$ and $\varepsilon > 0$ such that $\varepsilon A = B + B^T B$.\\
{\bf Hint:} Differentiate the mapping $F: \mathcal{M}_n \to \mathcal{M}_n$ defined by $F(X) = X + X^TX$.
\end{exercise}
\begin{proof}
Let $F(X) = X + X^TX$, and we need to find $DF(0)$. It suffices to find a map $L:\mathcal{M}_n \to \mathcal{M}_n$ such that
\begin{align*}
    \lim_{H \to 0} \frac{F(H) - F(0) - L(H)}{\|H\|} = 0.
\end{align*}
Taking $L(H) = H$ gives
\begin{align*}
    \lim_{H \to 0} \left\|\frac{H + H^TH - H}{H}\right\| = \lim_{H \to 0} \frac{\left\|H^TH \right\|}{\|H\|} \leq \lim_{H \to 0} \|H\| = 0.
\end{align*}
Then, $DF(0) = I$. By inverse function theorem, $F$ is a diffeomorphism near $0 \in \mathcal{M}_n$. Then, $F$ maps a neighorhoood $U \subset \mathcal{M}_n$, where $0 \in U$ onto a neighorhood $V \in \mathcal{M}_n$, that is also $0 \in V$. Then, for any $A \in \mathcal{M}_n$, there is a $\varepsilon > 0$ small enough such that $\varepsilon \|A\| < \delta$, where $\delta$ is the radius of a ball contained in $V$, that is, $B^{n^2}(0,\delta) \subset V$. Thus, $A \in V$ and there exists $B \in U$ such that $\varepsilon A = B + B^T B$.
\end{proof}

\medskip

\begin{exercise}
Let $f$ be a polynomial of total degree at most three in $(x, y, z) \in \mathbb{R}^3$.  Prove that:
$$
\int_{x^2 + y^2 + z^2 \le 1} f(x, y, z)\hspace{1.5pt} dx\hspace{1.5pt}  dy \hspace{1.5pt} dz =
\frac{4\pi f((0, 0, 0)) }{3} +  \frac{2\pi \left(\Delta f\right)((0, 0, 0))}{15}.
$$
Here $\displaystyle \Delta = \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} + \frac{\partial^2}{\partial z^2}$ is the Laplacian operator on $\mathbb{R}^3$.
\end{exercise}
\begin{proof}
By Taylor's formula, 
\begin{align*}
    f(x) = f(0) + \sum^3_{i=1} \frac{\partial f}{\partial x_i}(0) x_i + \frac{1}{2} \sum^3_{i,j=1} \frac{\partial^2 f}{\partial x_i \partial x_i}(0)x_i x_i + \frac{1}{6} \sum^3_{i,j,k=1} \frac{\partial^3 f}{\partial x_i \partial x_i \partial x_k}(0)x_i x_i x_k + R.
\end{align*}
and since $f$ is a polynomial of degree at most $3$, then the remainder $R = 0$. Also, with the property of odd function,
\begin{align*}
    \int_B x_i\, dx = 0,\quad \int_B x_i x_i x_k\, dx = 0,
\end{align*}
and
\begin{align*}
    \int_B x_i x_i\, dx = 0,\,\, i\neq j.
\end{align*}
Then, 
\begin{align*}
    \int_B f(x, y, z)\, dx\,dy\,dz = f(0) \underbrace{\int_B\, dx}_{4\pi/3}  + \frac{1}{2} \sum^3_{i=1} \frac{\partial^2 f}{\partial x^2_i}(0) \int_B x_i^2\, dx,
\end{align*}
also, with 
\begin{align*}
    \int_B x_1^2\, dx = \int_B x_2^2\, dx = \int_B x_3^2\, dx & = \frac{1}{3} \int_B |x|^2\, dx \\
    & = \int^1_0 \int_{S(r)} r^2\, d\sigma\, dr \\
    & = \int^1_0 r^2 \cdot 4\pi r^2\, dr = \frac{4\pi}{5},
\end{align*}
the proof is completed. 
\end{proof}


\newpage

\section{August 2017 Exam}

\begin{exercise}{\rm *}\label{Aug_2017_1}
For any real number $x$ and $y$, define
\begin{align*}
    I_y(x) = \int^{\pi}_{\frac{\pi}{2}} \left(e^{-t^2 x} \cos (ty) - e^{t^2 y} \sin (tx) \right)\, dt.
\end{align*}
Prove that there exists $\varepsilon > 0$ such that for each $y \in (-\varepsilon, \varepsilon)$, there is some $x \in (0, 1)$ (depending on $y$) with $I_y(x) = 0$.
\end{exercise}
\begin{proof}
Define $F: \mathbb{R}^2 \to \mathbb{R}$ by $F(x,y) = I_y(x)$. Then, $F \in C^1$ and also we have
\begin{align*}
    F(0,0) & = \int^{\pi}_{\frac{\pi}{2}} \left(e^{0} \cos (0) - e^{0} \sin (0) \right)\, dt = \frac{\pi}{2} > 0, \\
    F\left(\frac{1}{2}, 0\right) & = \int^{\pi}_{\frac{\pi}{2}} \left(e^{-t^2/2} \cos (0) - e^{0} \sin \left(\frac{t}{2}\right) \right)\, dt \\
    & = \int^{\pi}_{\frac{\pi}{2}} \left(e^{-t^2/2} - \sin \left(\frac{t}{2}\right) \right)\, dt < 0.
\end{align*}
Indeed, to see the second inequality, let $f(t) = e^{-t^2/2} - \sin (t/2)$, and then for $t \in [\pi/2, \pi]$,
\begin{align*}
    f'(t) = - te^{-t^2/2} - 1/2 \cos \left(\frac{t}{2}\right) < 0,
\end{align*}
and since $f(\pi/2) < 0$, the result follows. 

By the intermediate value theorem, for function $F(x, 0)$, there exists $x_0 \in \left(0, 1/2\right)$ such that $F(x_0, 0) = 0$. Since $tx_0 \in (0, \pi/2)$, then
\begin{align*}
    \frac{\partial F}{\partial x}(x_0, 0) = \int^{\pi}_{\frac{\pi}{2}} \left(- t^2 e^{-t^2 x_0} - t \cos (tx_0) \right)\, dt < 0.
\end{align*}
Then by implicit function theorem, there is a unique $C^1$ function $x = g(y)$ near $(x_0, 0)$ such that $F(g(y), y) = 0$, that is, there exists $\varepsilon > 0$ such that for each $y \in (-\varepsilon, \varepsilon)$, there is some $x \in (0,1)$ such that $I_y(x) = 0$.
\end{proof}

\begin{proof}[Second Proof of Exercise \ref{Aug_2017_1}]
Since 
\begin{align*}
    \lim_{y \to 0} \frac{2 \cos \left(\frac{3\pi}{4}y\right) \sin \left(\frac{\pi}{4}y\right)}{y} = \frac{\pi}{2} > 0,
\end{align*}
then there exists $\varepsilon_1 > 0$ such that for $y \in (-\varepsilon_1, \varepsilon_1), y \neq 0$, we have
\begin{align*}
    \frac{2 \cos \left(\frac{3\pi}{4}y\right) \sin \left(\frac{\pi}{4}y\right)}{y} > 0.
\end{align*}

Also, we have
\begin{align*}
    \lim_{y \to 0} e^{-\frac{\pi^2}{4}} \cos \left(\frac{\pi}{2} y\right) - e^{t^2 y} = e^{-\frac{\pi^2}{4}} - 1 < 0,
\end{align*}
then there exists $\varepsilon_2 > 0$ such that for $y \in (-\varepsilon_2, \varepsilon_2), y \neq 0$, we have
\begin{align*}
    e^{-\frac{\pi^2}{4}} \cos \left(\frac{\pi}{2} y\right) - e^{t^2 y} < 0.
\end{align*}

Now let $\varepsilon = \min\left\{\varepsilon_1, \varepsilon_2, 1/2 \right\}$, then for $y \in (-\varepsilon, \varepsilon), y \neq 0$, we have
\begin{align*}
    I_y(0) = \int^{\pi}_{\frac{\pi}{2}} \cos (ty) \, dt = \frac{\sin(ty)}{y} \Bigg|^{\pi}_{\frac{\pi}{2}} = \frac{2 \cos \left(\frac{3\pi}{4}y\right) \sin \left(\frac{\pi}{4}y\right)}{y} > 0,
\end{align*}
also, $I_0(0) = \pi/2$. So, for $y \in (-\varepsilon, \varepsilon)$, $I_y(0) > 0$.

Let 
\begin{align*}
    f(t) = e^{-t^2} \cos (ty) - e^{t^2 y} \sin t,
\end{align*}
then, when $y \in (-\varepsilon, \varepsilon)$,
\begin{align*}
    f\left(\frac{\pi}{2}\right) = e^{-\frac{\pi^2}{4}} \cos \left(\frac{\pi}{2} y\right) - e^{t^2 y} < 0.
\end{align*}
And then $t \in [\pi/2, \pi], y \in (-\varepsilon, \varepsilon)$, we have
\begin{align*}
    f'(t) = - 2t e^{-t^2} \cos (ty) - y e^{-t^2} \sin (ty) - ty e^{t^2y} \sin t - e^{t^2 y} \cos t < 0.
\end{align*}
Then $f(t)$ is monotonically decreasing on $[\pi/2, \pi]$. Then, $f(t) \leq f(\pi/2) < 0$. Therefore, 
\begin{align*}
    I_y(1) = \int^{\pi}_{\frac{\pi}{2}} \left( e^{-t^2} \cos (ty) - e^{t^2 y} \sin t \right)\, dt < 0.
\end{align*}

Since for any $y \in (-\varepsilon, \varepsilon)$, $I_y(x)$ is continuous on $[0,1]$, then by the intermediate value theorem, there exists $x_0 \in [0,1]$ such that $I_y(x_0) = 0$.
\end{proof}

\medskip

\begin{exercise}
For any closed subset $X$ of $\mathbb{R}^n$, prove that there is a countable subset $S$ of $X$ such that every continuous function $f: X \to \mathbb{R}$ is determined by its values on $S$.
\end{exercise}
\begin{proof}
Let $\{B_k\}$ be the family of open balls, whose centers have rational numbers, with radius $1/k, k \in \mathbb{N}$. Then, $B_k$ is countable, since the set of rational numbers is countable.

For certain $B \in \{B_k\}$ such that $B \cap X \neq \varnothing$, let $X_k = \{x: x \in B \cap X\}$. Then $\{X_k\}$ is countable, therefore $\displaystyle S = \bigcup^\infty_{k=1} X_k$ is a countable subset of $X$.

Since $X$ is closed, then $\overline{S} \subset X$. Also, for any $x \in X$, there exists $k \in \mathbb{N}$, such that $x$ belongs to a certain $B \in \{B_k\}$, and $B$ contains a point $\widetilde{x}$ of $X_k$, hence of $S$. Since $k$ is arbitrary, then there exists a sequence $\{\widetilde{x}_k\} \in S$ such that $\lim_{k\to\infty} \widetilde{x}_k = x$, then $x \in \overline{S}$. Thus, $X \subset \overline{S}$. 

Thus, $X = \overline{S}$, and every continuous function $f: X \to \mathbb{R}$ is determined by its values on $S$.
\end{proof}

\medskip

\begin{exercise}{\rm *}
Let $\displaystyle f(x) = \ln x / x$, defined for any real number $x > 0$.
\begin{enumerate}[label=(\alph*)]
    \item Find the coefficients $h_n$ of the Taylor series of $f$, centered at $x = 1$:
    \begin{align*}
        \mathcal{T}(f)(x) = \sum^\infty_{n = 0} h_n (x - 1)^n,
    \end{align*}
    and determine, with proof, all open intervals on which the series $\mathcal{T}(f)$ converges uniformly to $f$.
    
    \item Does $\displaystyle \mathcal{T}(f)(2) = \sum^\infty_{n = 0} h_n$ converges to $\ln 2 / 2$?
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Since $f(x) = \ln x / x$, then $xf(x) = \ln x$. Then, taking derivative on both sides gives,
    \begin{align*}
        & xf(x) + x^2 f'(x) = 1, \\
        & f(x) + 3x f'(x) + x^2 f''(x) = 0, \\
        & 4f'(x) + 5x f''(x) + x^2 f^{(3)}(x) = 0,
    \end{align*}
    and by induction, we could conclude
    \begin{align*}
        n^2 f^{n-1}(x) + (2n + 1)x f^{n}(x) + x^2 f^{(n+1)}(x) = 0.
    \end{align*}
    Then we have equations
    \begin{align*}
        & f(1) + f'(1) = 1, \\
        & f(1) + 3f'(1) + f''(1) = 0, \\
        & 4f'(1) + 5f''(1) + f^{(3)}(1) = 0, \\
        & \cdots \\
        & n^2 f^{n-1}(1) + (2n + 1) f^{n}(1) + f^{(n+1)}(1) = 0.
    \end{align*}
    Since 
    \begin{align*}
        h_n = \frac{f^{(n)}(1)}{n!},
    \end{align*}
    we have
    \begin{align*}
        & h_0 = 0, \\
        & h_0 + h_1 = 0, \\
        & \cdots \\
        & n^2 (n-1)! h_{n-1} + (2n + 1) n! h_n + (n + 1)! h_{n+1} = 0.
    \end{align*}
    Then we have
    \begin{align*}
        & n h_{n-1} + (2n + 1) h_n + (n + 1) h_{n+1} = 0, \\
        \Rightarrow & n (h_{n-1} + h_n) = - (n + 1) (h_n + h_{n+1}) \\
        \Rightarrow & n (h_{n-1} + h_n) = (-1)^{n-1} \\
        \Rightarrow & h_n = (-1)^{n-1} \sum^n_{k=1} \frac{1}{k}.
    \end{align*}
    
    Now, we have 
    \begin{align*}
        \mathcal{T}(f)(x) = \sum^\infty_{n = 0} \left( (-1)^{n-1} \sum^n_{k=1} \frac{1}{k} \right) (x - 1)^n.
    \end{align*}
    And then 
    \begin{align*}
        \lim_{n\to\infty} \left| \frac{h_{n-1}}{h_n} \right| & = \frac{1 + \frac{1}{2} + \cdots + \frac{1}{n-1}}{1 + \frac{1}{2} + \cdots + \frac{1}{n}} = 1, 
    \end{align*}
    thus the Taylor series converges on $(0,2)$.
    
    \item For $x = 2$, we have
    \begin{align*}
        \mathcal{T}(f)(2) = \sum^\infty_{n=0} h_n,
    \end{align*}
    and since $\displaystyle \lim_{n\to\infty} h_n = \infty$, then $\displaystyle \sum^\infty_{n=1} h_n$ does not converges. 
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}
Let $f(x) = \left\lfloor x \right\rfloor$ be the greatest integer function, denoted on $\mathbb{R}$ by $f(x) = n$ for all $x \in [n, n+1), n \in \mathbb{Z}$. Show that there is a sequence $\{p_k\}_{k \in \mathbb{N}}$ of polynomials converging pointwise to $f$ on $\mathbb{R}\setminus \mathbb{Z}$, such that for any compact set $K \subset \mathbb{R}$,
\begin{align*}
    \lim_{k\to\infty} \int_K \left| f(x) - p_k(x) \right|\, dx = 0.
\end{align*}
\end{exercise}
\begin{proof}
Define $g_k$ for any $k \in \mathbb{N}$ by
\begin{align*}
    g_k(x) = \begin{cases}
        n, & \displaystyle  x \in \left[n, n+1-\frac{1}{k}\right), \\
        kx - (k-1)(n+1), & \displaystyle  x \in \left[n+1-\frac{1}{k}, n+1\right],
    \end{cases}
\end{align*}
and it is easy to see that $g_k(x)$ is continuous on $\mathbb{R}$. By Weierstrass theorem, for any $k \in \mathbb{N}$, there is a polynomial $p_k$ such that 
\begin{align*}
    \left| p_k(x) - g_k(x) \right| < \frac{1}{k}, x \in [-m, m],
\end{align*}
where $m > 0$ is arbitrary. These $p_k$ form a sequence $\{p_k\}$. 
\begin{enumerate}[label=(\alph*)]
    \item Now for any $x \in \mathbb{R}\setminus \mathbb{Z}$, then there exists $M$ such that $x \in (m, m+1)$. Then for any $\varepsilon > 0$, let $N = \max\left\{\left\lfloor 1/\varepsilon \right\rfloor + 1, 1/(n + 1 - x) \right\}$, then for all $k > N$, we have $x \in [m, m + 1 - 1/k]$, and then
    \begin{align*}
        \left| f(x) - p_k(x) \right| & \leq \left| f(x) - g_k(x) \right| + \left| p_k(x) - g_k(x) \right| \\
        & \leq |m - m| + \frac{1}{k} = \frac{1}{k} < \varepsilon.
    \end{align*}
    Thus, $\{p_k\}$ converges pointwise to $f$ on $\mathbb{R}\setminus \mathbb{Z}$. 
    
    \item Since compact set is bounded and closed, then $K \subset [-M, M]$ for some $M > 0$ and $M \in \mathbb{N}$. For any $\varepsilon > 0$, let $N = \left\lfloor 3M/\varepsilon \right\rfloor + 1$, then for all $k > N$, we have
    \begin{align*}
        \int_K \left| f(x) - p_k(x) \right|\, dx & \leq \int^{M}_{-M} \left| f(x) - p_k(x) \right|\, dx \\
        & \leq \int^{M}_{-M} \left| f(x) - g_k(x) \right|\, dx + \int^{M}_{-M} \left| g_k(x) - p_k(x) \right|\, dx \\
        & \leq \sum^{M-1}_{k=-M} \frac{1}{2k} + \int^{M}_{-M} \frac{1}{k} \, dx < \frac{3M}{k} < \varepsilon.
    \end{align*}
    Thus, we have
    \begin{align*}
        \lim_{k\to\infty} \int_K \left| f(x) - p_k(x) \right|\, dx = 0.
    \end{align*}
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}
For a fixed $k \in \mathbb{N}$, define $f_k: \mathbb{R}^2 \to \mathbb{R}$ by
\begin{align*}
    f_k(x,y) = \begin{cases}
        \displaystyle \frac{x^2(x + y^2)}{x^2 + y^{2k}}, & {\rm if}\,\, (x,y) \neq (0,0), \\
        0, & {\rm if}\,\, (x,y) = (0,0).
    \end{cases}
\end{align*}
Prove that $f_1$ is not differentiable at $(0,0)$, but $f_k$ is differentiable at $(0,0)$ for each $k \geq 0$.\\
{\bf Hint:} At some point it may help to separately consider the case $|x| \geq |y|^k$ and $|x| < |y|^k$.
\end{exercise}
\begin{proof}
The partial derivative at $(0,0)$ is 
\begin{align*}
    f_{k_x}(0,0) & = \lim_{x \to 0} \frac{f_k(x,0) - f_k(0,0)}{x} = \lim_{x \to 0}\frac{x - 0}{x} = 1, \\
    f_{k_y}(0,0) & = \lim_{y \to 0} \frac{f_k(0,y) - f_k(0,0)}{x} = \lim_{y \to 0}\frac{0 - 0}{y} =  0.
\end{align*}
Also, we have
\begin{align*}
    \left| \frac{f_k(s,t) - f_k(0,0) - f_{k_x}(0,0)s - f_{k_y}(0,0)t}{\sqrt{s^2 + t^2}} \right| = \frac{\left|s^2t^2 - st^{2k}\right|}{(s^2 + t^{2k}) \sqrt{s^2 + t^2}}.
\end{align*}
~\begin{enumerate}[label=(\alph*)]
    \item For $k = 1$, when $s = mt$ for some $m \in \mathbb{R}$ and $m \neq 0$, we have
    \begin{align*}
        \lim_{(s,t)\to (0,0)} \frac{\left|s^2t^2 - st^2\right|}{(s^2 + t^2) \sqrt{s^2 + t^2}} & = \lim_{t\to 0} \frac{\left|m^2t^4 - mt^3\right|}{(m^2 + 1)^{\frac{3}{2}} t^3} \\
        & = \lim_{t\to 0} \left| \frac{m^2}{(m^2 + 1)^{\frac{3}{2}}} t - \frac{m}{(m^2 + 1)^{\frac{3}{2}}} \right| \\
        & = \frac{|m|}{(m^2 + 1)^{\frac{3}{2}}} \neq 0,
    \end{align*}
    thus $f_1(x,y)$ is not differentiable at $(0,0)$.
    
    \item For $k \geq 2$, with $|a - b| \leq \sqrt{2(a^2 + b^2)}$ and $a^2 + b^2 \geq 2ab$, we have
    \begin{align*}
        \frac{\left|s^2t^2 - st^{2k}\right|}{(s^2 + t^{2k}) \sqrt{s^2 + t^2}} & = \frac{\left|s^2t^2 - st^{2k}\right|}{(s^2 + t^{2k})^{\frac{3}{2}}} \\
        & \leq \frac{\left|s^2t^2 - st^{2k}\right|}{ (2st^k)^{\frac{3}{2}}} \\
        & \leq \frac{\left|2s^4t^4 + 2s^2t^{4k}\right| }{(2st^k)^{\frac{3}{2}}}\\
        & \leq \frac{\left|2s^2t^4 \left( s^2 + t^{4k-4} \right) \right|}{(2st^k)^{\frac{3}{2}}} \\
        & \leq \frac{\left|2s^2t^4 \left( 2st^{2k-2} \right) \right|}{(2st^k)^{\frac{3}{2}}} \\
        & = \left| \sqrt{2} s^{\frac{3}{2}} t^{\frac{1}{2}k+2} \right| \xrightarrow[]{(s,t)\to(0,0)} 0,
    \end{align*}
    thus $f_k(x,y)$ is differentiable at $(0,0)$.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}{\rm *}
The quaternionic square of $\mathbf{x} = (x,y,z,w) \in \mathbb{R}^4$ is 
\begin{align*}
    s(\mathbf{x}) = \left( x^2 - y^2 - z^2 - w^2, 2xy, 2xz, 2xw \right).
\end{align*}
For any $\mathbf{b} \in \mathbb{R}^4$ such that $\left|\mathbf{b}\right| \leq 1/16$, prove that the equation
\begin{align*}
    s(\mathbf{x}) - \mathbf{x} + \mathbf{b} = 0
\end{align*}
has a unique solution $\mathbf{x}$ such that $\left|\mathbf{x}\right| \leq 1/8$.
\end{exercise}
\begin{proof}
If $\left|\mathbf{x}\right| \leq 1/16$  and $\left|\mathbf{b}\right| \leq 1/16$, then 
\begin{align*}
    |s(\mathbf{x}) + \mathbf{b}| & \leq |s(\mathbf{x})| + |\mathbf{b}| \\
    & = \sqrt{(x^2 + y^2 + z^2 + w^2)^2 - 4(y^2z^2 + y^2w^2 + z^2w^2)} + |\mathbf{b}| \\
    & \leq \sqrt{(x^2 + y^2 + z^2 + w^2)^2} + |\mathbf{b}| \\
    & = |\mathbf{x}| + |\mathbf{b}| \\
    & \leq \left(\frac{1}{8}\right)^2 + \frac{1}{16} < \frac{1}{8}.
\end{align*}

Now define $f:B(1/8) \to B(1/8)$ by
\begin{align*}
    f(\mathbf{x}) = s(\mathbf{x}) + b,
\end{align*}
where $B(1/8) = \left\{ \mathbf{x} \in \mathbb{R}^4 : \left|\mathbf{x}\right| \leq 1/8 \right\}$. And $f$ is well defined from above inequality, since $B(1/8)$ is a closed subset of $\mathbb{R}^4$ and then $(B(1/8), d)$ is complete. 

For any $\mathbf{x} = (x,y,z,w) \in B(1/8)$,
\begin{align*}
    (Ds)(\mathbf{x}) = \begin{pmatrix}
        2x & -2y & -2z & -2w \\
        2y & 2x  & 0   & 0   \\
        2z & 0   & 2x  & 0   \\
        2w & 0   & 0   & 2x   
    \end{pmatrix},
\end{align*}
and $x^2 + y^2 + z^2 + w^2 \leq 1/64$. Also, for $\mathbf{y} = (a, b, c, d), \left|\mathbf{y}\right|=1$, we have
\begin{align*}
    \left\|(Ds)(\mathbf{x}) \right\| = & \sup_{|\mathbf{y}|=1} \left|(Ds)(\mathbf{x}) \cdot \mathbf{y}\right| \\
    = & \sup_{|\mathbf{y}|=1} 2 \sqrt{(ax - by - cz - dw)^2 + (ay + bx)^2 + (az + cx)^2 + (aw + dx)^2} \\
    \leq & \sup_{|\mathbf{y}|=1} 2 \sqrt{\left(|x| + |y| + |z| + |w|\right)^2 + (x + y)^2 + (x + z)^2 + (x + w)^2} \\
    \leq & \sup_{|\mathbf{y}|=1} 2 \sqrt{\left(x^2 + y^2 + z^2 + w^2\right) + 2 \left(|xy| + |xz| + |xw| + |yz| + |yw| + |zw|\right)} \\
    &  + 2 \sqrt{(x + y)^2 + (x + z)^2 + (x + w)^2} \\
    & \leq 2 \sqrt{\frac{1}{64} + 2 \times \frac{6}{64} + 3 \times \frac{3}{64}} = \frac{11}{16} < 1.
\end{align*}
Then, for any $\mathbf{x}, \mathbf{y} \in B(1/8)$, by mean value theorem,
\begin{align*}
    \left|f(\mathbf{x}) - f(\mathbf{y})\right| = \left|s(\mathbf{x}) - s(\mathbf{y})\right| \leq \sup_{\mathbf{x} \in B(1/8)} \left\|(Ds)(\mathbf{x}) \right\| \cdot \left|\mathbf{x} - \mathbf{y}\right| \leq \frac{11}{16} \left|\mathbf{x} - \mathbf{y}\right|,
\end{align*}
which means $f$ is a contraction. By the Banach Fixed-Point Theorem, $f(\mathbf{x}) = \mathbf{x}$ has a unique solution. That is, for any $\mathbf{b} \in \mathbb{R}^4$ such that $\left|\mathbf{b}\right| \leq 1/16$, the equation
\begin{align*}
    s(\mathbf{x}) - \mathbf{x} + \mathbf{b} = 0
\end{align*}
has a unique solution $\mathbf{x}$ such that $\left|\mathbf{x}\right| \leq 1/8$.
\end{proof}



\newpage

\section{May 2017 Exam}

\begin{exercise}{\rm *}\label{May_2017_1}
Suppose $f:[1, \infty) \to \mathbb{R}$ is Riemann integrable on $[1, A]$ for every $A > 1$, and $\displaystyle \lim_{x \to \infty} xf(x) = 1$. Prove that
\begin{align*}
    \lim_{t \to \infty} \frac{1}{t} \int^{e^t}_{1} f(x)\, dx = 1.
\end{align*}
\end{exercise}
\begin{proof}
Since $\displaystyle \lim_{x \to \infty} xf(x) = 1$, then for any $\varepsilon > 0$, there exists $x_0 > 1$ such that for $x > x_0$, $|xf(x) - 1| < \varepsilon$.

Now let
\begin{align*}
    t = \max \left\{\ln x_0, \frac{\int^{x_0}_{1} \frac{|xf(x) - 1|}{x}\, dx}{\varepsilon} \right\} + 1,
\end{align*}
then we have
\begin{align*}
    \left|\frac{1}{t} \int^{e^t}_{1} f(x)\, dx - 1\right| & = \frac{1}{t} \left|\int^{e^t}_{1} f(x)\, dx - \int^{e^t}_{1} \frac{1}{x}\, dx \right| \\
    & \leq \frac{1}{t} \int^{e^t}_{1} \frac{|xf(x) - 1|}{x}\, dx \\
    & \leq \frac{1}{t} \int^{x_0}_{1} \frac{|xf(x) - 1|}{x}\, dx + \frac{1}{t} \int^{e^t}_{x_0} \frac{|xf(x) - 1|}{x}\, dx \\
    & < \varepsilon + \frac{1}{t} \int^{e^t}_{x_0} \frac{\varepsilon}{x}\, dx \\
    & = \varepsilon + \frac{1}{t} \varepsilon (t - \ln x_0) < 2 \varepsilon.
\end{align*}
Thus, we have
\begin{align*}
    \lim_{t \to \infty} \frac{1}{t} \int^{e^t}_{1} f(x)\, dx = 1.
\end{align*}
\end{proof}

\medskip

\begin{exercise}{\rm *}
~\begin{enumerate}[label=(\alph*)]
    \item If every member of s sequence $\left\{f_n\right\}$ of functions on $[0,1]$ is $K$-Lipschitz for a fixed $K > 0$, that is, if $\left|f_n(x) - f_n(y)\right| \leq K |x - y|$ for all $x , y \in [0,1]$ and all $n \in \mathbb{N}$. Show that $\left\{f_n\right\}$ converges pointwise if and only if it converges uniformly.
    
    \item Prove that
    \begin{align*}
        \lim_{n\to\infty} \int^1_0 n \log \left(1 + \frac{x}{n}\right)\, dx = \frac{1}{2}.
    \end{align*}
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item \begin{enumerate}[label=\arabic*)]
        \item ($\Rightarrow$) Since $\left\{f_n\right\}$ converges pointwise to a function $f$, then for any $x \in [0,1]$ and any $\varepsilon > 0$, there exists $N_{x, \varepsilon} \in \mathbb{N}$ such that for all $n > N_{x, \varepsilon}$, 
        \begin{align*}
            \left| f_n(x) - f(x) \right| < \frac{\varepsilon}{3}.
        \end{align*}
        Then for any $x, y \in [0,1]$ and $\varepsilon > 0$, there exists an $N_{x,y} = \max \left\{N_{x, \varepsilon}, N_{y, \varepsilon}\right\}$, such that, 
        \begin{align*}
            |f(x) - f(y)| & \leq \left|f(x) - f_{N_{x,y}}(x)\right| + \left|f_{N_{x,y}}(x) - f_{N_{x,y}}(y)\right| + \left|f_{N_{x,y}}(y) - f(y)\right| \\
            & < \frac{\varepsilon}{3} + K |x - y| + \frac{\varepsilon}{3} = \frac{2 \varepsilon}{3} + K |x - y|.
        \end{align*}
        And if $y \in \left(x - \varepsilon / (3K), x + \varepsilon / (3K) \right)$, then for $n > N_{x,y}$, we have 
        \begin{align*}
            |f(x) - f(y)| < \frac{2 \varepsilon}{3} + K \frac{\varepsilon}{3K} = \varepsilon.
        \end{align*}
    
        Since $[0,1]$ is compact, then $[0,1]$ can be covered by finite subcoverings, that is there exists $M > 0$ and $M \in \mathbb{N}$ such that
        \begin{align*}
            [0,1] \subset \bigcup^M_{j=1} B \left(x_j, \frac{\varepsilon}{3K} \right),
        \end{align*}
        where $B \left(x_j, \varepsilon / (3K)\right) = \left(x_j - \varepsilon / (3K), x_j + \varepsilon / (3K) \right)$ and $x_j \in [0,1]$.
    
        Now, for any $\varepsilon > 0$, let $N = \max \left\{ N_{x_j, \varepsilon} \right\}, j = 1, \cdots, M$, then for any $x \in [0,1]$, there exists some $i$ such that $x \in B \left(x_i, \varepsilon / (3K)\right)$. Then, for all $n > N$, we have
        \begin{align*}
            |f_n(x) - f(x)| & \leq |f_n(x) - f_n(x_j)| + |f_n(x_j) - f(x_j)| + |f(x_j) - f(x)| \\
            & < K \frac{\varepsilon}{3K} + \frac{\varepsilon}{3} + \frac{\varepsilon}{3} = \varepsilon.
        \end{align*}
        Thus, $\left\{f_n\right\}$ converges uniformly to a function $f$.
        
        \item ($\Leftarrow$) If $\left\{f_n\right\}$ converges uniformly to a function $f$, then it is obvious that $\left\{f_n\right\}$ converges pointwise to $f$.
    \end{enumerate}
    
    \item Let $f_n(x) = n \log \left(1 + \frac{x}{n}\right)$, and for any $x \in [0,1]$,
    \begin{align*}
        \lim_{n \to \infty} f_n(x) = \lim_{n \to \infty} n \log \left(1 + \frac{x}{n}\right) = \lim_{n \to \infty} n \log \left(1 + \frac{x}{n}\right)^{\frac{x}{n} \cdot \frac{n}{x}} = \lim_{n \to \infty} n \frac{x}{n} = x,
    \end{align*}
    which implies that $f_n(x)$ converges pointwise to $f(x) = x$. 
    
    For all $x , y \in [0,1]$, by Mean Value Theorem, we have
    \begin{align*}
        |f_n(x) - f_n(y)| & = \left|n \log \left(1 + \frac{x}{n}\right) - n \log \left(1 + \frac{y}{n}\right) \right| \\
        & = n \left| \frac{1}{n + \theta} (x - y) \right| \\
        & = \frac{n}{n + \theta} |x - y|
    \end{align*}
    where $\theta \in (x,y)$ or $\theta \in (y,x)$. Then, $\left\{f_n\right\}$ is $1$-Lipschitz, and by part (a), $\left\{f_n\right\}$ converges uniformly to $f(x) = x$. Thus,
    \begin{align*}
        \lim_{n \to \infty} \int^1_0 n \log \left(1 + \frac{x}{n}\right)\, dx & =  \int^1_0 \lim_{n \to \infty} n \log \left(1 + \frac{x}{n}\right)\, dx = \int^1_0 x \, dx = \frac{1}{2}.
    \end{align*}
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}\label{May_2017_3}
Show that the Hilbert cube
\begin{align*}
    \mathcal{H} = \left\{(x_1, x_2, \cdots) | 0 \leq x_n \leq 2^{-n}, n \in \mathbb{N} \right\}
\end{align*}
is compact when it is equipped with the metric $d(\mathbf{x}, \mathbf{y}) = \sup \{\left|x_n - y_n\right|\, |\, n \in \mathbb{N}\}$, where $\mathbf{x} = (x_1, x_2, \cdots)$ and $\mathbf{y} = (y_1, y_2, \cdots)$.\\
{\bf Hint:} Use a diagonal argument.
\end{exercise}
\begin{proof}
Let $\{\mathbf{x}_k\}^\infty_{k=1}$ be a sequence in $\mathcal{H}$, where $\mathbf{x}_k = \left(x^k_1, x^k_2, \cdots\right)$.

Since $\{x^k_1\}^\infty_{k=1}$ is a sequence in $\left[0, 2^{-1}\right]$, which is a compact set in $\mathbb{R}$, it has convergent subsequence $\left\{x^{n_{1,k}}_1\right\}^{\infty}_{k=1}$, which converges to limit $x_1 \in \left[0, 2^{-1}\right]$. Then, $\left\{\mathbf{x}^{n_{1,k}}\right\}$ is a subsequenc of $\{\mathbf{x}_k\}$ with 
$$\lim_{k \to \infty} x^{n_{1,k}}_1 = x_1.$$

Similarly, since $\{x^k_2\}^\infty_{k=1}$ is a sequence in $\left[0, 2^{-2}\right]$, which is a compact set in $\mathbb{R}$, it has convergent subsequence $\left\{x^{n_{2,k}}_2\right\}^{\infty}_{k=1}$ with limit $x_2 \in \left[0, 2^{-2}\right]$, where $\{n_{2,k}\}$ is a subsequence of $\{n_{1,k}\}$. Then, $\left\{\mathbf{x}^{n_{2,k}}\right\}$ is a subsequence of $\left\{\mathbf{x}^{n_{1k}}\right\}$ with 
$$\lim_{k \to \infty} x^{n_{2,k}}_2 = x_2.$$

Continue this process and for every $m \in \mathbb{N}$, there is a sequence $\left\{x^{n_{m,k}}_m \right\}$ converging to $x_m \in \left[0, 2^{-m}\right]$. And then $\left\{\mathbf{x}_{n_{m,k}} \right\}$ is a sunsequence of $\{\mathbf{x}_k\}$ with the first $m$ coordinates converging to $x_m$, respectively.

Now use diagonal method and let $\mathbf{y}_m = \mathbf{x}_{n_{m,m}} = \left(x^{n_{m,m}}_1, x^{n_{m,m}}_2, \cdots \right)$, then $\{\mathbf{y}_m\}$ is a subsequence of $\{\mathbf{x}_k\}$. Let $\mathbf{x} = (x_1, x_2,  \cdots)$, and then it is clear that $\mathbf{x} \in \mathcal{H}$. Also, for every $i \in \mathbb{N}$, $\{n_{i+1,k}\}^{\infty}_{k=1}$ is a subsequence of $\{n_{i,k}\}^{\infty}_{k=1}$, and then $\{n_{i,i}\}$ is a subsequence of $\{n_{i,k}\}^{\infty}_{k=1}$. Then, we have
\begin{align*}
    d(\mathbf{y}_m, \mathbf{x}) & = d\left(\mathbf{x}_{n_{m,m}}, \mathbf{x} \right) \\
    & = \sup_{i \in \mathbb{N}} \left|x^{n_{m,m}}_{i} - x_i \right| \\
    & \leq \max \left\{ \sup_{i = 1, \cdots, m-1} \left|x^{n_{m,m}}_{i} - x_i \right|, 2^{-m} \right\},
\end{align*}
and then 
\begin{align*}
    \lim_{m \to \infty} d(\mathbf{y}_m, \mathbf{x}) = 0,
\end{align*}
which implies $\{\mathbf{y}_m\}$ is a subsequence of $\{\mathbf{x}_k\}$ that converges to $\mathbf{x}$. Thus, every sequence in $\mathcal{H}$ has a subsequence that converges to a limit in $\mathcal{H}$, so $\mathcal{H}$ is compact with the metric defined above.
\end{proof}

\medskip

\newpage
\begin{proof}[Second Proof of Exercise \ref{May_2017_3}]{\rm *}
It suffice to show that $\mathcal{H}$ is complete and totally bounded.
\begin{enumerate}[label=(\alph*)]
    \item Let $\{\mathbf{x}_k\}$ be a Cauchy sequence in $\mathcal{H}$, where $\mathbf{x}_k = \left(x^k_1, x^k_2, \cdots\right)$. 

    Then, for any $\varepsilon > 0$, there exists $N \in \mathbb{N}$ such that for all $n, m > N$, we have $d(\mathbf{x}_n, \mathbf{x}_m) < \varepsilon$. Then, we have $\left|x^n_i - x^m_i \right| < \varepsilon$ for all $i = 1,2,\cdots$. Thus, for any fixed $i$, $\left\{x^k_i\right\}^{\infty}_{k=1}$ is a Cauchy sequence in $\left[0, 2^{-i}\right]$. Since $\left[0, 2^{-i}\right]$ is complete, then $\left\{x^k_i\right\}^{\infty}_{k=1}$ converges to some $x_i \in \left[0, 2^{-i}\right]$. 

    Now let $\mathbf{x} = (x_1, x_2, \cdots)$, then $\mathbf{x} \in \mathcal{H}$, and we have
    \begin{align*}
        \lim_{k \to \infty} d(\mathbf{x}_k, \mathbf{x}) = \lim_{k \to \infty} \sup_{i \in \mathbb{N}} \left|x^k_i - x_i\right| = 0,
    \end{align*}
    which implies that every Cauchy sequence in $\mathcal{H}$ converges to a limit in $\mathcal{H}$. Thus, $\mathbb{H}$ is complete.
    
    \item For any $\varepsilon > 0$, there exists $M \in \mathbb{N}$ such that $2^{-M} < \varepsilon$. For any $n < N, n \in \mathbb{N}$,  we have 
    \begin{align*}
        \left[0, 2^{-n}\right] \subset \bigcup_{x \in \left[0, 2^{-i}\right]} B(x, \varepsilon),
    \end{align*}
    and since $\left[0, 2^{-i}\right]$ is compact, then there exist finite coverings $\displaystyle \bigcup^{K_n}_{i=1} B(x_{n, i}, \varepsilon)$, where $K_n \in \mathbb{N}$ such that 
    \begin{align*}
        \left[0, 2^{-n}\right] \subset \bigcup^{K_n}_{i=1} B \left(x^{(n)}_i, \varepsilon \right),
    \end{align*}
    where $x^{(n)}_i \in \left[0, 2^{-n}\right]$.
    
    Let $A_n = \left\{x^{(n)}_1, x^{(n)}_2, \cdots, x^{(n)}_{K_n}\right\}$, and define 
    \begin{align*}
        H_\varepsilon = \left\{(x_1, x_2, \cdots) | x_i \in A_i, i = 1, \cdots, M; x_i = 0, i > M \right\}.
    \end{align*}
    For any $\mathbf{x} = (x_1, x_2, \cdots) \in \mathcal{H}$, there exists $\mathbf{y} = \left(x^{(1)}_{k_1}, x^{(2)}_{k_2}, \cdots, x^{(M)}_{k_M}, 0, 0, \cdots \right) \in H_{\varepsilon}$, such that 
    \begin{align*}
        d(\mathbf{x}, \mathbf{y}) & = \max \left\{\sup_{i = 1, \cdots, M} \left|x_i - x^{(i)}_{k_i} \right|, \sup_{i > M} \left| x_i \right| \right\} \\
        & < \max \left\{\varepsilon, 2^{-M} \right\} \leq \varepsilon.
    \end{align*}
    Then, $\mathcal{H}$ is covered by a finite coverings $\{B(\mathbf{x}, \varepsilon) | \mathbf{x} \in H_\varepsilon\}$, since $H_\varepsilon$ has $\displaystyle \prod^M_{n=1} K_n$ elements. Thus, $\mathcal{H}$ is totally bounded.
\end{enumerate}
Hence, since $\mathcal{H}$ is complete and totally bounded, it is compact with the metric defined above.
\end{proof}

\medskip

\begin{exercise}
~\begin{enumerate}[label=(\alph*)]
    \item Prove that the series below 
    \begin{align*}
        f(x) = \sum^\infty_{n=0} \frac{nx}{1 + n^4x^2}
    \end{align*}
    converges uniformly on $[a, \infty)$ for any $a > 0$.
    
    \item Now for a square $N$(i.e. with $\sqrt{N} \in \mathbb{N}$), show that 
    \begin{align*}
        f\left(\frac{1}{N}\right) \geq \frac{N}{2} \sum_{n \geq \sqrt{N}} \frac{1}{n^3},
    \end{align*}
    and using an integral to estimate the sum, show that $f\left(1/N\right) \geq 1/4$. Conclude that the series does not converge uniformly on $\mathbb{R}$.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item For $x \geq a$ and $n \geq 0$, we have
    \begin{align*}
        \left|\frac{nx}{1 + n^4x^2}\right| \leq \frac{nx}{n^4x^2} = \frac{1}{n^3x} \leq \frac{1}{an^3},
    \end{align*}
    and by Weierstrass $M$-test\footnote{{\bf Weierstrass $M$-test:} Suppose that $f_n: A \to \mathbb{R}$, $A \subset \mathbb{R}$, is a sequence of function such that $|f_n(x)| \leq a_n$ for all $x \in A$ and $n \in \mathbb{N}$. If the series $\displaystyle \sum^\infty_{n=1}a_n$ converges, then the series $\displaystyle \sum^\infty_{n=1}f_n(x)$ converges uniformly.}, since $\displaystyle \sum^\infty_{n=1} \frac{1}{an^3}$ converges, then the series $f(x)$ converges uniformly.
    
    \item First, since the series converges uniformly on $[1/N, \infty)$, then $f(1/N)$ is well-defined. Then,
    \begin{align*}
        f\left(\frac{1}{N}\right) = \sum^\infty_{n=0} \frac{nN}{N^2 + n^4} = \sum^\infty_{n=0} \frac{N}{\frac{N^2}{n} + n^3} \geq \sum^\infty_{n=\sqrt{N}} \frac{N}{\frac{N^2}{n} + n^3} \geq \sum^\infty_{n=\sqrt{N}} \frac{N}{\frac{n^4}{n} + n^3} = \frac{N}{2} \sum^\infty_{n\geq \sqrt{N}} \frac{1}{n^3}.
    \end{align*}
    
    Second, considering the integral form for $\displaystyle \sum^\infty_{n\geq \sqrt{N}} \frac{1}{n^3}$ gives
    \begin{align*}
        f\left(\frac{1}{N}\right) \geq \frac{N}{2} \int^\infty_{\sqrt{N}} \frac{1}{x^3}\, dx \cdot 1 = \frac{N}{2} \frac{1}{2N} = \frac{1}{4},
    \end{align*}
    where $1$ is the length of the partition of the interval $[\sqrt{N}, \infty)$. 
    
    Finally, if the series $f(x)$ converges uniformly on $\mathbb{R}$, then $f(x)$ is continuous at $0$. Then,
    \begin{align*}
        \frac{1}{4} \leq \lim_{N \to \infty} f\left(\frac{1}{N^2}\right) = \lim_{x\to 0} f(x) = f(0) = 0,
    \end{align*}
    which is a contradiction. Thus, the series does not converge uniformly on $\mathbb{R}$.
\end{enumerate}
\end{proof}

\begin{proof}[Second Proof of (a)]\footnote{This proof is similar to the first one, since it is exactly the method used to prove Weierstrass $M$-test.} Since for $x \geq a$ and $n \geq 0$,
\begin{align*}
    \left|\frac{nx}{1 + n^4x^2}\right| \leq \frac{1}{an^3},
\end{align*}
then it suffice to show that the partial sums of the sequence is Cauchy sequence\footnote{Let $A$ be any set and $f_n:A \to \mathbb{R}$ be a sequence of functions. The pointwise and uniform convergence of the series $\displaystyle \sum^\infty_{n=1}f_n(x)$ is defined respectively as the pointwise and the uniform convergence of the sequence of partial sums $s_k(x) = \displaystyle \sum^k_{n=1}f_n(x)$.}. Indeed, since $\displaystyle \sum^\infty_{n=1} \frac{1}{an^3}$ converges, then there exists $N \in \mathbb{N}$ such that 
\begin{align*}
    \sum^\infty_{n=N} \frac{1}{an^3} < \varepsilon.
\end{align*}
Then for $m > k \geq N$ and any $x \geq a$,
\begin{align*}
    \left|\sum^m_{n=0} \frac{nx}{1 + n^4x^2} - \sum^k_{n=0} \frac{nx}{1 + n^4x^2} \right| = \left|\sum^m_{n=k+1} \frac{nx}{1 + n^4x^2}  \right| \leq \sum^m_{n=k+1} \frac{1}{an^3} \leq \sum^\infty_{n=N} \frac{1}{an^3} < \varepsilon.
\end{align*}
Since $\displaystyle f(x) = \lim_{m\to\infty}\sum^m_{n=0} \frac{nx}{1 + n^4x^2}$, letting $m \to \infty$ implies that for all $k > N$ and all $x \geq a$
\begin{align*}
    \left|f(x) - \sum^k_{n=0} \frac{nx}{1 + n^4x^2} \right| < \varepsilon.
\end{align*}
Thus this means that the series $f(x)$ converges uniformly.
\end{proof}

\medskip

\begin{exercise}
For $n \geq 1$, suppose $f:\mathbb{R}^n \to \mathbb{R}$ is second-differentiable, and $\left(D^2f\right)_p$  is positive semidefinite for all $p$, i.e., $\left(D^2f\right)_p(\mathbf{v}, \mathbf{v}) \geq 0$ for all $\mathbf{v} \in \mathbb{R}^n$. Show that $f$ is convex in the sense that for any $p, q \in \mathbb{R}^n$ and $t \in [0,1]$,
\begin{align*}
    f((1-t)p + tq) \leq (1 - t)f(p) + tf(q).
\end{align*}
In particular, you must show this for $n = 1$. \\
{\bf Hint:} In the general case, reduce to the case when $n = 1$.
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item When $n = 1$, without losing generality, suppose $p < q$, and by mean value theorem, there exists $x_1 \in [p, (1-t)p + tq]$ and $x_2 \in [(1-t)p + tq, q]$ such that 
    \begin{align*}
        f((1-t)p + tq) - f(p) & = t(q - p) f'(x_1), \\
        f(q) - f((1-t)p + tq) & = (1 - t)(q - p) f'(x_2).
    \end{align*}
    Also, by mean value theorem, since $f$ is second-differentiable, then
    \begin{align*}
        f((1-t)p + tq) - (1 - t)f(p) - tf(q) & = t(1 - t)(q - p)\left[f'(x_1) - f'(x_2) \right] \\
        & = t(1 - t)(q - p)(x_1 - x_2) f''(\theta),
    \end{align*}
    where $\theta \in [x_1, x_2]$. 
    
    Now, since $f''(x) \geq 0$ for any $x \in \mathbb{R}$, then we have 
    \begin{align*}
        f((1-t)p + tq) - (1 - t)f(p) - tf(q) \leq 0,
    \end{align*}
    and hence $f$ is convex for $n = 1$.
    
    \item For $p, q \in \mathbb{R}^n$, let 
    \begin{align*}
        g(t) & = (1-t)p + tq,\\
        h(t) & = f(g(t)).
    \end{align*}
    By the chain rule, we have
    \begin{align*}
        h'(t) & = (Df)_{g(t)}\cdot (Dg)_t = (Df)_{g(t)}\cdot (q - p), \\
        h''(t) & = \left(D^2f\right)_{g(t)}(q - p, q - p) \geq 0.
    \end{align*}
    By the similar argument in (a), we have
    \begin{align*}
        f((1-t)p + tq) - (1 - t)f(p) - tf(q) \leq 0,
    \end{align*}
    and hence $f$ is convex for $n > 1$.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}
Given $(p, q) \in \mathbb{R}^2$, let $f_{(p, q)}: \mathbb{R} \to \mathbb{R}$ be the polynomial:
\begin{align*}
    f_{(p, q)}(x) = x^3 - px + q.
\end{align*}
\begin{enumerate}[label=(\alph*)]
    \item Prove that for any $(y,z) \in \mathbb{R}^2$, there is a unique $(p, q) \in \mathbb{R}^2$ such that $f_{(p, q)}(y) = f_{(p, q)}(z) = 0$, and that the map $\phi: \mathbb{R}^2 \to \mathbb{R}^2$ taking $(y, z)$ to this $(p, q)$ is smooth.
    
    \item{\rm *} Prove that $\phi$ maps at most six-to-one, and that for $(y_0, z_0) \in \mathbb{R}^2$ with $\phi(y_0, z_0) = (p_0, q_0)$, $\phi$ restricts to a diffeomorphism from a suitable neighborhood of $(x_0, y_0)$ to a neighborhood of $(p_0, q_0)$ if and only if $\phi^{-1}(p_0, q_0)$ contains exactly six points.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item 
    \begin{enumerate}[label=\arabic*)]
        \item If $y \neq z$, then $f_{(p, q)}(y) = f_{(p, q)}(z) = 0$ gives
        \begin{align*}
            p = z^2 + zy + y^2, \quad q = yz^2 + zy^2,
        \end{align*}
        that is, for any $(y,z) \in \mathbb{R}^2, y \neq z$, there is a unique $(p, q) = \left(z^2 + zy + y^2, yz^2 + zy^2\right)$ such that $f_{(p, q)}(y) = f_{(p, q)}(z) = 0$. And thus, $\phi(y, z) = \left(z^2 + zy + y^2, yz^2 + zy^2\right)$ for $y \neq z$.
        
        \item If $y = z$, then extend $\phi(y,z)$ defined above to $y = z$ gives $\phi(y, z) = (3y^2, 2y^3)$. Then, 
        $$y^3 - (3y^2)y + 2y^3 = 0.$$ 
        Thus, $\phi(y,z)$ satisfies $\phi(y, z) = \left(z^2 + zy + y^2, yz^2 + zy^2\right)$ for $y = z$.
    \end{enumerate}
    Hence, $\phi(y, z)$ satisfies $\phi(y, z) = \left(z^2 + zy + y^2, yz^2 + zy^2\right)$ for all $(y, z) \in \mathbb{R}^2$. And it is obvious that $\phi(y, z)$ is smooth. Indeed, $\phi(y, z)$ is differentiable everywhere.
    
    \item 
    \begin{enumerate}[label=\arabic*)]
        \item For any $(p_0, q_0)$, if $\phi(y, z) = (p_0, q_0)$, then 
        \begin{align*}
            z^2 + zy + y^2 = p_0, \quad yz^2 + zy^2 = q_0,
        \end{align*}
        which gives
        \begin{align*}
            y^3 - p_0 y + q_0 = 0.
        \end{align*}
        If $q_0 \neq 0$, then $y$ has at most three solutions. Also, $yz^2 + zy^2 = q_0$ gives
        \begin{align*}
            z = \frac{-y^2 \pm \sqrt{y^4 + 4q_0y}}{2y},
        \end{align*}
        and then $z$ has at most six solutions.
        
        If $q_0 = 0$, then we have solutions
        \begin{align*}
            \begin{cases}
                y = 0 \\
                z = \pm \sqrt{p_0}
            \end{cases}, 
            \begin{cases}
                y = \pm \sqrt{p_0} \\
                z = 0
            \end{cases},
            \begin{cases}
                y = \pm \sqrt{p_0} \\
                z = \mp \sqrt{p_0}
            \end{cases},
        \end{align*}
        and there are also six solutions. Thus, $\phi$ maps at most six-to-one.
        
        \item Since $\phi(y, z) = \left(z^2 + zy + y^2, yz^2 + zy^2\right)$, then 
        \begin{align*}
            |D\phi| & = \det \begin{pmatrix}
                z + 2y & y + 2z \\
                z^2 + 2zy & y^2 + 2yz
            \end{pmatrix} \\
            & = (y - z)\left(2y^2 + 5yz + 2z^2\right) \\
            & = (y - z)(z + 2y)(y + 2z).
        \end{align*}
        Also, $\phi$ restricts to a diffeomorphism if and only if $|D\phi| \neq 0$, that is 
        \begin{align}\label{condition_of_diffeomorphism}
            y \neq z, \quad z \neq -2y, \quad y \neq -2z.
        \end{align}
        If $\phi(y, z) = (p_0, y_0)$, then by (a) we have $f_{(p_0,q_0)}(y) = f_{(p_0,q_0)}(z) = 0$, that is, 
        \begin{align*}
            y^3 - p_0y + q_0 = 0, \quad z^3 - p_0z + q_0 = 0.
        \end{align*}
        
        ($\Rightarrow$) If $\phi$ is a diffeomorphism, then $y \neq z, z \neq -2y, y \neq -2z$. 
        
        When $q_0 = 0$, then it has solution for $p_0 \geq 0$ since $y, z \in \mathbb{R}$. By (a), $p_0 > 0$ gives six solutions for $(y,z)$, which satisfy the condition (\ref{condition_of_diffeomorphism}), and $p_0 = 0$ gives $y = z$, which does not satisfy (\ref{condition_of_diffeomorphism}).
        
        When $q_0 \neq 0$. If $x^3 - p_0x + q_0 = 0$ has only one solution, then $y = z$, which is a contradiction. If it has two solutions, then we have two cases: if $\sqrt{y^4 + 4q_0y} = 0$, then $z = - y/2$, which is a contradiction; and if $\sqrt{y^4 + 4q_0y} > 0$, then $z$ has two solutions $\frac{-y^2 \pm \sqrt{y^4 + 4q_0y}}{2y}$, and $y$ is one of them, which is also a contradiction. If it has three solutions, then $\sqrt{y^4 + 4q_0y} > 0$, and there are six solutions for $(y,z)$.
        
        Thus, $\phi$ being a diffeomorphism implies that $\phi^{-1}(p_0, q_0)$ contains six points.
        
        ($\Leftarrow$) If $\phi^{-1}(p_0, q_0)$ contains six points.
        
        When $q_0 = 0$, we know that there are six solutions for $(y, z)$ if and only if $p_0 > 0$. And every solution satisfies (\ref{condition_of_diffeomorphism}). 
        
        When $q_0 \neq 0$, we know that $x^3 - p_0x + q_0 = 0$ must have three solutions $x_1, x_2, x_3$, and then by Vieta's formula, $\sum^3_{i=1} x_i = 0$. Also, for $i = 1,2,3$, 
        $$x_i^4 + 4 q_0 x_i > 0.$$
        If $x_1 = z_1^- = \frac{-x_1^2 - \sqrt{x_1^4 + 4q_0x_1}}{2x_1}$, then $2x_1^2 = -x_1^2 - \sqrt{x_1^4 + 4q_0x_1}$, and it only holds for $x_1 = 0$, which is a contradiction. If $x_1 = z_1^+ = \frac{-x_1^2 + \sqrt{x_1^4 + 4q_0x_1}}{2x_1}$, then $q_0 = 2x_1^3$, which gives $x_1 = \sqrt[3]{q_0/2}$; now we assume $x_2 = z_1^- = -2x_1$, then we have $x_3 = 0 - x_1 - x_2 = x_1$, which is also a contradiction. Then, $x^3 - p_0x + q_0 = 0$ having three solutions makes sure that $z \neq -2y, y \neq -2z$. 
        
        Thus, $\phi^{-1}(p_0, q_0)$ containing exactly six points implies that $|D\phi| \neq 0$, and hence $\phi$ is a diffeomorphism.
    \end{enumerate}
\end{enumerate}
\end{proof}


\newpage
\section{August 2016 Exam}

\begin{exercise}
Given $x_0 > 0$, define a sequence $\{x_n\}$ recursively by $x_n = 3 \left(\sqrt{x_{n-1} + 1} - 1\right)$ for $n \in \mathbb{N}$. For any such $x_0$, show that $\{x_n\}$ converges, and find its limit.
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item If $x_0 = 3$, then $x_n = 3$ for all $n = 1, 2, \cdots$.
    
    \item If $x_0 > 3$, then $x_n > 3$ for all $n = 1, 2, \cdots$. Also, we have
    \begin{align*}
        x_n - x_{n-1} & = \left(\sqrt{x_{n-1} + 1} - 1\right) - x_{n-1} \\
        & = 3 \frac{(\sqrt{x_{n-1} + 1} - 1)(\sqrt{x_{n-1} + 1} + 1)}{\sqrt{x_{n-1} + 1} + 1} - x_{n-1} \\
        & = \frac{2x_{n-1} - \sqrt{x_{n-1} + 1} x_{n-1} }{\sqrt{x_{n-1} + 1} + 1} < 0,
    \end{align*}
    where in the last step we used that $\sqrt{x_{n-1} + 1} > 2$. Then, when $x_0 > 3$, $\{x_n\}$ is a decreasing sequence with lower bound $3$, and thus it is convergent.
    
    \item If $x_0 \in (0,3)$, then $x_n \in (0,3)$ for all $n = 1, 2, \cdots$. Also, by the similar argument in (b), we have $x_n - x_{n-1} > 0$. Then, when $x_0 \in (0,3)$, $\{x_n\}$ is a increasing sequence with upper bound $3$, and thus it is convergent.
\end{enumerate}
Then, for all $x_0 > 0$, $\{x_n\}$ is a convergent sequence. Now, since $x_n = 3 \left(\sqrt{x_{n-1} + 1} - 1\right)$, letting $n \to \infty$ and assuming $\displaystyle \lim_{n\to\infty} x_n = k$ would gives 
\begin{align*}
    k = 3 \left(\sqrt{k + 1} - 1\right),
\end{align*}
and thus $k = 3$.
\end{proof}

\medskip

\begin{exercise}
Prove that there is an increasing sequence of integers $a_1 < a_2 < \cdots$ such that for every $k \in \mathbb{N}$, the sequence $\{\sin (k a_n)\}^\infty_{n=1}$ converges.
\end{exercise}
\begin{proof}
Since $\{\sin n\}^\infty_{n=1}$ is bounded in $\mathbb{R}$, then it has a convergent subsequence $\left\{\sin n_{i,1} \right\}^\infty_{i = 1}$, where $n_{1,1} < n_{2,1} < \cdots$.

Also, since $\left\{\sin (2n_{i,1}) \right\}^\infty_{i=1}$ is bounded in $\mathbb{R}$, it has a convergent subsequence $\left\{\sin (2n_{i,2}) \right\}^\infty_{i=1}$, where $n_{1,2} < n_{2,2} < \cdots$ and for any $i$, $n_{i, 2} = n_{j,1}$ for some $j > i$. Therefore, for each $k \in \mathbb{N}$, there exist $n_{k,1} < n_{k,2} < \cdots$ such that $\left\{\sin (kn_{i,k}) \right\}^\infty_{i=1}$ converges. And we can continue this process.

Using diagonal argument, we choose $n_{k,k}$, then we have $n_{k,k} > n_{k-1, k-1}$, since $\{n_{i, k}\}$ is a subsequence of $\{n_{i,k-1}\}$. Thus, $\{\sin (k n_{k,k})\}$ converges, where $\{n_{k,k}\}$ is an increasing sequence of integers.
\end{proof}

\medskip

\begin{exercise}{\rm *}
For $n \geq 2$, define $f_n: [0,1] \to [0,1]$ by
\begin{align*}
    f_n(x) = \begin{cases}
        nx, & 0 \displaystyle \leq x \leq \frac{1}{n}, \\
        \displaystyle \frac{n}{n-1}(1 - x), & \displaystyle \frac{1}{n} \leq x \leq 1.
    \end{cases}
\end{align*}
Prove that $\displaystyle \sum^\infty_{n=2} [f_n(x)]^n$ converges pointwise on $[0,1]$ to a function $f(x)$ that is continuous on $(0,1]$, but that the improper integral $\displaystyle \int^1_0 f(x)\, dx$ diverges (the integral is improper at $0$). 
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item For $x = 0$, then $\displaystyle \sum^\infty_{n=2} [f_n(x)]^n = 0$. For any $x \in (0,1]$, there exists $N \in \mathbb{N}$ such that $1 / (N+1) \leq x \leq 1 / N$. Then,
    \begin{align*}
        \sum^\infty_{n=2} [f_n(x)]^n & = \sum^N_{n = 2} \left( \frac{x}{n} \right)^n + \sum^\infty_{n=N+1} \left( \frac{n}{n - 1} (1-x) \right)^n,
    \end{align*}
    where the first term is constant and for second term, by Cauchy test, we have
    \begin{align*}
        \lim_{n \to \infty} \sqrt[n]{\left( \frac{n(1 - x)}{n} \right)^n} = 1 - x < 1,
    \end{align*}
    and then the second term converges. Then, $\displaystyle \sum^\infty_{n=2} [f_n(x)]^n$ converges pointwise on $[0,1]$ to a function $f(x)$.
    
    It remains to show that $f(x)$ is continuous on $(0,1]$, and we need to show that $\displaystyle \sum^\infty_{n=2} [f_n(x)]^n$ converges uniformly on $(0,1]$. For any $x \in (0,1]$, there exists a $K \in \mathbb{N}$ such that $1 / (K+1) \leq x \leq 1 / K$. Then, 
    \begin{align*}
        \left| \sum^K_{n=2} [f_n(x)]^n - \sum^\infty_{n=2} [f_n(x)]^n \right| & = \left| \sum^\infty_{n=K+1} [f_n(x)]^n\right| \\
        & = \left| \sum^\infty_{n=K+1} \left( \frac{n}{n - 1} (1-x) \right)^n \right|,
    \end{align*}
    and by the previous argument, the series $\displaystyle \sum^\infty_{n=2} \left(\frac{n(1-x)}{n-1}\right)^n$ converges. Then, 
    \begin{align*}
        \lim_{K \to \infty} \left| \sum^K_{n=2} [f_n(x)]^n - \sum^\infty_{n=2} [f_n(x)]^n \right| = 0,
    \end{align*}
    which implies that $\displaystyle \sum^\infty_{n=2} [f_n(x)]^n$ converges uniformly on $[1/K, 1]$. Thus, $f(x)$ is continuous on $(0,1]$.
    
    \item For any $k \in \mathbb{N}$, $k \geq 2$, we have
    \begin{align*}
        \int^1_{\frac{1}{k}} f(x)\, dx & = \int^1_{\frac{1}{k}} \sum^\infty_{n=2} [f_n(x)]^n \, dx \\
        & = \sum^k_{n=2} \int^{\frac{1}{n}}_{\frac{1}{k}}(nx)^n \, dx + \sum^k_{n=2} \int^1_{\frac{1}{n}} \left(\frac{n(1-x)}{n-1}\right)^n \, dx + \sum^\infty_{n=k+1} \int^1_{\frac{1}{k}} \left(\frac{n(1-x)}{n-1}\right)^n \, dx \\
        & \geq 0 + \sum^k_{n=2} \frac{n-1}{n(n+1)} + 0 \geq \sum^k_{n=2} \frac{1}{(n-1) + 3 + \frac{2}{n-1}} \\
        & \geq \sum^k_{n=2} \frac{1}{n-1},
    \end{align*}
    and since $\displaystyle \sum^\infty_{n=2} \frac{1}{n-1}$ diverges, the integral diverges.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}
Let $f:A \to X$ be a mapping between a dense subset $A \subset \mathbb{R}^n$ and a complete metric space $(X, d)$. Assume that $d(f(x), f(y)) \leq |x - y|$ for all $x, y \in A$.
\begin{enumerate}[label=(\alph*)]
    \item Prove that there is a mapping $F:\mathbb{R}^n \to X$ such that $d(F(x), F(y)) \leq |x - y|$ for all $x, y \in \mathbb{R}^n$ and $F(x) = f(x)$ whenever $x \in A$.
    
    \item{\rm *} Provide an example showing that the claim in (a) is not true if we do not assume that space $(X, d)$ is complete.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item Since $A$ is dense in $\mathbb{R}^n$, then for any $x \in \mathbb{R}^n$, there is a sequence $\{x_n\} \subset A$ such that
    $$\lim_{n \to \infty} x_n = x.$$
    Now define for any $x \in \mathbb{R}^n$ and $\displaystyle \lim_{n \to \infty} x_n = x$, 
    \begin{align*}
        F(x) = \lim_{n\to\infty} f(x_n).
    \end{align*}
    Then, for any $x, y \in \mathbb{R}^n$, there exist $\{y_n\} \subset A$ such that $\displaystyle \lim_{n\to\infty}y_n = y$, and then we have
    \begin{align*}
        d(F(x), F(y)) \leq d(F(x), f(x_n)) + d(f(x_n), f(y_n)) + d(f(y_n), F(y)).
    \end{align*}
    Since $(X, d)$ is a complete metric space, then any Cauchy sequence in $X$ converges in $X$. Now we have $\{f(x_n)\}$ is a Cauchy sequence in $X$, indeed, for $n, m \to \infty$,
    \begin{align*}
        \lim_{n, m \to \infty} d(f(x_n), f(x_m)) \leq \lim_{n, m \to \infty} |x_n - x_m| = 0.
    \end{align*}
    Thus, by the definition of $F(x)$ and letting $n \to \infty$
    \begin{align*}
        d(F(x), F(y)) & \leq \lim_{n \to \infty} \left( d(F(x), f(x_n)) + d(f(x_n), f(y_n)) + d(f(y_n), F(y)) \right) \\
        & = \lim_{n \to \infty} d(f(x_n), f(y_n)) \\
        & \leq \lim_{n \to \infty} |x_n - x_m| \\
        & = |x - y|,
    \end{align*}
    which implies that $F$ satisfies the condition in the argument (a).
    
    \item Let $A = \mathbb{Q}^n, X = \mathbb{Q}$. Define for $x = (x_1, x_2, \cdots, x_n) \in A$, $f(x) = x_1$. Then we have
    \begin{align*}
        d(f(x), f(y)) = |x_1 - y_1| \leq |x - y|.
    \end{align*}
    
    For $x = (\sqrt{2}, 0, \cdots, 0) \in \mathbb{R}^n$, we have a sequence $\{x_n = (a_n, 0, \cdots, 0)\} \subset A$ such that $\displaystyle \lim_{n \to \infty} a_n = \sqrt{2}$. For $F:\mathbb{R}^n \to X = \mathbb{Q}$, then we have $F(x) = q$ for some $q \in \mathbb{Q}$. Then, we have
    \begin{align*}
        \lim_{n\to\infty} d(F(x), F(x_n)) & = \lim_{n\to\infty} |q - a_n| = |q - \sqrt{2}| \neq 0,
    \end{align*}
    however, $\left|x - x_n\right| \to 0$, then this is a contradiction and we find the example.
    \end{enumerate}
\end{proof}

\medskip

\begin{exercise}
Suppose $f(x,y)$ is a $C^2$ function on the unit disk $\mathbb{D} = \{(x,y)\,|\,x^2+y^2\leq 1\}$ such that for some $M>0$ and all $(x,y)\in\mathbb{D}$,
$$ \left[f_{xx}(x,y)\right]^2 + 2\left[f_{xy}(x,y)\right]^2 + \left[f_{yy}(x,y)\right]^2 \leq M. $$
If $f(0,0) = f_x(0,0) = f_y(0,0) = 0$, show that
$$ \left\lvert \iint_{\mathbb{D}} f(x,y)\,\mathit{dx}\mathit{dy} \right\rvert \leq \frac{\pi\sqrt{M}}{4}. $$
\end{exercise}
\begin{proof}
By Taylor's formula, 
\begin{align*}
    f(x) = f(0) + \sum^2_{i=1} \frac{\partial f}{\partial x_i}(0) x_i + \frac{1}{2} \sum^2_{i,j=1} \frac{\partial^2 f}{\partial x_i \partial x_i}(0)x_i x_j + R,
\end{align*}
we have that for $f(x,y)$,
\begin{align*}
    f(x,y) = f(0,0) + f_x(0,0)x + f_y(0,0)y + \frac{1}{2} f_{xx}(\xi,\mu) x^2 + f_{xy}(\xi,\mu) xy + \frac{1}{2} f_{yy}(\xi,\mu) y^2.
\end{align*}
Then, taking integral gives
\begin{align*}
    \left|\iint_{\mathbb{D}} f(x,y)\, dxdy \right| & \leq \iint_{\mathbb{D}} \left|f(x,y)\right|\, dxdy \\
    & \leq \iint_{\mathbb{D}} \left| \frac{1}{2} f_{xx}(\xi,\mu) x^2 + f_{xy}(\xi,\mu) xy + \frac{1}{2} f_{yy}(\xi,\mu) y^2 \right|\, dxdy \\
    & \leq \iint_{\mathbb{D}} \sqrt{\left[f_{xx}(x,y)\right]^2 + 2\left[f_{xy}(x,y)\right]^2 + \left[f_{yy}(x,y)\right]^2} \sqrt{\frac{x^4}{4} + \frac{x^2y^2}{2} + \frac{y^4}{4}}\, dxdy \\
    & \leq \sqrt{M} \iint_{\mathbb{D}} \sqrt{\frac{r^4}{4}} \, dxdy \\
    & = \sqrt{M} \int^1_0 \int^{2\pi}_0 \sqrt{\frac{r^4}{4}} r \, d\theta dr \\
    & = \frac{\pi \sqrt{M}}{4},
\end{align*}
where we used Cauchy-Schwarz inequality*.
\end{proof}

\medskip

\begin{exercise}
Let $\gamma:\mathbb{R}\to\mathbb{R}^n$ and $\mathbf{v}_i:\mathbb{R}\to\mathbb{R}^n$, $i=1,2,\ldots,n-1$, be $C^\infty$ smooth functions such that
for any $t\in\mathbb{R}$ the vectors
$$
\gamma'(t),\mathbf{v}_1(t),\ldots,\mathbf{v}_{n-1}(t)
$$
form an orthonormal basis of $\mathbb{R}^n$ (here we differentiate $\gamma$ but {\bf do not} differentiate $\mathbf{v}_i$, $i=1,2,\ldots,n-1$).

Consider the mapping $\Phi:\mathbb{R}^n\to\mathbb{R}^n$ defined by
$$
\Phi(x_1,\ldots,x_n)=\gamma(x_n)+\sum_{i=1}^{n-1} x_i\mathbf{v}_i(x_n).
$$
\begin{enumerate}[label=(\alph*)]
    \item Find the derivative $D\Phi(x_1,\ldots,x_n)$;
    
    \item Prove that $\Phi$ is a diffeomorphism in a neighborhood of a point of the form $(0,\ldots,0,x_n)$;
    
    \item Find the limit
	$$ \lim_{r\to 0} \frac{|\Phi(B^n(0,r))|}{|B^n(0,r)|},$$
	where $B^n(0,r)$ denotes the ball of radius $r$ centered at the origin and $|A|$ stands for the volume of the set $A$.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item 
    \begin{align*}
        D\Phi(x_1,\cdots,x_n) = \begin{pmatrix}
            \mathbf{v}_1(x_n) &  &  &  &  \\
            & \mathbf{v}_2(x_n) &  &  &  \\
            &  & \ddots &  & \\
            &  &  & \mathbf{v}_{n-1}(x_n) \\
            &  &  &  & \gamma'(x_n) + \sum^{n-1}_{i=1}x_i \mathbf{v}_i'(x_n)
        \end{pmatrix}.
    \end{align*}
    
    \item 
    \begin{align*}
        D\Phi(0,\ldots, 0, x_n) = \begin{pmatrix}
            \mathbf{v}_1(x_n) &  &  &  &  \\
            & \mathbf{v}_2(x_n) &  &  &  \\
            &  & \ddots &  & \\
            &  &  & \mathbf{v}_{n-1}(x_n) \\
            &  &  &  & \gamma'(x_n)
        \end{pmatrix}.
    \end{align*}
    is an orthogonal matrix and then $\det D\Phi(0,\ldots, 0, x_n) = \pm 1 \neq 0$. Then it remains to show that $\Phi$ is bijection. And this is obvious. Thus, $\Phi$ is a diffeomorphism in a neighborhood of a point of the form $(0,\ldots,0,x_n)$.
    
    \item By the change of variable, we have
    \begin{align*}
        \left|\Phi(B^n(0,r))\right| = \int_{B^n(0,r)} |J_\Phi|\, dA
    \end{align*}
    and $|J_\Phi| \to 1$ as $r \to 0$, Then for any $\varepsilon > 0$, there exists $r$ such that $\left||J_\Phi| - 1\right| < \varepsilon$, and then
    \begin{align*}
        \left|\frac{|\Phi(B^n(0,r))|}{|B^n(0,r)|} - 1\right| = \left|\frac{\int_{B^n(0,r)} |J_\Phi|\, dA}{|B^n(0,r)|} - 1\right|< \varepsilon.
    \end{align*}
    Thus, the limit is equal to $1$.
\end{enumerate}
\end{proof}


\newpage
\section{May 2016 Exam}

\begin{exercise}\label{May_2016_1}
Prove that if a sequence $\{a_n\}^\infty_{n=1}$ of real numbers is convergent to a finite limit, $\lim_{n\to\infty} a_n = g \in \mathbb{R}$, then
\begin{align*}
    \lim_{x\to\infty} e^{-x} \sum_{n=0}^\infty a_n\frac{x^n}{n!} = g.
\end{align*}
\end{exercise}
\begin{proof}
Since $\displaystyle \lim_{n\to\infty} a_n = g$, then for any $\varepsilon > 0$, there exists $N > 0$ such that for all $n > N$, $\left|a_n - g\right| < \varepsilon$. Then, we have
\begin{align*}
    \left|e^{-x} \sum_{n=0}^\infty a_n\frac{x^n}{n!} - g \right| & = \left|e^{-x} \sum_{n=0}^\infty (a_n - g)\frac{x^n}{n!}\right| \\
    & = \left|e^{-x}\sum_{n=0}^N (a_n - g)\frac{x^n}{n!} + e^{-x}\sum_{n=N+1}^\infty (a_n - g)\frac{x^n}{n!} \right| \\
    & \leq \left|e^{-x}\sum_{n=0}^N (a_n - g - \varepsilon)\frac{x^n}{n!} \right| + \left|e^{-x}\sum_{n=0}^N \varepsilon \frac{x^n}{n!} \right| + \left|e^{-x}\sum_{n=N+1}^\infty \varepsilon \frac{x^n}{n!} \right| \\
    & = \left|e^{-x}\sum_{n=0}^N (a_n - g - \varepsilon) \frac{x^n}{n!} \right| + \left| e^{-x}\sum_{n=0}^\infty \varepsilon \frac{x^n}{n!} \right| \\
    & = \left|e^{-x}\sum_{n=0}^N (a_n - g - \varepsilon) \frac{x^n}{n!} \right| + \varepsilon.
\end{align*}
For the first term on the right hand side, by the L’Hopital’s rule, we have
\begin{align*}
    \lim_{x \to \infty} \sum_{n=0}^N \frac{(a_n - g - \varepsilon)x^n}{e^x n!} & = \lim_{x \to \infty} \sum_{n=1}^N \frac{(a_n - g - \varepsilon) x^{n-1}}{e^x (n-1)!} \\
    & = \lim_{x \to \infty} \sum_{n=2}^N \frac{(a_n - g - \varepsilon) x^{n-2}}{e^x (n-2)!} \\
    & \qquad \qquad \vdots \\
    & = \lim_{x \to \infty} \frac{(a_n - g - \varepsilon)}{e^x} = 0.
\end{align*}
Thus, 
\begin{align*}
    \lim_{x \to \infty} \left|e^{-x} \sum_{n=0}^\infty a_n\frac{x^n}{n!} - g \right| < \varepsilon.
\end{align*}
\end{proof}

\medskip

\begin{proof}[Second Proof of Exercise \ref{May_2016_1}]
For the first term on the right hand side, we can interchange the order of limit and summation, since it is smmation of finitely numbers, that is
\begin{align*}
    \lim_{x\to\infty} \left|e^{-x}\sum_{n=0}^N (a_n - g - \varepsilon) \frac{x^n}{n!} \right| = \sum_{n=0}^N \lim_{x\to\infty} (a_n - g - \varepsilon) \frac{x^n}{e^x n!} = 0.
\end{align*}
\end{proof}

\medskip

\begin{exercise}{\rm *}
Let $f: \mathbb{R} \to \mathbb{R}$ be a $C^4$ function such that for all $x, h \in \mathbb{R}$, we have
\begin{align*}
    f(x + h) = f(x) + f'(x)h + \frac{1}{2} f''\left(x + \frac{1}{3}h \right)h^2.
\end{align*}
Show that the forth derivative $f^{(4)}(x) = 0$ for any $x \in \mathbb{R}$.
\end{exercise}
\begin{proof}
Taking the derivative with respect to $x$ gives
\begin{align*}
    f'(x + h) = f'(x) + f''(x)h + \frac{1}{2} f'''\left(x + \frac{1}{3}h \right)h^2,
\end{align*}
and taking the derivative with respect to $h$ gives
\begin{align*}
    f'(x + h) = f'(x) + \frac{1}{6} f'''\left(x + \frac{1}{3}h \right)h^2 + f''\left(x + \frac{1}{3}h \right)h.
\end{align*}
Then we have
\begin{align*}
    f''(x)h + \frac{1}{2} f'''\left(x + \frac{1}{3}h \right)h^2 = \frac{1}{6} f'''\left(x + \frac{1}{3}h \right)h^2 + f''\left(x + \frac{1}{3}h \right)h,
\end{align*}
and then
\begin{align*}
    f''\left(x + \frac{1}{3}h \right)h - f''(x)h = \frac{1}{3} f'''\left(x + \frac{1}{3}h \right)h^2,
\end{align*}
substituting $\frac{1}{3}h$ by $t$ gives
\begin{align*}
    \frac{f''(x + t) - f''(x)}{t} = f'''(x + t).
\end{align*}
Substitute $x$ by $x - t$ and then $t$ by $-t$, then we have
\begin{align*}
    \frac{f''(x + t) - f''(x)}{t} = f'''(x).
\end{align*}
Thus, $f'''(x + t) = f'''(x)$ for any $t$, which implies $f'''(x)$ is constant and hence $f^{(4)}(x) = 0$ for any $x \in \mathbb{R}$.
\end{proof}

\medskip

\begin{exercise}{\rm *}\label{May_2016_3}
Let $f_n: \mathbb{R}^k \to \mathbb{R}^m$ be continuous maps ($n = 1,2, \cdots$). Let $K$ be a compact subset of $\mathbb{R}^k$. Suppose $f_n \rightrightarrows f$ uniformly on $K$. Prove that $S = f(K) \cup \bigcup^\infty_{n=1} f_n(K)$ is compact.
\end{exercise}
\begin{proof}
Since $f_n \rightrightarrows f$ and $f_n$ is continuous, then $f$ is also continuous. It suffices to show that $S$ is bounded and closed.
\begin{enumerate}[label=(\alph*)]
    \item First, we prove that $S$ is bounded. Since $f$ is continuous and $K$ is compact, then we have $f(K)$ is also compact, thus bounded. Since $f_n$ uniformly converges to $f$, then for $\forall\varepsilon > 0$, there exists $N > 0$ and $\delta > 0$ such that for $\forall n \geq N$ and $\forall x\in K$, $\|f_n(x) - f(x)\| \leq \varepsilon$. Then this also holds for $\varepsilon = 1$ for $n \geq N$. Then $\bigcup^\infty_{n = N}f_n(K)$ is also bounded since it is the set of all points that within distance $1$ to a compact set $f(K)$.  Also, $\bigcup^{N-1}_{n = 0}f_n(K)$ is also bounded since it is finite sum of compact sets.
    
    \item Second, we prove that $S$ is closed. For every sequence $\{y_i\}^\infty_{i=1}\in S$ such that $y_i\to y$, we need to prove that $y\in S$. If infinitely many $y_i$'s belong to $f(K)$ or $f_n(K)$ for some $n\in\mathbb{N}$, then $y_i$ converges to a point in $f(K)$ or $f_n(K)$ since both are compact sets, which implies $y\in S$. 
    
    Otherwise, if every $f_n(K)$ only contians finite components of $\{y_i\}$, then there is a subsequence $\{y_{i_j}\}^\infty_{j = 1}$ such that $y_{i_j}\in f_{n_{i_j}}(K)$, and $y_{i_j} = f_{n_{i_j}}(x_{i_j}), x_{i_j}\in K$. Since $K$ is compact, then $x_{i_j}$ has a convergent subsequence $\{x_{i_{j_l}}\}$ such that $x_{i_{j_l}} \to x \in K$. And since $f_n$ uniformly converges to $f$, then we have 
    \begin{align*}
        y\leftarrow y_{i_{j_l}} = f_{n_{i_{j_l}}}\left(x_{i_{j_l}} \right) \to f(x)\in f(K) \subset S
    \end{align*}
    Thus, $y = f(x)\in S$.
\end{enumerate}
\end{proof}

\medskip

\begin{proof}[Second Proof of Exercise \ref{May_2016_3}]
Since $f_n \rightrightarrows f$ and $f_n$ is continuous, then $f$ is also continuous. Then $f(K)$ is compact. It suffices to show that every sequence in $S$ has a subsequence that converges to point in $S$. Suppose $\{y_k\} \in S$ is a sequence with $\displaystyle \lim_{k\to\infty} y_k = y \in \mathbb{R}^m$. 

If infinitely many $y_i$'s belong to $f(K)$ or $f_n(K)$ for some $n$, then $y \in S$, since every sequence in a compact set $f(K)$ or $f_n(K)$ has a subsequence that converges to point in $f(K)$ or $f_n(K)$.

If there are only finite number of $y_i$'s belong to $\bigcup^\infty_{n=m} f_n(K)$ for some $m \in \mathbb{N}$. Suppose $y_{k_1} \in f_{m_1}(K)$ for some $m_1$, then there exists $m_2 > m_1$, $k_2 > k_1$, such that $y_{k_2} \in f_{m_2}(K)$. And there exists a sequence $\{x_j\} \in K$ such that $f_{m_j}(x_j) = y_{k_j}$. Since $K$ is compact, then $\{x_j\}$ has a convergent subsequence $\{x_{n_j}\}$ which converges to a point $x \in K$. Then,
\begin{align*}
    |y - f(x)| & \leq \left|y - f_{m_{n_j}}(x_{n_j}) \right| + \left|f_{m_{n_j}}(x_{n_j}) - f(x_{n_j}) \right| + \left| f(x_{n_j}) - f(x) \right|, \\
    & = \left|y - y_{k_{n_j}} \right| + \left|f_{m_{n_j}}(x_{n_j}) - f(x_{n_j}) \right| + \left| f(x_{n_j}) - f(x) \right|.
\end{align*}
Letting $j \to \infty$, and we can have $\left|f_{m_{n_j}}(x_{n_j}) - f(x_{n_j}) \right| \to 0$ by uniform convergence, also we have $\left| f(x_{n_j}) - f(x) \right| \to 0$ by the continuity of $f$. Then, $\left|y - f(x)\right| \leq \left|y - y_{k_{n_j}} \right| \to 0$. Thus, $y = f(x) \in f(K) \subset S$, and hence $S$ is compact.
\end{proof}

\medskip

\begin{exercise}{\rm *}
Let $D = \left\{(x, y)\, | \, x^2 + y^2 < 1\right\}$ be the unit disk in $\mathbb{R}^2$. Let $f, g \in C^2(D)$ be such that $g$ is bounded on $D$, $f(x,y) \to + \infty$ as $x^2 + y^2 \to 1$, and moreover, $\Delta f = e^f$ and $\Delta g \geq e^g$ at all points of $D$. Here $\displaystyle \Delta = \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2}$ is the Laplacian. Show that $f(x, y) \geq g(x,y)$ for any $(x, y) \in D$.
\end{exercise} 
\begin{proof}
Let
\begin{align*}
    F(x, y) = \begin{cases}
        \displaystyle e^{g-f}, & \displaystyle x^2 + y^2 < 1 \\
        0, & \displaystyle x^2 + y^2 = 1
    \end{cases}
\end{align*}
then $F$ is continuous. Let $\Omega = \left\{(x, y)\, | \, x^2 + y^2 < s \right\}$ for some $0 < s < 1$ such that $F(x, y) \leq 1/2$ on $D \setminus \Omega$. Since $\overline{\Omega}$ is compact and $F$ is continuous on $\overline{\Omega}$, then $F$ attains its maximum on $\overline{\Omega}$, that is, there exists $(x_0, y_0) \in \overline{\Omega}$ such that $F(x,y) \leq F(x_0,y_0)$ for all $(x, y) \in \overline{\Omega}$.

If $(x_0,y_0) \in \partial \overline{\Omega}$, then for all $(x, y) \in D$, $F(x, y) \leq F(x_0, y_0) \leq 1/2$. Thus, $e^{g - f} < 1$, which implies $f > g$ for all $(x, y) \in D$.

If $(x_0,y_0) \in \Omega$, then we have
\begin{align*}
    F_x(x_0, y_0) = 0, \quad F_y(x_0, y_0)  = 0,
\end{align*}
and
\begin{align*}
    D^2 F(x_0, y_0) & = F_{xx}(x_0, y_0) F_{yy}(x_0, y_0) - F_{xy}^2(x_0, y_0) \geq 0, \\
    F_{xx}(x_0, y_0) & \leq 0.
\end{align*}
Then we have $F_{yy}(x_0, y_0) \leq 0$. Also, since
\begin{align*}
    F_{xx} & = e^{g-f}(g_x - f_x)^2 + e^{g-f}(g_{xx} - f_{xx}), \\
    F_{yy} & = e^{g-f}(g_y - f_y)^2 + e^{g-f}(g_{yy} - f_{yy}),
\end{align*}
then we have
\begin{align*}
    F_{xx}(x_0, y_0) & = e^{g-f}(g_{xx}(x_0, y_0) - f_{xx}(x_0, y_0)), \\
    F_{yy}(x_0, y_0) & = e^{g-f}(g_{yy}(x_0, y_0) - f_{yy}(x_0, y_0)).
\end{align*}
Then,
\begin{align*}
    F_{xx}(x_0, y_0) + F_{yy}(x_0, y_0) = e^{g-f}(\Delta e^g(x_0, y_0) - \Delta f(x_0, y_0)) \leq 0.
\end{align*}
Since
\begin{align*}
    e^{g-f}(\Delta g(x_0, y_0) - \Delta f(x_0, y_0)) \geq e^{g-f}(e^{g(x_0, y_0)} - e^{f(x_0, y_0)}),
\end{align*}
which implies $f(x_0, y_0) \geq g(x_0, y_0)$. Then, for all $(x, y) \in \overline{\Omega}$, $e^{g-f} \leq e^0 = 1$. Thus, for all $(x, y) \in D$, $e^{g-f} \leq e^0 = 1$ and hence $f(x, y) \geq g(x, y)$.
\end{proof}

\medskip

\begin{exercise}
Let $F(x, y) = e^x y^3 + 2x^2 y^2 - y \cos x + 2 \sin x$, $(x, y) \in \mathbb{R}^2$. Prove that there exist functions $f, g, h \in C^\infty$ defined on an open neighborhood $U \subset \mathbb{R}$ of $0$, such that $F(x,f(x)) = F(x,g(x)) = F(x,h(x)) = 0$ and $f(x) < g(x) < h(x)$ for every $x \in U$. Find $f'(0), g'(0)$ and $h'(0)$.
\end{exercise}
\begin{proof}
Since $F(0,y) = y^3 - y$, then $F(0,y) = 0$ has three roots $y_1 = -1, y_2 = 0, y_3 = 1$. And since $F_y(x, y) = 3e^xy^2 + 4x^2y - \cos x$, then
\begin{align*}
    F_y(0, y_1) = 2 > 0, \quad F_y(0, y_2) = -1 < 0, \quad F_y(0, y_3) = 2 > 0.
\end{align*}
By implicit function theorem, there exists an unique function $y = f(x), f \in C^\infty$ such that $F(x, f(x)) = 0$ on the neighborhood $B_1$ of $(0, -1)$. Let $U_1 = \{x \, |\, \left|f(x) + 1\right| < 1/2, (x, y) \in B_1\}$.

Similarly, there exists $U_2, U_3$ and $g, h$ such that
\begin{align*}
    F(x, g(x)) = 0, \quad F(x, h(x)) = 0,
\end{align*}
and 
\begin{align*}
    U_2 & = \left\{x \, \Bigg|\, \left|g(x) - 0\right| < \frac{1}{2}, (x, y) \in B_2\right\}, \\
    U_3 & = \left\{x \, \Bigg|\, \left|h(x) - 1\right| < \frac{1}{2}, (x, y) \in B_3\right\}.
\end{align*}
Let $U = U_1 \cap U_2 \cap U_3$, then for $x \in U$,
\begin{align*}
    f(x) < -\frac{1}{2} < g(x) < \frac{1}{2} < h(x).
\end{align*}
By the chain rule, we have
\begin{align*}
    F_x(x, f(x)) + F_y(x, f(x))f'(x) = 0,
\end{align*}
and then letting $x = 0, y = -1$ gives $f'(0) = - 1/2$. Similarly, $g'(0) = 2, h'(0) = - 3/2$.
\end{proof}

\medskip

\begin{exercise}{\rm *}
Suppose that smooth functions $f_k: \mathbb{R}^k \to \mathbb{R}$ are defined for $k = 1, 2, \cdots, 9$. Let $\Phi = (\phi_1, \cdots, \phi_{10}): \mathbb{R}^{10} \to \mathbb{R}^{10}$ be a mapping defined by
\begin{align*}
    \phi_1(x_1, \cdots, x_{10}) & = x_1 \\
    \phi_2(x_1, \cdots, x_{10}) & = 2x_2 + f_1(x_1) \\
    \phi_3(x_1, \cdots, x_{10}) & = 3x_3 + f_2(x_1, x_2) \\
    & \cdots \\
    \phi_{10}(x_1, \cdots, x_{10}) & = 10x_{10} + f_9(x_1, \cdots, x_9).
\end{align*}
\begin{enumerate}[label=(\alph*)]
    \item Prove that $\Phi$ is a diffeomorphism of $\mathbb{R}^{10}$ onto an open subset of $\mathbb{R}^{10}$.
    
    \item Find the volume of $\Phi((-1,1)^{10})$.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item We have
    \begin{align*}
        J\Phi(x) = \begin{vmatrix}
            1 & 0 & 0 & \cdots & 0 \\
            \frac{\partial f_1}{\partial x_1} & 2 & 0 & \cdots & 0 \\
            \frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & 3 & \cdots & 0 \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            \frac{\partial f_9}{\partial x_1} & \frac{\partial f_9}{\partial x_2} & \frac{\partial f_9}{\partial x_3} & \cdots & 10
        \end{vmatrix} = 10!,
    \end{align*}
    then $J\Phi(x) \neq 0$.
    
    Also, for $(x_1, \cdots, x_{10}), (y_1, \cdots, y_{10}) \in \mathbb{R}^{10}$ such that $\Phi(x_1, \cdots, x_{10}) = \Phi(y1, \cdots, y_{10})$, then by the definition of $\Phi$, we can have $(x_1, \cdots, x_{10}) = (y_1, \cdots, y_{10})$. Then, $\Phi$ is one-to-one. Therefore, $\Phi$ is invertible\footnote{Actually, by $J\Phi(x) \neq 0$, we can immediately know that $\Phi$ is a diffeomorphism and invertible in a neighborhood of any point $(x_1,\cdots, x_{10}) \in \mathbb{R}^{10}$. We do not need to check if it is one-to-one, since it is obvious.}, and thus, $\Phi$ is a diffeomorphism of $\mathbb{R}^{10}$ onto $\Phi(\mathbb{R}^{10})$.
    
    Since $\Phi$ is invertible in a neighborhood $U_x$ of any point $x = (x_1,\cdots, x_{10}) \in \mathbb{R}^{10}$, and $\phi^{-1}$ is continuous $\Phi(U_x)$, then $(\Phi^{-1})^{-1}(U_x) = \Phi(U_x)$ is also open. Then,
    \begin{align*}
        \Phi(\mathbb{R}^{10}) = \Phi \left( \bigcup_{x \in \mathbb{R}^{10}} U_x \right) = \bigcup_{x \in \mathbb{R}^{10}} \Phi \left(U_x \right)
    \end{align*}
    is also open.
    
    \item The volume of $\Phi((-1,1)^{10})$ is
    \begin{align*}
        V = \left| \int_{(-1,1)^{10}} |J\Phi(x)|\, dx \right| = 2^{10}10!.
    \end{align*}
\end{enumerate}
\end{proof}


\newpage
\section{August 2015 Exam}

\begin{exercise}{\rm *}\label{August_2015_1}
Let $f:(-\infty, \infty) \to \mathbb{R}$ be continuous and $\displaystyle \lim_{x \to \infty} f(f(x)) = \infty$. Prove that $\displaystyle \lim_{x \to \infty} |f(x)| = \infty$.
\end{exercise}
\begin{proof}
Suppose $\displaystyle \lim_{x\to\infty}\left|f(x)\right| \leq M <\infty$, which is finite. Then there exists a sequence $\{x_n\}\to\infty$, such that $\left|f(x)\right|\leq M$. Since the sequence $\{f(x_n)\}$ is bounded, then there exists a subsequcne $\{x_{n_k}\}\to\infty$ such that $\displaystyle \lim_{x_{n_k}\to\infty}f(x_{n_k}) = a$. Since $f$ is continuous, then we have $\displaystyle \lim_{x_{n_k}\to\infty}f(f(x_{n_k})) = f(a)$, which is a contradiction. The proof is complete.
\end{proof}

\medskip

\begin{proof}[Second Proof of Exercise \ref{August_2015_1}]
Suppose $\displaystyle \lim_{x\to\infty}\left|f(x)\right| \neq \infty$, that is, there exists $M > 0$ such that for all $N \in \mathbb{N}$, $\exists x > N$, $\left|f(x)\right| \leq M$. 

Let $N_1 = 1$, there exists $x_1 > 1$. Let $N_2 = [x_1] + 1$, then there exists $x_2 > N_2 > x_1$. Continue this process and we have a sequence $\{x_n\} \to \infty$, such that $\left|f(x_n)\right| \leq M$. Also, $f$ is continuous, then there exists $M_1 > 0$ such that if $x \in [-M, M]$, then $\left| f(x) \right| \leq M_1$. 

Since $f(x_n) \in [-M, M]$, then $f(f(x_n)) \leq M_1$, which is a contradiction.
\end{proof}

\medskip

\begin{exercise}\label{August_2015_2}
Let $f_n: X \to \mathbb{R}, n = 1, 2, \cdots$ be a sequence of continuous on a metric space $X$ such that the series $\displaystyle \sum^\infty_{n=1} f_n(x)$ converges for all $x \in X$ and
\begin{align*}
    \sup_{x \in X} \left( \sum^\infty_{n=1} f_n(x)^2 \right)^{\frac{1}{2}} < \infty.
\end{align*}
Prove that if a sequence of real numbers $c_n, n = 1, 2, \cdots$ satisfies $\displaystyle \sum^\infty_{n=1} c_n^2 < \infty$, then the series 
\begin{align*}
    \sum^\infty_{n=1} c_n f_n(x)
\end{align*}
converges everywhere to a continuous function.
\end{exercise}
\begin{proof}
Since for all $x \in X$, 
\begin{align*}
    \sup_{x \in X} \left( \sum^\infty_{n=1} f_n(x)^2 \right)^{\frac{1}{2}} < \infty,
\end{align*}
then there exists $M > 0$ such that for all $x \in X$,
\begin{align*}
    \sup_{x \in X} \left( \sum^\infty_{n=1} f_n(x)^2 \right)^{\frac{1}{2}} < M.
\end{align*}
Also, since $\displaystyle \sum^\infty_{n=1} c_n^2 < \infty$, then for any $\varepsilon > 0$, there exists $N \in \mathbb{N}$ such that for all $m > n > N$, we have
\begin{align*}
    \left| \sum^m_{k=n} c_k^2 \right| \leq \frac{\varepsilon^2}{M}.
\end{align*}

By Cauchy-Schwarz inequality, then for any $x \in X$, we have
\begin{align*}
    \left| \sum^m_{k=n} c_k f_n(x) \right| \leq \left( \sum^m_{k=n} c_n^2 \right)^{\frac{1}{2}} \left( \sum^m_{k=n} f_n(x)^2 \right)^{\frac{1}{2}} < \varepsilon,
\end{align*}
and then $\displaystyle \sum^\infty_{n=1} c_n f_n(x)$ converges uniformly. Therefore, $\displaystyle \sum^\infty_{n=1} c_n f_n(x)$ converges to a continuous function.
\end{proof}

\medskip

\begin{proof}[Second Proof of Exercise \ref{August_2015_2}]
Define $\displaystyle f(x) = \sum_{n=1}^\infty c_n f_n(x)$, and we can prove that $f(x)$ also converges for $x\in X$. Indeed, with Cauchy–Schwarz inequality, we have
\begin{align*}
    \sum_{n=1}^\infty c_nf_n(x) & \leq \left(\sum_{n=1}^\infty c_n^2\right)^{\frac{1}{2}} \left(\sum_{n=1}^\infty f_n(x)^2\right)^{\frac{1}{2}} \\
    & \leq \left(\sum_{n=1}^\infty c_n^2\right)^{\frac{1}{2}} \sup_{x\in X} \left(\sum_{n=1}^\infty f_n(x)^2\right)^{\frac{1}{2}} < \infty
\end{align*}

It remains to prove that $\displaystyle f(x) = \lim_{n\to\infty}\sum_{n=1}^\infty c_n f_n(x)$ is a continuous function. Since $\displaystyle \sum_{n=1}^\infty c_nf_n(x) < \infty$, then $\displaystyle \lim_{n\to\infty} c_n f_n = 0$. Thus, for every $\varepsilon > 0$, there exists $N > 0$, such that for $n > N$, $\displaystyle \sum_{n=N+1}^\infty c_nf_n(x) < \infty$. Also, for the same $\varepsilon$, we can choose $\delta > 0$ such that if $|x - y| < \delta$, then 
\begin{align*}
    \left|f_n(x) - f_n(y)\right| < \frac{\varepsilon^2}{N \left(\sum_{n=1}^\infty c_n^2\right)}
\end{align*}
for all $n = 1,2,\cdots$. Indeed, we could find such $\delta$ since $f_n$'s are continuous functions. Thus, if $|x - y| < \delta$, we have 
\begin{align*}
    \left|f(x) - f(y)\right| & \leq \sum_{n=1}^\infty c_n \left|f_n(x) - f_n(y)\right|\\
    & = \sum_{n=1}^N c_n \left|f_n(x) - f_n(y)\right| + \sum_{n=N+1}^\infty c_n \left|f_n(x) - f_n(y)\right|\\
    & \leq \left(\sum_{n=1}^N c_n^2\right)^{\frac{1}{2}} \left(\sum_{n=1}^N \left|f_n(x) - f_n(y)\right|^2\right)^{\frac{1}{2}} + \varepsilon \\
    & \leq 2 \varepsilon 
\end{align*}
Thus, $f$ is a continuous function as defined above.
\end{proof}

\begin{exercise}
Suppose that a set $A \subset \mathbb{R}^n$ is a union of an increasing family of compact sets $A = \bigcup^\infty_{i=1} A_i$, $A_1 \subset A_2 \subset \cdots$, and also that there is a compact set $C \subset \mathbb{R}^n$ such that 
\begin{align*}
    \forall i \in \mathbb{N}, \forall x \in A \setminus A_i, d(x, C) < \frac{1}{i}.
\end{align*}
Prove that the closure $\overline{A}$ of $A$ satisfies $\overline{A} \subset A \cup C$.\\
{\bf Remark.} Here $\displaystyle d(x, C) = \inf_{y \in C}|x - y|$.
\end{exercise}
\begin{proof}
For $x \in \overline{A}$, there exists a sequence $\{x_n\} \subset A$ such that $\lim_{n\to\infty} x_n = x$.
\begin{enumerate}[label=(\alph*)]
    \item If there exists $m \in \mathbb{N}$ such that $\{x_n\} \subset A_m$, then since $A_m$ is compact, then there exists a subsequence $\left\{x_{n_k}\right\}$ of $\{x_n\}$ such that $\displaystyle \lim_{k\to\infty} x_{n_k} = x$. Then, $x \in A_m$, and hence $\overline{A} \subset A$.
    
    \item Otherwise, for each $m \in \mathbb{N}$, there exists $x_{n_m} \notin A_m$. And then $d\left(x_{n_m}, C\right) < 1/m$ for all $m \in \mathbb{N}$. Then we have
    \begin{align*}
        d(x, C) & = \inf_{y \in C} |x - y| \\
        & = \inf_{y \in C} \left( |x - x_{n_m}| + |x_{n_m} - y| \right) \\
        & = |x - x_{n_m}| + \inf_{y \in C} \left(|x_{n_m} - y| \right) \\
        & < |x - x_{n_m}| + \frac{1}{m} \xrightarrow[]{m \to \infty} 0,
    \end{align*}
    which implies $x \in C$, and hence $\overline{A} \subset C$.
\end{enumerate}
By (a) and (b), we have $\overline{A} \subset A \cup C$.
\end{proof}

\medskip

\begin{exercise}
Let $n \geq 3$. Consider an $n$-times continuously differentiable function $f \in C^n(\mathbb{R})$ such that $f^{(k)}(0) = (0), k = 2, 3 \cdots, n-1$ and $f^{(n)}(0) \neq 0$. Clearly, by the mean value theorem, for $h > 0$ there is $0 < \theta(h) < h$ such that 
\begin{align*}
    f(h) - f(0) = h f'(\theta(h)).
\end{align*}
Prove that
\begin{align*}
    \lim_{h \to 0} \frac{\theta(h)}{h} = \left( \frac{1}{n} \right)^{\frac{1}{n-1}}.
\end{align*}
{\bf Hint:} Expand $f$ and $f'$ using Taylor's formula.
\end{exercise}
\begin{proof}
By the Taylor's formula, we have
\begin{align*}
    f(h) & = f(0) + f'(0)h + \cdots + \frac{f^{(n)}(0)}{n!} h^n + h^n \psi_1(h),\\
    f'(h) & = f'(0) + f''(0)h + \cdots + \frac{f^{n}(0)}{(n-1)!}h^{n-1} + h^{n-1} \psi_2(h),
\end{align*}
where $h^n \psi_1(h) = o(h^n)$, $h^{n-1} \psi_2(h) = o(h^{n-1})$. By the mean value theorem, the first equation gives
\begin{align*}
    f'(\theta(h)) = f'(0) + f''(0)h + \cdots + \frac{f^{(n)}(0)}{n!}h^{n-1} + h^n \psi_1(h),
\end{align*}
and by the Taylor's formula for $f'$, we have
\begin{align*}
    f'(\theta(h)) = f'(0) + f''(0)\theta(h) + \cdots + \frac{f^{(n)}(0)}{(n-1)!}\theta(h)^{n-1} + h^{n-1} \psi_2(h).
\end{align*}
Above two equations gives 
\begin{align*}
    \frac{f^{(n)}(0)}{n!}h^{n-1} + h^n \psi_1(h) = \frac{f^{(n)}(0)}{(n-1)!}\theta(h)^{n-1} + h^{n-1} \psi_2(h),
\end{align*}
and letting $h \to 0$ implies
\begin{align*}
    \lim_{h\to 0} \frac{\theta(h)^{n-1}}{h^{n-1}} = \frac{1}{n},
\end{align*}
and thus 
\begin{align*}
    \lim_{h \to 0} \frac{\theta(h)}{h} = \left( \frac{1}{n} \right)^{\frac{1}{n-1}}.
\end{align*}
\end{proof}

\medskip

\begin{exercise}{\rm *}
Prove that $$\lim_{x\to\infty} e^{-x^2} \int^x_0 e^{t^2}\, dt = 0.$$
\end{exercise}
\begin{proof}
Since $\displaystyle \lim_{x\to\infty}e^{-x^2} = \infty$, $\displaystyle \lim_{x\to\infty}e^{-x^2} \int^x_0 e^{t^2}\, dt = \infty$, then by L'Hopital's rule, we have
\begin{align*}
    \lim_{x\to\infty} e^{-x^2} \int^x_0 e^{t^2}\, dt = \lim_{x\to\infty} \frac{\int^x_0 e^{t^2}\, dt}{e^{-x^2}} = \lim_{x\to\infty} \frac{e^{x^2}}{2x e^{x^2}} = \lim_{x\to\infty} \frac{1}{2x} = 0.
\end{align*}
\end{proof}

\medskip

\begin{exercise}{\rm *}\label{August_2015_6}
Let $f \in C^1(\mathbb{R})$ be a continuously differentiable function such that $\left|f'(x)\right| \leq 1/2$ for all $x \in \mathbb{R}$. Define $g: \mathbb{R}^2 \to \mathbb{R}^2$ by
\begin{align*}
    g(x, y) = (x + f(y), y + f(x)).
\end{align*}
Prove that
\begin{enumerate}[label=(\alph*)]
    \item $g$ is diffeomorphism,
    
    \item $g(\mathbb{R}^2) = \mathbb{R}^2$,
    
    \item the area $\left| g([0,1]^2) \right|$ of the image of the unit square belongs to the interval $[3/4, 5/4]$.
\end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item The determinant of Jordan of $g$ is
    \begin{align*}
        |J| = \begin{vmatrix}
            1 & f'(y) \\
            f'(x) & 1
        \end{vmatrix} = 1 - f'(x)f'(y) > \frac{3}{4} \neq 0.
    \end{align*}
    Also, for $(x_1, y_1), (x_2, y_2) \in \mathbb{R}^2$ such that $g(x_1, y_1) = g(x_2, y_2)$, we have
    \begin{align*}
        |g(x_1, y_1) - g(x_2, y_2)| & = \sqrt{|x_1 + f(y_1) - x_2 - f(y_2)|^2 + |y_1 + f(x_1) - y_2 - f(x_2)|^2} \\
        & \geq \frac{|x_1 + f(y_1) - x_2 - f(y_2)| + |y_1 + f(x_1) - y_2 - f(x_2)|}{\sqrt{2}} \\
        & \geq \frac{|x_1 - x_2| - |f(y_1) - f(y_2)| + |y_1 - y_2| - |f(x_1) - f(x_2)|}{\sqrt{2}} \\
        & \geq \frac{|x_1 - x_2| - \frac{1}{2}|y_1 - y_2| + |y_1 - y_2| - \frac{1}{2}|x_1 - x_2|}{\sqrt{2}} \\
        & = \frac{1}{2\sqrt{2}} \left(|x_1 - x_2| + |y_1 - y_2| \right),
    \end{align*}
    which implies $(x_1, y_1) = (x_2, y_2)$. Then $g$ is one-to-one from $\mathbb{R}^2 \to g(\mathbb{R}^2)$, and thus, $g$ is diffeomorphism.
    
    \item It suffices to show that $g(\mathbb{R}^2)$ is open and closed.
    \begin{enumerate}[label=\arabic*)]
        \item Since $g$ is invertible in a neighborhood $U_x$ of each point $x \in \mathbb{R}^2$, and $g^{-1}$ is continuous, then we have $g(U_x)$ is open\footnote{By {\bf Theorem 9.25} in {\it Principle of Mathematical Analysis}, Rudin, W\cite{11}: If $f$ is a $C^1$-mapping of an open set $E \subset \mathbb{R}^n$ into $\mathbb{R}^n$ and if $Df(x)$ is invertible for every $x \in E$, then $f(W)$ is an open subset of $\mathbb{R}^n$ for every open set $W \subset E$.}. Thus,
        \begin{align*}
            g(\mathbb{R}^2) = g \left( \bigcup_{x \in \mathbb{R}^2} U_x \right) = \bigcup_{x \in \mathbb{R}^2} g(U_x)
        \end{align*}
        is also open.
        
        \item For any point $y \in \overline{g(\mathbb{R}^2)}$, there exists a sequence $\{y_k\}$ in $g(\mathbb{R}^2)$ such that $\displaystyle \lim_{k\to\infty} y_k = y$. Also, since $g$ is one-to-one in $\mathbb{R}^2$, then there exists a sequence $\{x_k\}$ in $\mathbb{R}^2$ such that $g(x_k) = y_k$.
        
        By (a), $\{x_k\}$ is a Cauchy sequence\footnote{For $(x_1, y_1), (x_2, y_2) \in \mathbb{R}^2$, $\left|x_1 - x_2\right| + \left|y_1 - y_2\right| \leq 2\sqrt{2} \left|g(x_1, y_1) - g(x_2, y_2)\right|$.}. Since $\mathbb{R}^2$ is complete, then every Cauchy sequence in $\mathbb{R}^2$ converges to a point in $\mathbb{R}^2$, that is $\displaystyle \lim_{k\to\infty} x_k = x \in \mathbb{R}^2$. Since $g$ is continuous, then 
        \begin{align*}
            y = \lim_{k\to\infty} y_k = \lim_{k\to\infty} g(x_k) = g(x) \in g(\mathbb{R}^2),
        \end{align*}
        and therefore $g(\mathbb{R}^2)$ is closed.
    \end{enumerate}
    Thus, $g(\mathbb{R}^2) = \mathbb{R}^2$.
    
    \item By (a), we have $|J| \in [3/4, 5/4]$. And the area $A$ of $\left| g([0,1]^2) \right|$ is
    \begin{align*}
        A = \left|\int_{[0,1]^2} |J|\, dx dy \right| \in \left[ \frac{3}{4}, \frac{5}{4} \right] \int_{[0,1]^2} 1 \, dx dy = \left[ \frac{3}{4}, \frac{5}{4} \right].
    \end{align*}
\end{enumerate}
\end{proof}

\medskip

\begin{proof}[Second Proof of Exercise \ref{August_2015_6}(a)]
To prove the existence of a unique solution to $g(x,y) = (x_0, y_0)$, use the contraction principle. Let
\begin{align*}
    T(x,y) & = - g(x,y) + (x,y) + (x_0, y_0) \\
    & = (x_0-f(y), y_0-f(x)).
\end{align*}
Then,
\begin{align*}
    T(x,y) - T(x',y') & = (x_0-f(y), y_0-f(x)) - (x_0-f(y'), y_0-f(x')) \\
    & = (f(y') - f(y), f(x') - f(x)) \\
    & = \left(f'(\xi)(y'-y), f'(\zeta)(x'-x)\right),
\end{align*}
and then 
\begin{align*}
    \left|T(x,y) - T(x',y')\right| & = \sqrt{|f(y') - f(y)|^2 + |f(x') - f(x)|^2} \\
    & \leq \frac{1}{2} \sqrt{|y' - y|^2 + |x' - x|^2}.
\end{align*}
Thus, $T$ is a contraction and hence there is a unique fixed point $(x,y)$ of $T$ such that $(x_0-f(y), y_0-f(x)) = (x,y)$, and it follows that $$g(x,y) = (x+f(y), y+f(x)) = (x_0, y_0).$$
\end{proof}

\newpage
\section{April 2015 Exam}

\begin{exercise}
Define $f: \mathbb{R}^2 \to \mathbb{R}$ by
\begin{align*}
    f(x,y) = \begin{cases}
        \displaystyle \frac{x^2 (y^4 + 2x)}{x^2 + y^4}, & {\rm if}\,\, (x,y) \neq (0,0); \\
        0, & {\rm if}\,\, (x,y) = (0,0).
    \end{cases}
\end{align*}
Prove that $f$ is differentiable at $(0,0)$.
\end{exercise}
\begin{proof}
We have the partial derivatives at $(0,0)$ are
\begin{align*}
    \frac{\partial f}{\partial x}(0,0) & = \lim_{h\to 0} \frac{f(h,0) - f(0,0)}{h} = 2, \\
    \frac{\partial f}{\partial y}(0,0) & = \lim_{h\to 0} \frac{f(0,h) - f(0,0)}{h} = 0.
\end{align*}
Then we have
\begin{align*}
    \left| \frac{f(p,q) - f_x(0,0)p - f_y(0,0)q - f(0,0)}{\sqrt{p^2 + q^2}} \right| & = \left| \frac{pq^4 (p - 2)}{(p^2 + q^4) \sqrt{p^2 + q^2}} \right| \\
    & \leq \left| \frac{pq^4 (p - 2)}{ 2pq^2 \sqrt{p^2 + q^2}} \right| \\
    & \leq \left| \frac{q^2 (p - 2)}{ 2\sqrt{p^2 + q^2}} \right| \\
    & \leq \left| \frac{q^2 (p - 2)}{2q} \right| = \left| \frac{q(p - 2)}{2} \right|,
\end{align*}
and therefore
\begin{align*}
    \lim_{(p,q) \to (0,0)} \left| \frac{f(p,q) - f_x(0,0)p - f_y(0,0)q - f(0,0)}{\sqrt{p^2 + q^2}} \right| \leq \lim_{(p,q) \to (0,0)} \left| \frac{q(p - 2)}{2} \right| = 0.
\end{align*}
Thus, $f$ is differentiable at $(0,0)$.
\end{proof}

\medskip

\begin{exercise}\label{April_2015_2}
A graph of mapping $f: X \to Y$ is defined as
\begin{align*}
    G_f = \{(x,y) \in X \times Y \,\, | \,\, y = f(x) \}.
\end{align*}
Prove that if $X$ is a metric space and $Y$ is a compact metric space, then a map $f: X \to Y$ is continuous if and only if $G_f$ is a closed subset of $X \times Y$.
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item ($\Rightarrow$) We could pick a sequence $\{x_n\}$ in $X$ such that $\displaystyle \lim_{n\to\infty} x_n = x \in X$. Then there exists a sequence $\{y_n\}$ in $Y$ such that $f(x_n) = y_n$. Since $Y$ is compact, then $\{y_n\}$ has a convergent subsequence $\left\{ y_{n_k} \right\}$ such that $\displaystyle \lim_{k\to\infty} y_{n_k} = y \in Y$. 
    
    Also, by continuity of $f$, we have 
    $$f(x) = \lim_{k\to\infty} f\left(x_{n_k}\right) = \lim_{k\to\infty} y_{n_k} = y,$$ 
    and therefore $(x, y) \in G_f$. Thus, $G_f$ is closed.
    
    \item ($\Leftarrow$) Since $G_f$ is closed, then the convergent sequence $\{(x_n, y_n)\}$ in $G_f$ converges to a point in $G_f$, denoted by $(x,y)$, where $y_n = f(x_n)$. Then we have $\displaystyle \lim_{n\to\infty}(x_n, f(x_n)) = (x, f(x))$, that is for $\varepsilon > 0$, there exits a positive integer $N \in \mathbb{N}$, such that for all $n \geq N$, $d_X(x_n, x) < \varepsilon$ and $d_Y(f(x_n), f(x)) < \varepsilon$. Then, for this $\varepsilon > 0$, there exists $\delta = \varepsilon$, such that if $d_X(x_n, x) < \delta$, then $d_Y(f(x_n), f(x)) < \varepsilon$. Thus, $f$ is continuous.
\end{enumerate}
\end{proof}

\medskip

\begin{proof}[Second Proof of Exercise \ref{April_2015_2}]{\rm *}
~\begin{enumerate}[label=(\alph*)]
    \item ($\Rightarrow$) For the sequence $\{(x_n, f(x_n))\}$ in $G_f$ converges to some point $(x_0, y_0) \in X \times Y$. For any $\varepsilon > 0$, there exists $N > 0$ such that if $n > N$, then 
    \begin{align*}
        \sqrt{d_X^2(x_n, x_0) + d_Y^2(f(x_n), y_0)} < \varepsilon,
    \end{align*}
    which implies $d_X(x_n, x_0) < \varepsilon$ and $d_Y(f(x_n), y_0) < \varepsilon$. 
    
    Since $f$ is continuous, then for the same $\varepsilon$, there exists $\delta_\varepsilon > 0$ such that if $d_X(x, x_0) < \delta_\varepsilon$, then $d_Y(f(x), f(x_0)) < \varepsilon$.
    
    Let $\delta = \min \{\varepsilon, \delta_\varepsilon\}$, then if $d_X(x, x_0) < \delta_\varepsilon$, we have $d_Y(f(x), f(x_0)) < \varepsilon$. 
    
    Now let $N$ be such that for $n > N$, $d_X(x_n, x_0) < \delta$. Then we have
    \begin{align*}
        d_Y(f(x), f(x_0)) < \varepsilon, \quad d_Y(f(x_n), y_0) < \varepsilon,
    \end{align*}
    and therefore we have
    \begin{align*}
        d_Y(y_0, f(x_0)) \leq d_Y(y_0, f(x_n)) + d_Y(f(x_n), f(x_0)) < 2 \varepsilon.
    \end{align*}
    Thus, $y_0 = f(x_0)$ and hence $(x_0, y_0) \in G_f$.
    
    \item ($\Leftarrow$) Let $p: X \times X \to X$ be defined by $p(x, y) = x$. Let $K$ be a closed set in $X \times Y$.
    
    Let $\{x_n\}$ be a sequence in $p(K)$ with limit $x_0$. We can choose $(x_n, y_n) \in K$ such that $p(x_n, y_n) = x_n$. Since $Y$ is compact, then the sequence $\{y_n\}$ has a convergent subsequence $\{y_{n_k}\}$ such that $\displaystyle \lim_{k\to\infty} y_{n_k} = y_0 \in Y$. Then the sequence $\{x_{n_k}, y_{n_k}\}$ converges to $(x_0, y_0)$. Since $K$ is closed, then $(x_0, y_0) \in K$. Hence, $x_0 = p(x_0; y_0) \in p(K)$, which means $p(K)$ is closed, i.e. $p$ is a closed map. 
    
    For a closed set $C \subset Y$, we have
    \begin{align*}
        f^{-1}(C) = \{x \in X\, | \, f(x) \in C\},
    \end{align*}
    and on the other hand 
    \begin{align*}
        p\left((X \times C) \bigcap G_f \right) = \{x \in X\, | \, f(x) \in C\},
    \end{align*}
    and therefore
    \begin{align*}
        f^{-1}(C) = p\left((X \times C) \bigcap G_f \right),
    \end{align*}
    which is the intersection of two closed sets, then $f^{-1}(C)$ is closed. Thus, $f$ is continuous.
\end{enumerate}
\end{proof}

\medskip

\begin{proof}[Third Proof of Exercise \ref{April_2015_2}(b)]
Suppose $f$ is not continuous at some point $x_0$. Then there exists a sequence $\{x_n\}$ in $X$ converging to $x_0$ such that there exists $\varepsilon > 0$, for all $N \in \mathbb{N}$, there exists $n_k > N$ and $d_Y(f(x_{n_k}), f(x_0)) \geq 0$.

Since $Y$ is compact, then the sequence $\{f(x_{n_k})\}$ has a convergent subsequence $f(x_{n_{k_l}})$ such that $\displaystyle \lim_{l\to\infty} f(x_{n_{k_l}}) = y_0 \in Y$. Then the sequence $\left\{ \left(x_{n_{k_l}}, f(x_{n_{k_l}}) \right) \right\}$ in $G_f$ has limit $(x_0, y_0)$. Since $G_f$ is closed, then $(x_0, y_0) \in G_f$. Therefore, $y_0 = f(x_0)$, and then $\left\{ x_{n_{k_l}} \right\}$ has the limit $f(x_0)$, which is a contradiction.
\end{proof}

\medskip

\begin{exercise}
Prove that the series $$\sum^\infty_{n=1} \frac{x^\alpha}{n(n + x^2)}$$
\begin{enumerate}[label=(\alph*)]
    \item converges uniformly on $[0,\infty)$ when $\alpha = 1$,
    
    \item converges pointwise but not uniformly on $[0, \infty)$ when $\alpha = 2$.{\rm *}
    \end{enumerate}
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item When $\alpha = 1$, we have
    \begin{align*}
        \sum^\infty_{n=1} \frac{x^\alpha}{n(n + x^2)} = \sum^\infty_{n=1} \frac{1}{n(\frac{n}{x} + x)} \leq \sum^\infty_{n=1} \frac{1}{2n \sqrt{\frac{n}{x}} \sqrt{x}} \leq \sum^\infty_{n=1} \frac{1}{2n^{\frac{3}{2}}},
    \end{align*}
    and by Weierstrass M-test, $\displaystyle \sum^\infty_{n=1} \frac{1}{2n^{\frac{3}{2}}}$ is convergent, and then $\displaystyle \sum^\infty_{n=1} \frac{1}{n(\frac{n}{x} + x)}$ is uniformly convergent on $[0,\infty)$. 
    
    \item When $\alpha = 2$, we have
    \begin{align*}
        \sum^\infty_{n=1} \frac{x^\alpha}{n(n + x^2)} = \sum^\infty_{n=1} \frac{1}{\frac{n^2}{x^2} + n} \leq \sum^\infty_{n=1} \frac{x^2}{n^2},
    \end{align*}
    and since $\displaystyle \sum^\infty_{n=1} \frac{x^2}{n^2}$ is convergent for any $x \in [0,\infty)$, then $\displaystyle \sum^\infty_{n=1} \frac{x^2}{n(n + x^2)}$ converges pointwise on $[0,\infty)$.
    
    Suppose $\displaystyle \sum^\infty_{n=1} \frac{x^2}{n(n + x^2)}$ converges uniformly on $[0,\infty)$, then for any $\varepsilon > 0$, there exists $N \in \mathbb{N}$ such that for all $k >m > N$ and any $x \in [0,\infty)$, we have
    \begin{align*}
        \left|\sum^k_{n=1} \frac{x^2}{n(n + x^2)} - \sum^m_{n=1} \frac{x^2}{n(n + x^2)} \right| < \varepsilon.
    \end{align*}
    Letting $\varepsilon = 1$, then for $x \in [0,\infty)$, and $k > N + 1$, we have
    \begin{align*}
        \left|\sum^k_{n=N+1} \frac{x^2}{n(n + x^2)} \right| < 1.
    \end{align*}
    
    On the other hand, we have
    \begin{align*}
        \lim_{x\to\infty} \left|\sum^k_{n=N+1} \frac{x^2}{n(n + x^2)} \right| = \sum^k_{n=N+1} \lim_{x\to\infty} \frac{x^2}{n(n + x^2)} = \sum^k_{n=N+1} \frac{1}{n},
    \end{align*}
    and since $\displaystyle \sum^\infty_{n=1} \frac{1}{n}$ is divergent, then there exists $M \in \mathbb{N}$ such that $\displaystyle \sum^k_{n=N+1} \frac{1}{n} > 1$, which is a contradiction.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}{\rm *}
Find the distance between the two ellipsoids
\begin{align*}
    \frac{x_1^2}{a_1^2} + \frac{x_2^2}{a_2^2} + \cdots + \frac{x_n^2}{a_n^2} = 1,
\end{align*}
and
\begin{align*}
    \frac{x_1^2}{a_1^2} + \frac{x_2^2}{a_2^2} + \cdots + \frac{x_n^2}{a_n^2} = 2,
\end{align*}
where $a_1 > a_2 > \cdots > a_n > 0$.
\end{exercise}
\begin{proof}
Let
\begin{align*}
    f(x_1, \cdots, x_n, y_1, \cdots, y_n) = (x_1 - y_1)^2 + \cdots + (x_n - y_n)^2,
\end{align*}
where $x_i, y_i$ satisfy
\begin{align*}
    g_1 & = \frac{x_1^2}{a_1^2} + \frac{x_2^2}{a_2^2} + \cdots + \frac{x_n^2}{a_n^2} - 1 = 0, \\
    g_2 & = \frac{y_1^2}{a_1^2} + \frac{y_2^2}{a_2^2} + \cdots + \frac{y_n^2}{a_n^2} - 2 = 0.
\end{align*}

By Lagrange multiplier method, we know if $f$ achieves a minimum at $p$, then $\nabla f|_p$ is a linear combination of $\nabla g_1|_p$ and $\nabla g_2|_p$, that is, $\nabla f|_p = \lambda \nabla g_1|_p + \mu \nabla g_2|_p$. Then, for each $i = 1, \cdots, n$, we have
\begin{align*}
    2(x_i - y_i) & = 2 \lambda \frac{x_i}{a_i^2}, \\
    2(y_i - x_i) & = 2 \lambda \frac{y_i}{a_i^2}.
\end{align*}
If $\lambda = 0$ or $\mu = 0$, then $x_i = y_i$, which is contradiction to $g_1 \neq g_2$. Therefore, we have $y_i = \alpha x_i$ for some $\alpha \neq 0$ and $i = 1, \cdots, n$. By the definition of $g_1$ and $g_2$, we have $|\alpha| = 2$. Then,
\begin{align*}
    f_{\min}^2 & = \sum^n_{i=1} (\alpha - 1)^2 x_i^2 \\
    & \geq \sum^n_{i=1} (\sqrt{2} - 1)^2 x_i^2 \\
    & = (\sqrt{2} - 1)^2 a_n^2 \sum^n_{i=1} \frac{x_i^2}{a_n^2} \\
    & \geq (\sqrt{2} - 1)^2 a_n^2 \sum^n_{i=1} \frac{x_i^2}{a_i^2} = (\sqrt{2} - 1)^2 a_n^2.
\end{align*}
Also, when $x = (0, \cdots, 0, a_n)$, $y = (0, \cdots, 0, \sqrt{2}a_n)$, we have $f_{\min} = (\sqrt{2} - 1) a_n$.
\end{proof}

\medskip

\begin{exercise}
Let $Q: C^\infty(\mathbb{R}^n) \to \mathbb{R}$ be a linear mapping such that $Qf \geq 0$ whenever $f \in C^\infty(\mathbb{R}^n)$ satisfies $f(0) = 0$ and $f(x) \geq 0$ in a neighborhood of $0$. Prove that there are real numbers $a_{ij}, b_i$ and $c$ such that for any $g \in C^\infty(\mathbb{R}^n)$,
\begin{align*}
    Qg = \sum^n_{i,j=1} a_{ij} \frac{\partial^2 g}{\partial x_i \partial x_j}(0) + \sum^n_{i=1} b_i \frac{\partial g}{\partial x_i}(0) + cg(0).
\end{align*}
\end{exercise}
\begin{proof}
By Taylor formula, we have
\begin{align*}
    g(x) = g(0) + \sum^n_{i=1} \frac{\partial g}{\partial x_i}(0) x_i + \frac{1}{2} \sum^n_{i,j=1} \frac{\partial^2 g}{\partial x_i \partial x_j}(0) x_i x_j + h(x),
\end{align*}
where $h(x) = o(x^2)$.

Let $Q(1) = c, Q(x_i) = b_i, Q(x_ix_j) = 2a_{ij}$, then we have
\begin{align*}
    Qg = \sum^n_{i,j=1} a_{ij} \frac{\partial^2 g}{\partial x_i \partial x_j}(0) + \sum^n_{i=1} b_i \frac{\partial g}{\partial x_i}(0) + cg(0) + Qh.
\end{align*}

Since $h(x) = o(x^2)$, then $\displaystyle \lim_{x \to \infty} h(x)/|x|^2 = 0$, that is for any $\varepsilon > 0$, there exists a neighborhood $U_0$ of $0$ such that if $x \in U_0$, 
\begin{align*}
    - \varepsilon < \frac{h(x)}{|x|^2} < \varepsilon,
\end{align*}
which is equivalent to $\varepsilon |x|^2 - h(x) \geq 0, x \in U_x$. Also, $f(x) = \varepsilon |x|^2 - h(x) \in C^\infty(\mathbb{R}^n)$ and satisfies $f(0) = 0$, and then $Qf \geq 0$. Therefore, $Qh \leq Q\left(\varepsilon |x|^2\right)$, and letting $\varepsilon \to 0$ gives $Qh \leq 0$. Similarly, $ h(x) - \varepsilon |x|^2 \geq 0$ gives $Qh \geq 0$. Thus, $Qh = 0$, and we have the desired equation.
\end{proof}

\medskip

\begin{exercise}
Let $D =\{(x, y)\in\mathbb{R}^2 : x^2 + y^2\leq 1\}$ and $f\in C^\infty(\mathbb{R}^2)$. Suppose that $f(x, y) = 1$ for all
$(x, y)\in\partial D$. Prove that
$$
\iint_D \left(2f(x,y)+x\frac{\partial f}{\partial x}+y\frac{\partial f}{\partial y}\right)dA=2\pi.
$$
\end{exercise}
\begin{proof}
By Green's theorem, $\displaystyle \int_{\partial \Omega} P\, dx + Q\, dy = \iint_D \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y}$, then the area of $D$ is 
\begin{align*}
    A = \frac{1}{2} \int_{\partial D} x\, dy - y\, dx.
\end{align*}
Indeed, 
\begin{align*}
    \frac{1}{2} \int_{\partial D} x\, dy - y\, dx = \frac{1}{2} \iint_D 2\, dA = A.
\end{align*}

Now let $P = -yf, Q = xf$, then 
\begin{align*}
    \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} = 2f + x \frac{\partial f}{\partial x} + y \frac{\partial f}{\partial y},
\end{align*}
and thus
\begin{align*}
    \iint_D \left(2f(x,y)+x\frac{\partial f}{\partial x}+y\frac{\partial f}{\partial y}\right)\, dA & = \int_{\partial D} -yf\, dx + x f\, dy \\
    & = \int_{\partial D} -y \, dx + x \, dy \\
    & = \iint_D 2\, dA = 2\pi.
\end{align*}
\end{proof}


\newpage
\section{August 2014 Exam}

\begin{exercise}{\rm *}
Let $\alpha \in \mathbb{R}$, and let $f_\alpha: \mathbb{R}^2 \to \mathbb{R}$ be given by formulas
\begin{align*}
    f_{\alpha}(x,y) = \begin{cases}
        \displaystyle \frac{x^4 + y^4}{(x^2 + y^2)^{\alpha}}, & (x,y) \neq (0,0), \\
        0, & (x, y) = (0,0).
    \end{cases}
\end{align*}
Determine with proof, those values of $\alpha$ for which $f_{\alpha}$ is differentiable.
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item If $\alpha \leq 0$, then $f_\alpha$ is differentiable.
    
    \item If $\alpha \geq 2$, then $f_\alpha$ is not differentiable at $(0,0)$. Indeed, for direction $y = kx$, we have
    \begin{align*}
        \lim_{(x,y) \to (0,0)} \frac{x^4 + k^4x^4}{(1 + k^2)^{\alpha} x^{2\alpha}} = \lim_{x \to 0} \frac{1 + k^4}{(1 + k^2)^{\alpha} x^{2\alpha - 4}} = \pm \infty \neq 0,
    \end{align*}
    then $f_\alpha$ is not differentiable at $(0,0)$.
    
    \item If $0 < \alpha < 2$, $f_{\alpha}$ is continuous at $(0,0)$ by the similar argument in (b). Also, the partial derivatives at $(0,0)$ are given by
    \begin{align*}
        \frac{\partial f_{\alpha}}{\partial x}(0,0) & = \lim_{x\to 0} \frac{\frac{x^4}{x^{2\alpha}} - 0}{x} = \lim_{x\to 0} x^{3 - 2\alpha}, \\
        \frac{\partial f_{\alpha}}{\partial y}(0,0) & = \lim_{y\to 0} \frac{\frac{y^4}{y^{2\alpha}} - 0}{y} = \lim_{y\to 0} y^{3 - 2\alpha},
    \end{align*}
    and they only exist if $\alpha \leq 3/2$. 
    \begin{enumerate}[label=\arabic*)]
        \item For $\alpha < 3/2$, we have $\displaystyle \frac{\partial f_{\alpha}}{\partial x}(0,0) = \frac{\partial f_{\alpha}}{\partial y}(0,0) = 0$, and then for $(t,s) \in \mathbb{R}^2$, 
        \begin{align*}
            & \lim_{(t,s)\to (0,0)} \left| \frac{f_{\alpha}(t,s) - \frac{\partial f_{\alpha}}{\partial x}(0,0)t - \frac{\partial f_{\alpha}}{\partial y}(0,0)s - f_{\alpha}(0,0)}{\sqrt{t^2 + s^2}} \right| \\
            = & \lim_{(t,s)\to (0,0)} \left| \frac{t^4 + s^4}{(t^2 + s^2)^{\alpha + \frac{1}{2}}} \right| \\
            \leq & \lim_{(t,s)\to (0,0)} \left| \frac{(t^2 + s^2)^2}{(t^2 + s^2)^{\alpha + \frac{1}{2}}} \right| \\
            = & \lim_{(t,s)\to (0,0)} \left(t^2 + s^2\right)^{\frac{3}{2} - \alpha} = 0,
        \end{align*}
        which implies $f_{\alpha}$ is differentiable at $(0,0)$ for $0 < \alpha < 3/2$.
        
        \item For $\alpha = 3/2$, we have $\displaystyle \frac{\partial f_{\alpha}}{\partial x}(0,0) = \frac{\partial f_{\alpha}}{\partial y}(0,0) = 1$, then for $(t,s) \in \mathbb{R}^2$,
        \begin{align*}
            & \lim_{(t,s)\to (0,0)} \left| \frac{f_{\alpha}(t,s) - \frac{\partial f_{\alpha}}{\partial x}(0,0)t - \frac{\partial f_{\alpha}}{\partial y}(0,0)s - f_{\alpha}(0,0)}{\sqrt{t^2 + s^2}} \right| \\
            = & \lim_{(t,s)\to (0,0)} \left| \frac{t^4 + s^4 - (t+s)(t^2+s^2)^{\frac{3}{2}}}{(t^2+s^2)^2} \right| \\
            \overset{s = t}{=} & \lim_{(t,t)\to (0,0)} \left| \frac{t^4 + t^4 - (t+t)(t^2 + t^2)^{ \frac{3}{2}} }{(t^2 + t^2)^2} \right| = \frac{1 - 2^{\frac{3}{2}}}{2} \neq 0,
        \end{align*}
        which implies $f_{\alpha}$ is not differentiable at $(0,0)$ for $\alpha = 3/2$.
    \end{enumerate}
\end{enumerate}
From all conditions, $f_{\alpha}$ is differentiable in $\mathbb{R}^2$ if and only if $\alpha < 3/2$.
\end{proof}

\medskip

\begin{exercise}
Find the area enclosed by the curve in the Euclidean plane: $\displaystyle x^{2/3} + y^{2/3} = 1$.
\end{exercise}
\begin{proof}
Let $x = r^3 \cos^3 \theta$ and $y = r^3 \sin^3 \theta$, where $r \in [0,1], \theta \in [0, 2\pi]$. Then the original curve becomes $r^2 = 1$. Also, the Jacobian is
\begin{align*}
    J = \begin{pmatrix}
        \displaystyle \frac{\partial x}{\partial r} & \displaystyle  \frac{\partial x}{\partial \theta} \\
        \displaystyle \frac{\partial y}{\partial r} & \displaystyle  \frac{\partial y}{\partial \theta}
    \end{pmatrix} =  \begin{pmatrix}
        3r^2 \cos^3 \theta & -3r^3 \cos^2 \theta \sin \theta \\
        3r^2 \sin^3 \theta & 3r^3 \sin^2 \theta \cos \theta 
    \end{pmatrix},
\end{align*}
and thus the area can be computed as
\begin{align*}
    A = \int^1_0 \int^{2\pi}_0 |J|\, d\theta dr = \int^1_0 9r^5\, dr \int^{2\pi}_0 \sin^2 \theta \cos^2 \theta \, d\theta = \frac{3}{8} \pi.
\end{align*}
\end{proof}

\medskip

\begin{exercise}{\rm *}
Let $f:[0,1] \to \mathbb{R}$ be a function. For $x \in [0,1]$, define $\displaystyle \operatorname{osc}_x(f) = \limsup_{t\to x} f(t) - \liminf_{t\to x} f(t)$. For $k \in \mathbb{R}^{+}$, let $\mathbb{D}_k = \left\{ x \in [0,1]\, |\, \operatorname{osc}_x(f) \geq k \right\}$. Prove that the set $\mathbb{D}_k$ is closed for each $k \in \mathbb{R}^{+}$. Hence, or otherwise, show that the set of points where $f$ is discontinuous cannot be the set of irrational real numbers.
\end{exercise}
\begin{proof}
~\begin{enumerate}[label=(\alph*)]
    \item  For $x \in [0,1]$, if $\operatorname{osc}_x(f) \geq k$, then there exists $k > 0$ such that for $y \in B(x,r)$, we have\footnote{Also, we could use $\operatorname{diam} f([x-r,x+r]) \geq k$.}
    \begin{align*}
        \left| \sup_{y\in B(x,r)} f(y) - \inf_{y\in B(x,r)} f(y) \right| \geq k.
    \end{align*}
    
    For any $x_0 \notin \mathbb{D}_k$, we have
    \begin{align*}
        \lim_{r \to 0} \left| \sup_{y\in B(x_0,r)} f(y) - \inf_{y\in B(x_0,r)} f(y) \right| < k,
    \end{align*}
    then there exists $r_0 > 0$ such that for any $\Tilde{r} \leq r_0$ and any $y \in B(x_0, r_0)$, we have
    \begin{align*}
        \left| \sup_{y\in B(x_0,\Tilde{r})} f(y) - \inf_{y\in B(x_0,\Tilde{r})} f(y) \right| < k.
    \end{align*}
    Therefore, $B(x_0, r_0) \in \mathbb{D}_k^C$ is open. Thus, $\mathbb{D}_k$ is closed.
    
    \item Let the set of points where $f$ is discontinuous be $K$. Then,
    \begin{align*}
        K = \bigcup_{k\in\mathbb{N}} \mathbb{D}_{\frac{1}{k}}.
    \end{align*}
    Since $\mathbb{R} \setminus \mathbb{Q}$ is a set without interior, if $\mathbb{R} \setminus \mathbb{Q} = K$, it is a countable union of closed sets. By Baire's theorem\footnote{\begin{intheorem}[Baire's Theorem\cite{11}]
    If $\displaystyle \mathbb{R}^k = \cup^\infty_{n=1} F_n$, where each $F_n$ is a closed subset of $\mathbb{R}^k$, then at least one $F_n$ has a nonempty interior. \end{intheorem}}, at least one has nonempty interior. So $\mathbb{R} \setminus \mathbb{Q}$ has nonempty interior, which is a contradiction. Hence, the set of points where $f$ is discontinuous cannot be the set of irrational real numbers.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}{\rm *}
Let $F: \mathbb{R}^n \to \mathbb{R}^m$ be given by the formula, for any $x \in \mathbb{R}^n$,
\begin{align*}
    F(x) = A(x) + B(x,x),
\end{align*}
where $A: \mathbb{R}^n \to \mathbb{R}^m$ is a linear map and $B: \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}^m$ is a symmetric bilinear form on $\mathbb{R}^n$, with values in $\mathbb{R}^m$.
\begin{enumerate}[label=(\alph*)]
    \item Prove that $A$ is injective if and only if $F$ is injective near the origin.
    
    \item Prove that if $A$ is surjective, then $F$ is surjective near the origin.
    
    \item Prove that $A$ is an isomorphism, if and only if $F$ is smoothly invertible near the origin.
\end{enumerate}
\end{exercise}
\begin{proof}
For all $x, u \in \mathbb{R}^n$,
\begin{align*}
    (DF)_x(u) & = \lim_{t\to 0} \frac{F(x + tu) - F(x)}{t} \\
    & = \lim_{t\to 0} \frac{A(x+tu) + B(x+tu,x+tu) - A(x) - B(x,x)}{t} \\
    & = A(u) + 2B(u,x),
\end{align*}
and then $(DF)_x = A$.
~\begin{enumerate}[label=(\alph*)]
    \item ($\Rightarrow$) If $A$ is injective, then $\rank A = n \leq m$, and hence there exist $n$ rows of $A$ that form a square matrix $T$ with rank $n$. Without loss of generality, we can assume they are the first $n$ rows of $A$. Define $G: \mathbb{R}^n \to \mathbb{R}^n$ by
    \begin{align*}
        G(x) = (F_1(x), F_2(x), \cdots, F_n(x)),
    \end{align*}
    where $F_i$ is the $i$-th row of $F$. Then $G \in C^1$ and $(DG)_0 = T$, so $G$ is invertible. By the Implicit Function theorem, $G$ is a local diffeomorphism, that is there exists a neighborhood $U$ of the origin such that $G$ is a bijection from $U$ to $G(U)$. 
    
    For $x, y \in U$, if $F(x) = F(y)$, then 
    \begin{align*}
        \left(G(x), F_{n+1}(x), \cdots, F_m(x) \right) = \left(G(y), F_{n+1}(y), \cdots, F_m(y) \right).
    \end{align*}
    Since $G$ is bijection, then $x = y$, and thus $F$ is injective in the neighborhood $U$ of the origin.
    
    ($\Leftarrow$) If $F$ is injective near the origin, and suppose that $A$ is not injective, then there exists $v \in \mathbb{R}^n$, $v \neq 0$ such that $A(v) = 0$. Then for any $t > 0$, 
    \begin{align*}
        F(tv) = A(tv) + B(tv,tv) = B(-tv,-tv) = F(-tv),
    \end{align*}
    and this means that $F$ is not injective near the origin, which is a contradiction.
    
    \item If $A$ is surjective, then $\rank A = m \leq n$, and then there exists $m$ columns of $A$ such that there columns form a square matrix $S$ with rank $m$. Without loss of generality, we can assume they are the first $m$ columns of $A$. Define $Q: \mathbb{R}^m \to \mathbb{R}^m$ by
    \begin{align*}
        Q(x) = \left(F_1(x,0), F_2(x,0), \cdots, F_m(x,0) \right),
    \end{align*}
    where $F_i$ is the $i$-th row of $F$. Then $Q \in C^1$ and $(DQ)_0 = S$, so $Q$ is invertible. By the Implicit Function theorem, $Q$ is a local diffeomorphism, that is there exists a neighborhood $V$ of the origin such that $Q$ is a bijection from $V$ to $Q(V)$. 
    
    For $x\in Q(V) \subset \mathbb{R}^m$, there exists a unique $y \in V \subset \mathbb{R}^m$ such that $Q(y) = x$. Then, 
    \begin{align*}
        F(y,0) = \left(F_1(y,0), F_2(y,0), \cdots, F_m(y,0) \right) = Q(y) = x,
    \end{align*}
    and thus $F$ is surjective near the origin.
    
    \item Since $A$ is an isomorphism, then $A$ is invertible. So the derivative of $F$ near the origin $(DF)_0 = A$ is also invertible. By the Implicit Function theorem, $F$ is a local diffeomorphism, that is $F$ is smooth and invertible near the origin.
\end{enumerate}
\end{proof}

\medskip

\begin{exercise}
Let $A: \mathbb{R}^n \to \mathbb{R}^n$ be an orthogonal linear transformation, so $\left\|Ax \right\| = \left\|x \right\|$, for any $x \in \mathbb{R}^n$. Let $u: \mathbb{R}^n \to \mathbb{R}$ be $\mathcal{C}^2$ and harmonic: $\nabla \cdot \nabla u = \nabla^2 u = 0$. Prove that the composition $u \circ A$ is also harmonic.
\end{exercise}
\begin{proof}
For $x = (x_1, \cdots, x_n) \in \mathbb{R}^n$, let
\begin{align*}
    y = (y_1, \cdots, y_n) = Ax = \left(\sum^n_{j=1} a_{1j}x_j, \cdots, \sum^n_{j=1} a_{nj}x_j \right)^T.
\end{align*}
By the Chain rule, we have
\begin{align*}
    \frac{\partial (u \circ A)}{\partial x_i}(x) = \sum^n_{j=1} \frac{\partial u}{\partial y_j} \frac{\partial y_j}{\partial x_i} = \sum^n_{j=1} a_{ji} \frac{\partial u}{\partial y_j},
\end{align*}
and then 
\begin{align*}
    \frac{\partial^2 (u \circ A)}{\partial x_i^2}(x) & = \frac{\partial}{\partial x_i}\left(\sum^n_{j=1} a_{ji} \frac{\partial u}{\partial y_j} \right) \\
    & = \sum^n_{j=1} a_{ji} \frac{\partial}{\partial x_i}\left(\frac{\partial u}{\partial y_j}(y) \right) \\
    & = \sum^n_{j=1} a_{ji} \sum^n_{k=1} \frac{\partial^2 u}{\partial y_j \partial y_k} \frac{\partial y_k}{\partial x_i} \\
    & = \sum^n_{j=1} a_{ji} \sum^n_{k=1} a_{ki} \frac{\partial^2 u}{\partial y_j \partial y_k}.
\end{align*}
Therefore, we have
\begin{align*}
    \nabla \cdot \nabla (u \circ A) & = \sum^n_{i=1} \frac{\partial^2 (u \circ A)}{\partial x_i^2}(x) \\
    & = \sum^n_{i=1} \sum^n_{j=1} a_{ji} \sum^n_{k=1} a_{ki} \frac{\partial^2 u}{\partial y_j \partial y_k} \\
    & = \sum^n_{i=1} \sum^n_{j=1} \sum^n_{k=1} a_{ji} a_{ki} \frac{\partial^2 u}{\partial y_j \partial y_k} \\
    & = \sum^n_{k=1} \sum^n_{j=1} \sum^n_{i=1} (A)_{ji} \left(A^T\right)_{ik} \frac{\partial^2 u}{\partial y_j \partial y_k} \\
    & = \sum^n_{k=1} \sum^n_{j=1} (I)_{jk} \frac{\partial^2 u}{\partial y_j \partial y_k} \\
    & = \sum^n_{k=1} \sum^n_{j=1} \delta_{jk} \frac{\partial^2 u}{\partial y_j \partial y_k} \\
    & = \sum^n_{j=1} \frac{\partial^2 u}{\partial y_j^2} = \nabla \cdot \nabla u = 0.
\end{align*}
Thus, $u \circ A$ is harmonic.
\end{proof}

\medskip

\begin{exercise}{\rm *}
Let $f: \mathbb{R} \to \mathbb{R}$ be $\mathcal{C}^2$. Suppose that $\left|f''(s) \right| \leq 1$ for all $s \in [0,2]$. Suppose also that the function $f$ has a local minimum at $s = 0$. Let $\mathbb{E}$ denote the closed unit ball, centered at the origin, in $\mathbb{R}^2$. Prove that
\begin{align*}
    \int_{\mathbb{E}} \int_{\mathbb{E}} \left(f(\|x\| + \|y\|) - f(\|y\|) \right)\, d^2x d^2y \leq \frac{25\pi^2}{36}.
\end{align*}
Here $\|(p,q)\| = \sqrt{p^2 + q^2}$, for any $(p,q) \in \mathbb{R}^2$.
\end{exercise}
\begin{proof}
Since $f$ has a local minimum at $s = 0$, we have $f'(0) = 0$. By Mean Value theorem, for $x \in [0,1]$,
\begin{align*}
    \left|f'(x)\right| = \left|f'(x) - f'(0)\right| = \left|f''(\xi) (x - 0) \right| \leq x,
\end{align*}
where $\xi \in [0,x]$. Also, by Taylor formula, 
\begin{align*}
    f(x + h) = f(x) + f'(x)h + \frac{f''(\xi)}{2}h^2, \xi \in [x, x+ h].
\end{align*}

For $x = (x_1, x_2), y = (y_1, y_2) \in \mathbb{R}^2$, consider $x = (r \cos \theta, r \sin \theta), y = (s \cos \omega, s \sin \omega)$, where $r, s \in [o,1]$ and $\theta, \omega \in [0,2\pi]$, then we have
\begin{align*}
    & \int_{\mathbb{E}} \int_{\mathbb{E}} \left(f(\|x\| + \|y\|) - f(\|y\|) \right)\, d^2x d^2y \\
    = & (2\pi)^2 \int^1_0 \int^1_0 \left( f(r + s) - f(s) \right) rs\, dr ds \\
    = & 4\pi^2 \int^1_0 \int^1_0 \left( f'(s)r + \frac{f''(\xi)}{2}r^2 \right)rs \, drds \\
    = & 4\pi^2 \int^1_0 \int^1_0 f'(s)r^2s \, drds + 2\pi^2 \int^1_0 \int^1_0 f''(\xi)r^3s\, drds \\
    = & \frac{4\pi^2}{3} \int^1_0 f'(s)s \, ds + \frac{\pi^2}{4} \int^1_0 f''(\xi) \, ds \\
    \leq & \frac{4\pi^2}{3} \int^1_0 s^2 \, ds + \frac{\pi^2}{4} \int^1_0 1 \, ds = \frac{25\pi^2}{36}.
\end{align*}
\end{proof}






\newpage
\bibliographystyle{unsrt}
\bibliography{bibliography}


\end{document}